{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers --index-url=https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple --trusted-host=artifactory.alight.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install en_core_web_sm-3.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (0,1,2,4,5,9,10,11,12,13,14,15,16,17,18,20,21,23,24,25,26,27,28,29,31,33,39,41,56,57,58,59,63,64,65,67,68,70,71,74,75,77,78,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,106,108,109,110,111,112,113,114,115,116,117,118,119,122,123,124,125,126,127,128,129,130,131,132,134,135,136,137,138,155,156,159,160,161,164,165) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_combined_web_iva_search = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/IVA/combined_new_adult-child_outer_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child a dependent?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i change my dependent daycare deduction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child last name change</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am trying to add my children as beneficiarie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how much timeoff do i get after the birth of m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input search_text page_name\n",
       "0                            step child a dependent?         NaN       NaN\n",
       "1     how do i change my dependent daycare deduction         NaN       NaN\n",
       "2                             Child last name change         NaN       NaN\n",
       "3  i am trying to add my children as beneficiarie...         NaN       NaN\n",
       "4  how much timeoff do i get after the birth of m...         NaN       NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_web_iva_search[['input','search_text','page_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/Deepali/IVA_cleaned_labelled(session_id_added).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'entry_id', 'client_id', 'person_internal_id',\n",
       "       'input_orig', 'labels', 'input_cleaned', 'input_cleaned_dl',\n",
       "       'next_unit_hit', 'previous_unit_hit', 'unit_name', 'response_text',\n",
       "       'session_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iva.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva1 = df_iva.drop(['Unnamed: 0','entry_id','client_id','person_internal_id','next_unit_hit',\n",
    "             'previous_unit_hit','response_text','session_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_cleaned</th>\n",
       "      <th>input_cleaned_dl</th>\n",
       "      <th>unit_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>open enrol</td>\n",
       "      <td>open enrollment</td>\n",
       "      <td>Annual Enrollment Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>enrol hra</td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>Health Reimbursement Account Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>tire discount</td>\n",
       "      <td>tire discount</td>\n",
       "      <td>Discounts Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>elig</td>\n",
       "      <td>eligible</td>\n",
       "      <td>Health Savings Account (HSA) Eligible Expenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>need updat mail address</td>\n",
       "      <td>need to update my mail address</td>\n",
       "      <td>Manage Address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       input_orig                labels  \\\n",
       "0                 Open enrollment            Enrollment   \n",
       "1                   enroll in hra                   HRA   \n",
       "2                   TIRE DISCOUNT       Discounts Issue   \n",
       "3                        eligible           HSA related   \n",
       "4  Need to update my mail address  General Acount issue   \n",
       "\n",
       "             input_cleaned                input_cleaned_dl  \\\n",
       "0               open enrol                 open enrollment   \n",
       "1                enrol hra                   enroll in hra   \n",
       "2            tire discount                   tire discount   \n",
       "3                     elig                        eligible   \n",
       "4  need updat mail address  need to update my mail address   \n",
       "\n",
       "                                        unit_name  \n",
       "0                     Annual Enrollment Clarifier  \n",
       "1          Health Reimbursement Account Clarifier  \n",
       "2                             Discounts Clarifier  \n",
       "3  Health Savings Account (HSA) Eligible Expenses  \n",
       "4                                  Manage Address  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iva1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva2 = df_iva1.drop(['input_cleaned','input_cleaned_dl'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>unit_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>Annual Enrollment Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>Health Reimbursement Account Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>Discounts Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>Health Savings Account (HSA) Eligible Expenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>Manage Address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       input_orig                labels  \\\n",
       "0                 Open enrollment            Enrollment   \n",
       "1                   enroll in hra                   HRA   \n",
       "2                   TIRE DISCOUNT       Discounts Issue   \n",
       "3                        eligible           HSA related   \n",
       "4  Need to update my mail address  General Acount issue   \n",
       "\n",
       "                                        unit_name  \n",
       "0                     Annual Enrollment Clarifier  \n",
       "1          Health Reimbursement Account Clarifier  \n",
       "2                             Discounts Clarifier  \n",
       "3  Health Savings Account (HSA) Eligible Expenses  \n",
       "4                                  Manage Address  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iva2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_combined_web_iva_search, df_iva2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat2 = df_concat[['input','input_orig','search_text','page_name','labels','session_start_cst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>input_orig</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child a dependent?</td>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i change my dependent daycare deduction</td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child last name change</td>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am trying to add my children as beneficiarie...</td>\n",
       "      <td>eligible</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how much timeoff do i get after the birth of m...</td>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                            step child a dependent?   \n",
       "1     how do i change my dependent daycare deduction   \n",
       "2                             Child last name change   \n",
       "3  i am trying to add my children as beneficiarie...   \n",
       "4  how much timeoff do i get after the birth of m...   \n",
       "\n",
       "                       input_orig search_text page_name                labels  \\\n",
       "0                 Open enrollment         NaN       NaN            Enrollment   \n",
       "1                   enroll in hra         NaN       NaN                   HRA   \n",
       "2                   TIRE DISCOUNT         NaN       NaN       Discounts Issue   \n",
       "3                        eligible         NaN       NaN           HSA related   \n",
       "4  Need to update my mail address         NaN       NaN  General Acount issue   \n",
       "\n",
       "     session_start_cst  \n",
       "0  2017-12-05 21:48:56  \n",
       "1  2017-12-11 09:45:09  \n",
       "2  2018-01-22 08:12:28  \n",
       "3  2017-10-15 10:34:01  \n",
       "4  2017-12-29 10:46:58  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2330066, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_concat2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971001, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append('../../nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(df, text_cols):\n",
    "    # Create a new dataframe to hold the cleaned text columns\n",
    "    cleaned_df = pd.DataFrame()\n",
    "    \n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean each text column and add it to the cleaned dataframe\n",
    "    for text_col in text_cols:\n",
    "        text_list = df[text_col].tolist()\n",
    "        text_list = [str(text) for text in text_list]\n",
    "        text_list = [text for text in text_list if text.strip() and not\n",
    "                     set(text).issubset(set(string.punctuation + string.whitespace))]\n",
    "        text_list = [x.lower() for x in text_list]\n",
    "        translator = str.maketrans(string.punctuation + string.digits + \"_\", \" \" * len(\n",
    "            string.punctuation + string.digits + \"_\"))\n",
    "        cleaned_list = []\n",
    "        for text in text_list:\n",
    "            cleaned_text = text.translate(translator)\n",
    "            cleaned_text = ' '.join(cleaned_text.split())\n",
    "            cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "            cleaned_list.append(cleaned_text)\n",
    "        cleaned_df[text_col] = cleaned_list\n",
    "    \n",
    "    # Add the non-text columns to the cleaned dataframe\n",
    "    for col in df.columns:\n",
    "        if col not in text_cols:\n",
    "            cleaned_df[col] = df[col]\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat3 = clean_text(df_concat2, text_cols=['input','search_text','page_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child dependent</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>child last name change</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much timeoff get birth child</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input search_text page_name  \\\n",
       "0                        step child dependent         nan       nan   \n",
       "1          change dependent daycare deduction         nan       nan   \n",
       "2                      child last name change         nan       nan   \n",
       "3  trying add children beneficiaries life ins         nan       nan   \n",
       "4                much timeoff get birth child         nan       nan   \n",
       "\n",
       "                       input_orig                labels    session_start_cst  \n",
       "0                 Open enrollment            Enrollment  2017-12-05 21:48:56  \n",
       "1                   enroll in hra                   HRA  2017-12-11 09:45:09  \n",
       "2                   TIRE DISCOUNT       Discounts Issue  2018-01-22 08:12:28  \n",
       "3                        eligible           HSA related  2017-10-15 10:34:01  \n",
       "4  Need to update my mail address  General Acount issue  2017-12-29 10:46:58  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child dependent</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>child last name change</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much timeoff get birth child</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input search_text page_name  \\\n",
       "0                        step child dependent                         \n",
       "1          change dependent daycare deduction                         \n",
       "2                      child last name change                         \n",
       "3  trying add children beneficiaries life ins                         \n",
       "4                much timeoff get birth child                         \n",
       "\n",
       "                       input_orig                labels    session_start_cst  \n",
       "0                 Open enrollment            Enrollment  2017-12-05 21:48:56  \n",
       "1                   enroll in hra                   HRA  2017-12-11 09:45:09  \n",
       "2                   TIRE DISCOUNT       Discounts Issue  2018-01-22 08:12:28  \n",
       "3                        eligible           HSA related  2017-10-15 10:34:01  \n",
       "4  Need to update my mail address  General Acount issue  2017-12-29 10:46:58  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4 = df_concat3.replace('nan', '')\n",
    "df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4['text'] = df_concat4[['input', 'search_text', 'page_name']].apply(lambda x: ' '.join([str(i) for i in x if not pd.isna(i)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop(['input','search_text','page_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop(['input_orig'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "      <td>step child dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "      <td>child last name change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "      <td>much timeoff get birth child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 labels    session_start_cst  \\\n",
       "0            Enrollment  2017-12-05 21:48:56   \n",
       "1                   HRA  2017-12-11 09:45:09   \n",
       "2       Discounts Issue  2018-01-22 08:12:28   \n",
       "3           HSA related  2017-10-15 10:34:01   \n",
       "4  General Acount issue  2017-12-29 10:46:58   \n",
       "\n",
       "                                           text  \n",
       "0                        step child dependent    \n",
       "1          change dependent daycare deduction    \n",
       "2                      child last name change    \n",
       "3  trying add children beneficiaries life ins    \n",
       "4                much timeoff get birth child    "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# Get the value counts of the 'labels' column\n",
    "label_counts = df_concat4['labels'].value_counts()\n",
    "print(sum(label_counts>500))\n",
    "# Filter the dataframe to only include rows where the label count is greater than 10000\n",
    "df_concat5 = df_concat4[df_concat4['labels'].isin(label_counts[label_counts > 10000].index)]\n",
    "\n",
    "# Get the shape of the resulting filtered dataframe\n",
    "df_concat5_shape = df_concat5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36517, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat5_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other              25225\n",
       "Health Benefits    11292\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat5['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4[df_concat4['text']=='nan '].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.to_excel('cc_df_concat4.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the texts which contain exact phrases from synonyms list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = ['day care', 'creche', 'childcare','daycare','nurseryschool','after school care',\n",
    "           'pre school', 'child', 'baby', 'minor care','minorcare','offspring',\n",
    "           'daynursery', 'infantschool','pre K', 'childcarer', 'childcarers','girl child',\n",
    " 'infantcare', 'play school', 'playgroup','boy child','Adolescent','Little one','Young one',\n",
    " 'nursery', 'nursery school', 'preschool', 'after schoolcare', 'day nursery', 'infant school', \n",
    " 'infant care', 'playschool', 'play group', 'kindergarten', 'childminding', 'babysitting',\n",
    "           'babysitter', \n",
    " 'nanny care', 'children supervision', 'toddler care', 'prekindergarten',\n",
    "           'Junior care','Sprout care',\n",
    "'child minding', 'baby sitting', 'baby sitter', 'children',\n",
    " 'nannycare', 'childrensupervision','child supervision', 'childsupervision', \n",
    "           'toddlercare','foster', 'childs','childrens',\n",
    "           'stepchild','step daughter', 'step son', 'grandchildren','grandchild',\n",
    "          'stepchildren','childbirth',\"children's\", 'childhood',\n",
    "          'childsupport','childcarereimbursement','childern','childrren',\n",
    "           'grndchild','childre','granchild','achild','stephchild','childen','childbrith',\n",
    "           'childrem','childeren','childd','childrfen ','mychildren',\n",
    "           'mychild','childres','childresn','childcareplus','childcarea','stepchildrens',\n",
    "           'childbirthing','dependentchild','childer','childacre','childrene','childrent','childcar',\n",
    "'childcarew','childcrare','childacre','grandchildern','grandchildreni','childten',\n",
    "          'childrewn','ghandchild','childrne','childtren','childreren','insurancechildren',\n",
    "           'mybchildren','childare','grandchilddren','childcard',\n",
    "          'childcard','grandchilddren','childare','mybchildren','insurancechildren',\n",
    "           'childfrens','childreren','childtren','childrne','ghandchild','childrewn','childm',\n",
    "          'childplus','childiren','childred','childrena','childrenen','childredn','childrebn',\n",
    "           'childred','childlren','childrins','childbonding','childn']\n",
    "words_4 = list(set([word.lower() for word in words_3]))\n",
    "\n",
    "len(words_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "mask = (df_concat4['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n",
    "\n",
    "df_concat4['category'] = ''\n",
    "df_concat4.loc[mask, 'category'] = 'Child care'\n",
    "# df_combined_web_iva_search.loc[df_combined_web_iva_search['category'] == '', 'category'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400634</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>2023-01-23 09:32:00</td>\n",
       "      <td>need add child born insurance</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60843</th>\n",
       "      <td>Smoking</td>\n",
       "      <td>2020-10-27 14:51:46</td>\n",
       "      <td>email send dependent daycare form  contentpage child adult pet care resources</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158288</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2022-09-26 11:57:28</td>\n",
       "      <td>uploaded document child care plus asking upload child care plus enroll hm cstm child care domain ben hm baseclientindicator client languageid en us</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593632</th>\n",
       "      <td>Other</td>\n",
       "      <td>2022-05-11 08:50:12</td>\n",
       "      <td>child depenedtnt turns</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3817</th>\n",
       "      <td>Payment Issue</td>\n",
       "      <td>2017-11-11 14:16:35</td>\n",
       "      <td>need change dependent child</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192374</th>\n",
       "      <td>W-2 Form</td>\n",
       "      <td>2021-02-04 08:44:51</td>\n",
       "      <td>trying add new child insurance add social security number yet</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350235</th>\n",
       "      <td>Other</td>\n",
       "      <td>2021-09-24 08:24:56</td>\n",
       "      <td>would ex wife able use health spending cards services covered children</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778636</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tuition assistance children contentpage need find child care facility</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>onsite child care lcc</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253581</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2022-08-31 13:31:59</td>\n",
       "      <td>wanted know enroll started working deluxe completed benefits need add children thought able add</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578451</th>\n",
       "      <td>Other</td>\n",
       "      <td>2022-08-23 18:42:37</td>\n",
       "      <td>child turn next year able stay insurance</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230561</th>\n",
       "      <td>Other</td>\n",
       "      <td>2019-02-12 12:51:17</td>\n",
       "      <td>mistakenly removed child cannot add</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212294</th>\n",
       "      <td>Other</td>\n",
       "      <td>2023-01-23 09:21:23</td>\n",
       "      <td>since currently contribute allowing access child care options</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348622</th>\n",
       "      <td>Other</td>\n",
       "      <td>2020-12-04 06:50:56</td>\n",
       "      <td>need medical insurance child enroll life insurance</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308414</th>\n",
       "      <td>Other</td>\n",
       "      <td>2019-12-12 08:28:49</td>\n",
       "      <td>signing basic life insurance day care spend</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81363</th>\n",
       "      <td>Claims</td>\n",
       "      <td>2020-11-20 10:54:19</td>\n",
       "      <td>hi child born may want add dependent get benefits aecom</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584946</th>\n",
       "      <td>Dental Plan</td>\n",
       "      <td>2022-11-04 15:24:07</td>\n",
       "      <td>child care plus child care enroll hmcstmchildcarepluslandingpageopen</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106724</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2019-02-12 14:06:59</td>\n",
       "      <td>partnerships daycares  hmcstmchildcarepluslandingpageopen</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337882</th>\n",
       "      <td>Other</td>\n",
       "      <td>2019-06-12 16:35:07</td>\n",
       "      <td>could send form file fsa account day care submit wanting fax</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477515</th>\n",
       "      <td>FSA</td>\n",
       "      <td>2021-11-01 19:42:09</td>\n",
       "      <td>wife expecting baby around end may information target need baby start bonding time wife child</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        labels    session_start_cst  \\\n",
       "400634   General Account issue  2023-01-23 09:32:00   \n",
       "60843                  Smoking  2020-10-27 14:51:46   \n",
       "158288         Health Benefits  2022-09-26 11:57:28   \n",
       "593632                   Other  2022-05-11 08:50:12   \n",
       "3817             Payment Issue  2017-11-11 14:16:35   \n",
       "192374                W-2 Form  2021-02-04 08:44:51   \n",
       "350235                   Other  2021-09-24 08:24:56   \n",
       "778636         Health Benefits                  NaN   \n",
       "1138602                    NaN                  NaN   \n",
       "253581              Enrollment  2022-08-31 13:31:59   \n",
       "578451                   Other  2022-08-23 18:42:37   \n",
       "230561                   Other  2019-02-12 12:51:17   \n",
       "212294                   Other  2023-01-23 09:21:23   \n",
       "348622                   Other  2020-12-04 06:50:56   \n",
       "308414                   Other  2019-12-12 08:28:49   \n",
       "81363                   Claims  2020-11-20 10:54:19   \n",
       "584946             Dental Plan  2022-11-04 15:24:07   \n",
       "106724         Health Benefits  2019-02-12 14:06:59   \n",
       "337882                   Other  2019-06-12 16:35:07   \n",
       "477515                     FSA  2021-11-01 19:42:09   \n",
       "\n",
       "                                                                                                                                                        text  \\\n",
       "400634                                                                                                                       need add child born insurance     \n",
       "60843                                                                          email send dependent daycare form  contentpage child adult pet care resources   \n",
       "158288   uploaded document child care plus asking upload child care plus enroll hm cstm child care domain ben hm baseclientindicator client languageid en us   \n",
       "593632                                                                                                                              child depenedtnt turns     \n",
       "3817                                                                                                                           need change dependent child     \n",
       "192374                                                                                       trying add new child insurance add social security number yet     \n",
       "350235                                                                              would ex wife able use health spending cards services covered children     \n",
       "778636                                                                                 tuition assistance children contentpage need find child care facility   \n",
       "1138602                                                                                                                               onsite child care lcc    \n",
       "253581                                                     wanted know enroll started working deluxe completed benefits need add children thought able add     \n",
       "578451                                                                                                            child turn next year able stay insurance     \n",
       "230561                                                                                                                 mistakenly removed child cannot add     \n",
       "212294                                                                                       since currently contribute allowing access child care options     \n",
       "348622                                                                                                  need medical insurance child enroll life insurance     \n",
       "308414                                                                                                         signing basic life insurance day care spend     \n",
       "81363                                                                                              hi child born may want add dependent get benefits aecom     \n",
       "584946                                                                                  child care plus child care enroll hmcstmchildcarepluslandingpageopen   \n",
       "106724                                                                                             partnerships daycares  hmcstmchildcarepluslandingpageopen   \n",
       "337882                                                                                        could send form file fsa account day care submit wanting fax     \n",
       "477515                                                       wife expecting baby around end may information target need baby start bonding time wife child     \n",
       "\n",
       "           category  \n",
       "400634   Child care  \n",
       "60843    Child care  \n",
       "158288   Child care  \n",
       "593632   Child care  \n",
       "3817     Child care  \n",
       "192374   Child care  \n",
       "350235   Child care  \n",
       "778636   Child care  \n",
       "1138602  Child care  \n",
       "253581   Child care  \n",
       "578451   Child care  \n",
       "230561   Child care  \n",
       "212294   Child care  \n",
       "348622   Child care  \n",
       "308414   Child care  \n",
       "81363    Child care  \n",
       "584946   Child care  \n",
       "106724               \n",
       "337882   Child care  \n",
       "477515   Child care  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat4[df_concat4['category']=='Child care'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109348, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4[df_concat4['category']=='Child care'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get text which are similar to phrases in synonnyms list for texts other than which are filtered above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# def find_similar_sentences(df, sentences, synonyms_list, threshold=0.90, category_name='Child care'):\n",
    "\n",
    "#     # Define an empty list to store the filtered rows\n",
    "#     filtered_rows = []\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         # Tokenize the sentence\n",
    "#         # tokens = tokenizer(sentence, padding=True, truncation=True, return_attention_mask=True,\n",
    "#         #                    return_token_type_ids=False)\n",
    "#         tokens = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "#         # print(tokens.keys())\n",
    "#         # Get the BERT embeddings for the tokens\n",
    "#         with torch.no_grad():\n",
    "#             # outputs = model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "#             outputs = model(input_ids=torch.tensor(tokens['input_ids']), attention_mask=torch.tensor(tokens['attention_mask']))\n",
    "#             embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Define an empty list to store the matching tokens and phrases\n",
    "#         matching_tokens_phrases = []\n",
    "\n",
    "#         # Compute the cosine similarity between the sentence and each phrase/synonym\n",
    "#         for phrase in synonyms_list:\n",
    "#             # Tokenize the phrase\n",
    "#             # phrase_tokens = tokenizer(phrase, padding=True, truncation=True, return_attention_mask=False,\n",
    "#             #                           return_token_type_ids=False)\n",
    "#             phrase_tokens = tokenizer(phrase, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "#             # Get the BERT embeddings for the phrase\n",
    "#             with torch.no_grad():\n",
    "#                 phrase_outputs = model(**phrase_tokens)\n",
    "#                 phrase_embeddings = phrase_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "#             # Compute the cosine similarity between the sentence and the phrase\n",
    "#             cos_sim = torch.nn.functional.cosine_similarity(embeddings, phrase_embeddings)\n",
    "\n",
    "#             # Convert the tensor to a Python float using the .item() method\n",
    "#             cos_sim = cos_sim.item()\n",
    "\n",
    "#             if cos_sim > threshold:\n",
    "        \n",
    "#                 for i, token in enumerate(tokens['input_ids'][0]):\n",
    "#                     # Get the token string and convert it from byte to string\n",
    "#                     token_str = tokenizer.convert_ids_to_tokens(token.item())\n",
    "\n",
    "#                     # Compute the cosine similarity between the token and the phrase\n",
    "#                     token_cos_sim = torch.nn.functional.cosine_similarity(phrase_embeddings, embeddings[i].unsqueeze(0))\n",
    "#                     token_cos_sim = token_cos_sim.item()\n",
    "\n",
    "#                     # If the cosine similarity is above the threshold, add the matching token and phrase to the list\n",
    "#                     if token_cos_sim > threshold:\n",
    "#                         matching_tokens_phrases.append((token_str, phrase))\n",
    "#                         break\n",
    "\n",
    "#         # If there are any matching tokens and phrases, add the sentence to the filtered rows\n",
    "#         if matching_tokens_phrases:\n",
    "#             # filtered_rows.append((sentence, matching_tokens_phrases))\n",
    "#             filtered_rows.append((sentence, tuple(matching_tokens_phrases)))\n",
    "\n",
    "\n",
    "#     # Convert the list of similar sentences to a set to remove duplicates\n",
    "#     filtered_rows = set(filtered_rows)\n",
    "\n",
    "#     # Convert the filtered rows to a new dataframe\n",
    "#     filtered_df = pd.DataFrame(columns=['text', 'matching_tokens_phrases'])\n",
    "#     for row in filtered_rows:\n",
    "#         filtered_df = filtered_df.append({'text': row[0], 'matching_tokens_phrases': row[1]}, ignore_index=True)\n",
    "\n",
    "#     # Add a category column to the filtered dataframe\n",
    "#     filtered_df['category'] = category_name\n",
    "\n",
    "#     return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def find_similar_sentences(df, sentences, synonyms_list, threshold=0.90, category_name='Child care', batch_size=32):\n",
    "\n",
    "#     filtered_rows = []  # create an empty list to hold the filtered rows\n",
    "#     num_batches = int(np.ceil(len(sentences) / batch_size))  # compute the number of batches based on the batch size and number of sentences\n",
    "    \n",
    "#     # loop over each batch of sentences\n",
    "#     for batch_idx in range(num_batches):\n",
    "#         start_idx = batch_idx * batch_size\n",
    "#         end_idx = min((batch_idx + 1) * batch_size, len(sentences))\n",
    "#         batch_sentences = sentences[start_idx:end_idx]\n",
    "#         # print(batch_sentences)\n",
    "#         # Tokenize the batch of sentences using the BERT tokenizer\n",
    "#         batch_tokens = tokenizer.batch_encode_plus(batch_sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#         # Get the BERT embeddings for the tokens\n",
    "#         with torch.no_grad():\n",
    "#             batch_outputs = model(**batch_tokens)\n",
    "#             batch_embeddings = batch_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Iterate over each sentence in the batch\n",
    "#         for sentence_idx in range(len(batch_sentences)):\n",
    "#             # Get the tokens for the current sentence\n",
    "#             sentence_tokens = tokenizer.convert_ids_to_tokens(batch_tokens['input_ids'][sentence_idx])\n",
    "\n",
    "#             # Compute the cosine similarity between the batch embeddings and the phrase embeddings\n",
    "#             cos_sim = torch.nn.functional.cosine_similarity(batch_embeddings[sentence_idx].unsqueeze(0), batch_embeddings, dim=1)\n",
    "\n",
    "#             # Get the cosine similarity for the current sentence\n",
    "#             sentence_cos_sim = cos_sim[0].item()\n",
    "\n",
    "#             # If the cosine similarity is above the threshold, directly add the sentence to the filtered rows\n",
    "#             if sentence_cos_sim >= threshold:\n",
    "#                 filtered_rows.append((batch_sentences[sentence_idx], ()))\n",
    "                \n",
    "#             # If the cosine similarity is below the threshold, find the matching tokens and phrases\n",
    "#             else:\n",
    "#                 matching_tokens_phrases = []\n",
    "\n",
    "#                 # Iterate over each token in the sentence\n",
    "#                 for token_idx in range(len(sentence_tokens)):\n",
    "#                     token_str = sentence_tokens[token_idx]\n",
    "\n",
    "#                     # Compute the cosine similarity between the token and the phrases in the synonyms list\n",
    "#                     for phrase in synonyms_list:\n",
    "#                         # Tokenize the phrase using the BERT tokenizer\n",
    "#                         phrase_tokens = tokenizer(phrase, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#                         # Get the BERT embeddings for the phrase\n",
    "#                         with torch.no_grad():\n",
    "#                             phrase_outputs = model(**phrase_tokens)\n",
    "#                             phrase_embeddings = phrase_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "#                         # Compute the cosine similarity between the token and the phrase\n",
    "#                         token_cos_sim = torch.nn.functional.cosine_similarity(phrase_embeddings, batch_embeddings[sentence_idx][token_idx].unsqueeze(0))\n",
    "#                         token_cos_sim = token_cos_sim.item()\n",
    "\n",
    "#                         # If the cosine similarity is above the threshold, add the matching token and phrase to the list\n",
    "#                         if token_cos_sim > threshold:\n",
    "#                             matching_tokens_phrases.append((token_str, phrase))\n",
    "\n",
    "#                 # If there are any matching tokens and phrases, add the sentence to the filtered rows\n",
    "#                 if matching_tokens_phrases:\n",
    "#                     filtered_rows.append((batch_sentences[sentence_idx], tuple(matching_tokens_phrases)))\n",
    "\n",
    "#     # Convert the list of similar sentences to a set to remove duplicates\n",
    "#     filtered_rows = set(filtered_rows)\n",
    "#     # print(filtered_rows)\n",
    "#     # Convert the filtered rows to a new dataframe\n",
    "#     filtered_df = pd.DataFrame(columns=['text', 'matching_tokens_phrases', 'category'])\n",
    "#     for row in filtered_rows:\n",
    "#         filtered_df = filtered_df.append({'text': row[0], 'matching_tokens_phrases': row[1], 'category': category_name}, ignore_index=True)\n",
    "    \n",
    "#     return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.parallel import DataParallel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "def find_similar_sentences(df, sentences, synonyms_list, threshold=0.90, category_name='Child care',\n",
    "                           batch_size=52):\n",
    "    # Create a tokenizer and a pre-trained BERT model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a set of unique synonyms\n",
    "    unique_synonyms = set(synonyms_list)\n",
    "\n",
    "    # Compute the average BERT embeddings for each unique phrase in the synonym list\n",
    "    phrase_embeddings = []\n",
    "    for phrase in unique_synonyms:\n",
    "        phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            phrase_outputs = model(torch.tensor(phrase_tokens).unsqueeze(0).to(device))\n",
    "            phrase_embedding = phrase_outputs.last_hidden_state.mean(dim=1)\n",
    "            phrase_embeddings.append(phrase_embedding)\n",
    "    phrase_embeddings = torch.cat(phrase_embeddings, dim=0)\n",
    "\n",
    "    # Use DataParallel to parallelize computations across multiple GPUs\n",
    "    print(torch.cuda.device_count())   \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = DataParallel(model)\n",
    "\n",
    "    # Create an empty list to hold the filtered rows\n",
    "    filtered_rows = []\n",
    "\n",
    "    # Loop over each batch of sentences\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        # Tokenize the batch of sentences using the BERT tokenizer\n",
    "        batch_tokens = tokenizer.batch_encode_plus(batch_sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # Get the BERT embeddings for the tokens\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = model(**batch_tokens)\n",
    "            batch_embeddings = batch_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Compute the cosine similarity between the batch embeddings and the phrase embeddings\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(batch_embeddings, phrase_embeddings, dim=1)\n",
    "\n",
    "        # Get the indices of the sentences that are above the threshold\n",
    "        sim_indices = torch.where(cos_sim >= threshold)[0].tolist()\n",
    "\n",
    "        # Iterate over each similar sentence\n",
    "        for idx in sim_indices:\n",
    "            filtered_rows.append((batch_sentences[idx], ()))\n",
    "        \n",
    "        matching_tokens_phrases = []\n",
    "        # Iterate over each sentence in the batch\n",
    "        for j in range(len(batch_sentences)):\n",
    "            # If the sentence has already been added to the filtered rows, skip it\n",
    "            if j in sim_indices:\n",
    "                continue\n",
    "\n",
    "            # matching_tokens_phrases = []\n",
    "\n",
    "            # Iterate over each token in the sentence\n",
    "            # for token_idx, token in enumerate(batch_tokens['input_ids'][j].numpy())\n",
    "            for token_idx, token in enumerate(batch_tokens['input_ids'][j].cpu().numpy()):\n",
    "                token_scalar = int(token)  # Convert numpy array to scalar\n",
    "                token_str = tokenizer.convert_ids_to_tokens(token_scalar)\n",
    "\n",
    "                # Compute the cosine similarity between the token and the phrases in the synonyms list\n",
    "                for phrase in unique_synonyms:\n",
    "                    # Tokenize the phrase using the BERT tokenizer\n",
    "                    phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False, truncation=True)\n",
    "\n",
    "                    # Get the BERT embeddings for the phrase\n",
    "                    with torch.no_grad():\n",
    "                        phrase_outputs = model(torch.tensor(phrase_tokens).unsqueeze(0))\n",
    "                        phrase_embedding = phrase_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "                    # Compute the cosine similarity between the token and the phrase\n",
    "                    token_cos_sim = torch.nn.functional.cosine_similarity(phrase_embedding, batch_embeddings[j][token_idx].unsqueeze(0))\n",
    "                    token_cos_sim = token_cos_sim.item()\n",
    "\n",
    "                    # If the cosine similarity is above the threshold, add the matching token and phrase to the list\n",
    "                    if token_cos_sim > threshold:\n",
    "                        matching_tokens_phrases.append((token_str, phrase))\n",
    "\n",
    "                        \n",
    "            # If there are any matching tokens and phrases, add the sentence to the filtered rows\n",
    "            if matching_tokens_phrases:\n",
    "                filtered_rows.append((batch_sentences[j], tuple(matching_tokens_phrases)))\n",
    "                \n",
    "    # Convert the list of similar sentences to a set to remove duplicates\n",
    "    filtered_rows = set(filtered_rows)\n",
    "    # print(filtered_rows)\n",
    "    # Convert the filtered rows to a new dataframe\n",
    "    filtered_df = pd.DataFrame(columns=['text', 'matching_tokens_phrases', 'category'])\n",
    "    for row in filtered_rows:\n",
    "        filtered_df = filtered_df.append({'text': row[0], 'matching_tokens_phrases': row[1], 'category': category_name}, ignore_index=True)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4 = df_concat4.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (52) must match the size of tensor b (122) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-11b083b9dd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m similar_df2 = find_similar_sentences(df_concat4, \n\u001b[1;32m      2\u001b[0m                                     \u001b[0mdf_concat4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_concat4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                     words_4)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# similar_df2 = find_similar_sentences(df_concat4, ['elder care','older care'], words_4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-f4164000d2a2>\u001b[0m in \u001b[0;36mfind_similar_sentences\u001b[0;34m(df, sentences, synonyms_list, threshold, category_name, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Compute the cosine similarity between the batch embeddings and the phrase embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Get the indices of the sentences that are above the threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (52) must match the size of tensor b (122) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "similar_df2 = find_similar_sentences(df_concat4, \n",
    "                                    df_concat4[df_concat4['category']=='']['text'].head(100).to_list(), \n",
    "                                    words_4)\n",
    "# similar_df2 = find_similar_sentences(df_concat4, ['elder care','older care'], words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>matching_tokens_phrases</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eldercare benefits</td>\n",
       "      <td>()</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need information elder care fsa covers husband much older need assistance work hours</td>\n",
       "      <td>()</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user friendly everything referred back computer elderly retired employees like computers dont remember password logged remember anyway need know p</td>\n",
       "      <td>()</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>husband hospitalized may need home physical therapy home care released trying figure covered looks like</td>\n",
       "      <td>()</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    text  \\\n",
       "0                                                                                                                                   eldercare benefits     \n",
       "1                                                                 need information elder care fsa covers husband much older need assistance work hours     \n",
       "2  user friendly everything referred back computer elderly retired employees like computers dont remember password logged remember anyway need know p     \n",
       "3                                              husband hospitalized may need home physical therapy home care released trying figure covered looks like     \n",
       "\n",
       "  matching_tokens_phrases    category  \n",
       "0                      ()  Child care  \n",
       "1                      ()  Child care  \n",
       "2                      ()  Child care  \n",
       "3                      ()  Child care  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_CC_df = pd.concat([df_concat4[df_concat4['category']=='Child care'], \n",
    "                       similar_df2]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109364, 5)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_CC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_CC_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109364, 5)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_CC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "non_similar_df = df_concat4[~df_concat4['text'].isin(only_CC_df['text'])]\n",
    "\n",
    "non_similar_df['category'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1734, 4)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_similar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated = pd.concat([only_CC_df, non_similar_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 5)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matching_tokens_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39388</th>\n",
       "      <td>HRA</td>\n",
       "      <td>2023-01-27 11:51:19</td>\n",
       "      <td>change childerns last name</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30476</th>\n",
       "      <td>Coverage Issue</td>\n",
       "      <td>2023-01-09 14:14:19</td>\n",
       "      <td>added childand plan came</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45861</th>\n",
       "      <td>Loan related</td>\n",
       "      <td>2023-03-15 09:52:38</td>\n",
       "      <td>need know difference price family vs chchild</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46780</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2023-01-05 11:13:37</td>\n",
       "      <td>add vchildern isurance</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71296</th>\n",
       "      <td>Other</td>\n",
       "      <td>2021-02-08 09:45:26</td>\n",
       "      <td>cover gradnchild depend care</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105914</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>2020-10-30 15:35:14</td>\n",
       "      <td>covered childe life insurance</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107378</th>\n",
       "      <td>view/print</td>\n",
       "      <td>2017-12-06 09:31:35</td>\n",
       "      <td>son receiving michild government program longer covered program income gone need add insurance process need</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51685</th>\n",
       "      <td>Claims</td>\n",
       "      <td>NaN</td>\n",
       "      <td>extended childca</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rothchild</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37126</th>\n",
       "      <td>Form 1095 Issue</td>\n",
       "      <td>2022-09-16 11:50:41</td>\n",
       "      <td>benefits savings retirement change made change transaction qsc actsp childgaineligelsewhe</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84843</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reimbursement formschildcare</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110177</th>\n",
       "      <td>Loan related</td>\n",
       "      <td>2022-07-27 09:17:51</td>\n",
       "      <td>need see plan covers mental health ttherapy childar</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80603</th>\n",
       "      <td>Paycheck Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>backup childc</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84312</th>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pay home caregiver otherchildcarediscounts</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89200</th>\n",
       "      <td>Holiday/Leave Issue</td>\n",
       "      <td>2022-11-23 10:04:37</td>\n",
       "      <td>howdoichangetheaddressofonemychildren</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20932</th>\n",
       "      <td>Login Issue</td>\n",
       "      <td>2022-11-21 11:56:54</td>\n",
       "      <td>childcase fsa</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109243</th>\n",
       "      <td>Other</td>\n",
       "      <td>2023-01-29 10:51:23</td>\n",
       "      <td>change dr need change ginger goodchild</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65848</th>\n",
       "      <td>1099 Form</td>\n",
       "      <td>2021-12-17 06:09:49</td>\n",
       "      <td>option dependent grdchild</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84949</th>\n",
       "      <td>Natural Disaster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>death childf</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110780</th>\n",
       "      <td>Other</td>\n",
       "      <td>2023-03-16 13:18:29</td>\n",
       "      <td>need update childerns ssn app</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       labels    session_start_cst  \\\n",
       "39388                     HRA  2023-01-27 11:51:19   \n",
       "30476          Coverage Issue  2023-01-09 14:14:19   \n",
       "45861            Loan related  2023-03-15 09:52:38   \n",
       "46780         Health Benefits  2023-01-05 11:13:37   \n",
       "71296                   Other  2021-02-08 09:45:26   \n",
       "105914  General Account issue  2020-10-30 15:35:14   \n",
       "107378             view/print  2017-12-06 09:31:35   \n",
       "51685                  Claims                  NaN   \n",
       "9012                      NaN                  NaN   \n",
       "37126         Form 1095 Issue  2022-09-16 11:50:41   \n",
       "84843              Enrollment                  NaN   \n",
       "110177           Loan related  2022-07-27 09:17:51   \n",
       "80603          Paycheck Issue                  NaN   \n",
       "84312                   Other                  NaN   \n",
       "89200     Holiday/Leave Issue  2022-11-23 10:04:37   \n",
       "20932             Login Issue  2022-11-21 11:56:54   \n",
       "109243                  Other  2023-01-29 10:51:23   \n",
       "65848               1099 Form  2021-12-17 06:09:49   \n",
       "84949        Natural Disaster                  NaN   \n",
       "110780                  Other  2023-03-16 13:18:29   \n",
       "\n",
       "                                                                                                                 text  \\\n",
       "39388                                                                                    change childerns last name     \n",
       "30476                                                                                      added childand plan came     \n",
       "45861                                                                  need know difference price family vs chchild     \n",
       "46780                                                                                        add vchildern isurance     \n",
       "71296                                                                                  cover gradnchild depend care     \n",
       "105914                                                                                covered childe life insurance     \n",
       "107378  son receiving michild government program longer covered program income gone need add insurance process need     \n",
       "51685                                                                                               extended childca    \n",
       "9012                                                                                                       rothchild    \n",
       "37126                     benefits savings retirement change made change transaction qsc actsp childgaineligelsewhe     \n",
       "84843                                                                                   reimbursement formschildcare    \n",
       "110177                                                          need see plan covers mental health ttherapy childar     \n",
       "80603                                                                                                  backup childc    \n",
       "84312                                                                      pay home caregiver otherchildcarediscounts   \n",
       "89200                                                                         howdoichangetheaddressofonemychildren     \n",
       "20932                                                                                                 childcase fsa     \n",
       "109243                                                                       change dr need change ginger goodchild     \n",
       "65848                                                                                     option dependent grdchild     \n",
       "84949                                                                                                   death childf    \n",
       "110780                                                                                need update childerns ssn app     \n",
       "\n",
       "       category matching_tokens_phrases  \n",
       "39388     Other                     NaN  \n",
       "30476     Other                     NaN  \n",
       "45861     Other                     NaN  \n",
       "46780     Other                     NaN  \n",
       "71296     Other                     NaN  \n",
       "105914    Other                     NaN  \n",
       "107378    Other                     NaN  \n",
       "51685     Other                     NaN  \n",
       "9012      Other                     NaN  \n",
       "37126     Other                     NaN  \n",
       "84843     Other                     NaN  \n",
       "110177    Other                     NaN  \n",
       "80603     Other                     NaN  \n",
       "84312     Other                     NaN  \n",
       "89200     Other                     NaN  \n",
       "20932     Other                     NaN  \n",
       "109243    Other                     NaN  \n",
       "65848     Other                     NaN  \n",
       "84949     Other                     NaN  \n",
       "110780    Other                     NaN  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated[(df_concatenated.text.str.contains('child')) & (df_concatenated.category=='Other')].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334, 5)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated[(df_concatenated.text.str.contains('child')) & (df_concatenated.category=='Other')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated.to_excel('BERT_CC_other_data_90%_sim.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_concatenated = pd.read_excel('BERT_CC_other_data_90%_sim.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated[df_concatenated.category=='Child care'].to_excel('BERT_only_child_care_training_data_90%_sim.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number og labels in label col where value of category col is Other\n",
    "df_concatenated[df_concatenated['category'] == 'Other']['labels'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated2 = df_concatenated.drop('session_start_cst', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2['labels'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2['category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated2['labels'] = df_concatenated2['labels'].fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    0.984392\n",
       "Other         0.015608\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2.category.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'labels', 'text', 'category', 'matching_tokens_phrases'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matching_tokens_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Natural Disaster</td>\n",
       "      <td>recently child send correct link add insurance</td>\n",
       "      <td>Child care</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>need help apllying back child care subsidy</td>\n",
       "      <td>Child care</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Dependent Care Spending</td>\n",
       "      <td>submit documents child care plus  hmcstmchildc...</td>\n",
       "      <td>Child care</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Payroll Issue</td>\n",
       "      <td>go drop coverage adult child</td>\n",
       "      <td>Child care</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dependent issue</td>\n",
       "      <td>hello order medical card children</td>\n",
       "      <td>Child care</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   labels  \\\n",
       "0           0         Natural Disaster   \n",
       "1           1                    Other   \n",
       "2           2  Dependent Care Spending   \n",
       "3           3            Payroll Issue   \n",
       "4           4          Dependent issue   \n",
       "\n",
       "                                                text    category  \\\n",
       "0   recently child send correct link add insurance    Child care   \n",
       "1       need help apllying back child care subsidy    Child care   \n",
       "2  submit documents child care plus  hmcstmchildc...  Child care   \n",
       "3                     go drop coverage adult child    Child care   \n",
       "4                hello order medical card children    Child care   \n",
       "\n",
       "  matching_tokens_phrases  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# def count_intnt_entits(text):\n",
    "#     doc = nlp(text)\n",
    "#     intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "#     entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "#     return len(intents), len(entities)\n",
    "\n",
    "def count_intnt_entits(text):\n",
    "    if str(text).isnumeric():\n",
    "        return 0,0    \n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "        intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "        entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "    except:\n",
    "        print(text)\n",
    "        raise\n",
    "    return len(intents), len(entities)\n",
    "\n",
    "def extract_ner_entities(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def length_entities(list_entities):\n",
    "    if (list_entities==np.nan or list_entities==None or list_entities==''):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list_entities)\n",
    "    \n",
    "def filter_named_entities(text):\n",
    "    # Process the text using Spacy\n",
    "    doc = nlp(text)\n",
    "    # Filter out named entities (ORG, PERSON, and GPE tags)\n",
    "    filtered_words = [token.text for token in doc if token.ent_type_ not in ['ORG', 'PERSON', 'GPE', \"LOC\", \"FAC\"]]\n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def text_preprocess(dataframe, text_col, category_col):\n",
    "#     # Drop duplicates and remove digits from the text column\n",
    "#     # dataframe = dataframe[[text_col, category_col]].drop_duplicates()\n",
    "#     # dataframe = dataframe[text_col].drop_duplicates()\n",
    "#     print(type(text_col))\n",
    "#     dataframe[text_col] = dataframe[text_col].str.replace('\\d+', '')\n",
    "    \n",
    "#     # Count intents and entities and add new columns\n",
    "#     dataframe[['no_of_intents', 'no_of_entities']] = dataframe.apply(lambda x: pd.Series(count_intnt_entits(x[text_col])), axis=1)  \n",
    "    \n",
    "#     # Extract named entities and add new columns\n",
    "#     dataframe['ner_enities'] = ''\n",
    "#     dataframe.loc[dataframe[text_col]!='', 'ner_enities'] = dataframe.loc[dataframe[text_col]!='', text_col].apply(extract_ner_entities)\n",
    "#     dataframe['len_ner_enities'] = dataframe['ner_enities'].apply(length_entities)\n",
    "#     dataframe3 = dataframe[dataframe['len_ner_enities']>0]\n",
    "#     dataframe3[text_col] = dataframe3[text_col].apply(filter_named_entities)\n",
    "#     dataframe6 = pd.concat([dataframe[dataframe['len_ner_enities']==0], dataframe3], axis = 0)\n",
    "#     dataframe6 = dataframe6.drop(['no_of_intents','no_of_entities','ner_enities','len_ner_enities'], axis=1)\n",
    "\n",
    "#     dataframe6[text_col] = dataframe6[text_col].str.strip()\n",
    "    \n",
    "#     return dataframe6\n",
    "\n",
    "def text_preprocess(dataframe, text_col, category_col):\n",
    "    # Drop duplicates and remove digits from the text column\n",
    "    dataframe = dataframe[[text_col, category_col]].drop_duplicates()\n",
    "    dataframe[text_col] = dataframe[text_col].str.replace('\\d+', '')\n",
    "    \n",
    "    # Count intents and entities and add new columns\n",
    "    dataframe[['no_of_intents', 'no_of_entities']] = dataframe.apply(lambda x: pd.Series(count_intnt_entits(x[text_col])), axis=1)  \n",
    "    \n",
    "    # Extract named entities and add new columns\n",
    "    dataframe['ner_enities'] = ''\n",
    "    dataframe.loc[dataframe[text_col]!='', 'ner_enities'] = dataframe.loc[dataframe[text_col]!='', text_col].apply(extract_ner_entities)\n",
    "    dataframe['len_ner_enities'] = dataframe['ner_enities'].apply(length_entities)\n",
    "    dataframe3 = dataframe[dataframe['len_ner_enities']>0]\n",
    "    dataframe3[text_col] = dataframe3[text_col].apply(filter_named_entities)\n",
    "    dataframe6 = pd.concat([dataframe[dataframe['len_ner_enities']==0], dataframe3], axis = 0)\n",
    "    dataframe6 = dataframe6.drop(['no_of_intents','no_of_entities','ner_enities','len_ner_enities'], axis=1)\n",
    "\n",
    "    dataframe6[text_col] = dataframe6[text_col].str.strip()\n",
    "    \n",
    "    return dataframe6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'labels', 'text', 'category', 'matching_tokens_phrases'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df_combined_cc_model_data_2 = text_preprocess(df_concatenated2, 'text','category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'category'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    109364\n",
       "Other           1734\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_cc_model_data_2.to_excel('final_cc_model_data_v6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_combined_cc_model_data_2 = pd.read_excel('final_cc_model_data_v6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "text          286\n",
       "category        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_cc_model_data_2 = df_combined_cc_model_data_2.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df_combined_cc_model_data_2[['text', 'category']].rename(columns={'category':'label'})\n",
    "dataset = list(data.itertuples(index=False, name=None))\n",
    "\n",
    "# dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# class TextClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels):\n",
    "#         super(TextClassifier, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "#         return logits\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize the dataset\n",
    "# tokenized_dataset = []\n",
    "# for text, label in dataset:\n",
    "#     input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "#     attention_mask = [1] * len(input_ids)\n",
    "#     tokenized_dataset.append((input_ids, attention_mask, label))\n",
    "\n",
    "# # Convert the tokenized dataset into PyTorch tensors\n",
    "# input_ids = pad_sequence([torch.tensor(x[0]) for x in tokenized_dataset], batch_first=True)\n",
    "# attention_mask = pad_sequence([torch.tensor(x[1]) for x in tokenized_dataset], batch_first=True)\n",
    "# labels = torch.tensor([1 if x[2] == \"Elder care\" else 0 for x in tokenized_dataset])\n",
    "\n",
    "# # Define the training parameters\n",
    "# batch_size = 2\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1e-5\n",
    "\n",
    "# # Create the DataLoader\n",
    "# dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define the model, optimizer, and loss function\n",
    "# model = TextClassifier(num_labels=2)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         input_ids, attention_mask, labels = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(input_ids, attention_mask)\n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(\"Epoch {}/{} complete. Loss: {}\".format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-52bf3125669d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Tokenize the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = []\n",
    "for text, label in dataset:\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    tokenized_dataset.append((input_ids, attention_mask, label))\n",
    "\n",
    "# Convert the tokenized dataset into PyTorch tensors\n",
    "input_ids = pad_sequence([torch.tensor(x[0]) for x in tokenized_dataset], batch_first=True)\n",
    "attention_mask = pad_sequence([torch.tensor(x[1]) for x in tokenized_dataset], batch_first=True)\n",
    "labels = torch.tensor([1 if x[2] == \"Child care\" else 0 for x in tokenized_dataset])\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "checkpoint_interval = 1 # save a checkpoint every 2 epochs\n",
    "\n",
    "# Create the DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = TextClassifier(num_labels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}/{} complete. Loss: {}\".format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "    # Save a checkpoint every checkpoint_interval epochs\n",
    "    if (epoch+1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f\"bert_cc_model_checkpoint_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6b62891da681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Resume training from the latest checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlatest_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bert_cc_latest_checkpoint.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Resume training from the latest checkpoint\n",
    "latest_checkpoint = torch.load(os.path.join(checkpoint_dir, 'bert_cc_latest_checkpoint.pth'))\n",
    "model.load_state_dict(latest_checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(latest_checkpoint['optimizer_state_dict'])\n",
    "epoch = latest_checkpoint['epoch'] + 1\n",
    "loss = latest_checkpoint['loss']\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epoch, num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {}/{} complete. Loss: {}\".format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "    # Save a checkpoint after each epoch\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'epoch_{}.pth'.format(epoch)))\n",
    "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'bert_cc_latest_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"Bert_EC_model_v2/ec_model_10_epoch_v2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elder care\n"
     ]
    }
   ],
   "source": [
    "# # Use the trained model to make predictions\n",
    "# input_text = \"My grandmother needs elder care services.\"\n",
    "# input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=True)])\n",
    "# attention_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids[0]]])\n",
    "# logits = model(input_ids, attention_mask)\n",
    "# probs = nn.functional.softmax(logits, dim=-1)\n",
    "# predicted_label = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# # Print the predicted label\n",
    "# if predicted_label == 1:\n",
    "#     print(\"Elder care\")\n",
    "# else:\n",
    "#     print(\"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_intnt_entits(text):\n",
    "    if str(text).isnumeric():\n",
    "        return 0,0    \n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "        intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "        entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "    except:\n",
    "        print(text)\n",
    "        raise\n",
    "    return len(intents), len(entities)\n",
    "\n",
    "def extract_ner_entities(sentence):\n",
    "    doc = nlp(str(sentence))\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def length_entities(list_entities):\n",
    "    if (list_entities==np.nan or list_entities==None or list_entities==''):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list_entities)\n",
    "    \n",
    "def filter_named_entities(text):\n",
    "    # Process the text using Spacy\n",
    "    doc = nlp(str(text))\n",
    "    # Filter out named entities (ORG, PERSON, and GPE tags)\n",
    "    filtered_words = [token.text for token in doc if token.ent_type_ not in ['ORG', 'PERSON', 'GPE', \"LOC\", \"FAC\"]]\n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "def text_preprocess(col):\n",
    "    df = pd.DataFrame({ 'text': col })\n",
    "    df = df.drop_duplicates()\n",
    "    df['text'] = df['text'].str.replace('\\d+', '')\n",
    "    df[['no_of_intents', 'no_of_entities']] = df.apply(lambda x: pd.Series(count_intnt_entits(x['text'])), axis=1)  \n",
    "\n",
    "    df['ner_enities'] = ''\n",
    "    df.loc[df['text']!='', 'ner_enities'] = df.loc[df['text']!='', 'text'].apply(extract_ner_entities)\n",
    "    df['len_ner_enities'] = df['ner_enities'].apply(length_entities)\n",
    "    df3 = df[df['len_ner_enities']>0]\n",
    "    df3['text'] = df3['text'].apply(filter_named_entities)\n",
    "    df6 = pd.concat([df[df['len_ner_enities']==0], df3], axis = 0)\n",
    "    df6 = df6.drop(['no_of_intents','no_of_entities','ner_enities','len_ner_enities'], axis=1)\n",
    "\n",
    "    df6['text'] = df6['text'].str.strip()\n",
    "    \n",
    "    return df6['text'].to_list()\n",
    "\n",
    "def clean_text(text_list):\n",
    "    # Clean the text\n",
    "    text_list = text_preprocess(text_list)\n",
    "    #text_list = [text for text in text_list if text.strip() and not set(text).issubset(set(string.punctuation + string.whitespace))]\n",
    "    text_list1 = []\n",
    "    for text in text_list:\n",
    "        if isinstance(text, str):\n",
    "            if text.strip() and not set(text).issubset(set(string.punctuation + string.whitespace)):\n",
    "                text_list1.append(text)\n",
    "            \n",
    "    text_list = text_list1\n",
    "    \n",
    "    text_list = [x.lower() for x in text_list]\n",
    "    # Define a translation table to replace punctuation and special characters with empty string\n",
    "    translator = str.maketrans(string.punctuation + \"_\", \" \" * len(string.punctuation + \"_\"))\n",
    "    # Loop through each text in the list and clean it\n",
    "    cleaned_list = []\n",
    "    for text in text_list:\n",
    "        # Replace punctuation and special characters with empty string\n",
    "        cleaned_text = text.translate(translator)\n",
    "        # Remove any remaining special characters, punctuation, or whitespaces\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        cleaned_list.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the saved model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.load_state_dict(torch.load(\"Bert_EC_model/ec_model_10_epoch.pth\"))\n",
    "\n",
    "# Load the input Excel file\n",
    "df = pd.read_excel('unseen_data.xlsx')\n",
    "cleaned_text_list = clean_text(df['text'].to_list())\n",
    "# Make predictions for each text in the Excel file\n",
    "predictions = []\n",
    "for text in cleaned_text_list:\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    attention_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids[0]]])\n",
    "    logits = model(input_ids, attention_mask).logits\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_label = torch.argmax(probs, dim=-1)\n",
    "    if predicted_label == 1:\n",
    "        predictions.append((text, 'Elder care', probs[0][1].item()))\n",
    "    else:\n",
    "        predictions.append((text, 'Other', probs[0][0].item()))\n",
    "\n",
    "\n",
    "# Save the predictions to a new file\n",
    "df_pred = pd.DataFrame(predictions, columns=['text', 'prediction', 'probability'])\n",
    "df_pred.to_excel('bert_ec_pred.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/3565708624.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('\\d+', '')\n",
      "/tmp/ipykernel_74027/3565708624.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['text'] = df3['text'].apply(filter_named_entities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>senior citizen care expense reimbursement</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elderly care plus</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aging care home</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>retirees care reimbirsement</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>senior assistance required</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>daycare expense reimbursement</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baby care licensed</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.999976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oldsters care home</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.991685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>contentpage eldercare subsidy</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elder statesmen care</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>elder women care</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>silver generation care</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.996034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>contentpage elder care subsidy</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gerontology care</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.931354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text  prediction  probability\n",
       "0   senior citizen care expense reimbursement  Elder care     0.999973\n",
       "1                           elderly care plus  Elder care     0.999985\n",
       "2                             aging care home  Elder care     0.999983\n",
       "3                 retirees care reimbirsement  Elder care     0.999841\n",
       "4                  senior assistance required  Elder care     0.999977\n",
       "5               daycare expense reimbursement       Other     0.999982\n",
       "6                          baby care licensed       Other     0.999976\n",
       "7                          oldsters care home       Other     0.991685\n",
       "8               contentpage eldercare subsidy  Elder care     0.999982\n",
       "9                        elder statesmen care  Elder care     0.999982\n",
       "10                           elder women care  Elder care     0.999983\n",
       "11                     silver generation care       Other     0.996034\n",
       "12             contentpage elder care subsidy  Elder care     0.999981\n",
       "13                           gerontology care       Other     0.931354"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_texts = [\"senior-citizen?care expense reimbursement\", \n",
    "                 \"elderly*care plus\",\n",
    "                 \"aging care home\", \"retirees care reimbirsement\", \n",
    "                 \"senior assistance required\",\"Daycare@expense reimbursement\",\n",
    "                 \"baby care licensed\", \"oldsters care home\",\"geriatric care home\",\n",
    "                 \"contentPage 2023 Eldercare!!!!!!!!!!!!!****@_Subsidy\",\"Elder statesmen care\",\n",
    "                 \"Elder women care\",\"Silver generation care\",\n",
    "                 \"contentPage {}[]/\\|?><,.;:!@#+\\t\\n\\r\\f\\v 2023 Elder care Subsidy\",\"gerontology care\",\n",
    "                 \"Elderly Care Plus Information\"]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cleaned_text_list = clean_text(list_of_texts)\n",
    "\n",
    "predictions = []\n",
    "for text in cleaned_text_list:\n",
    "    # Skip empty input strings\n",
    "    if not text:\n",
    "        continue\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    attention_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids[0]]])\n",
    "    token_type_ids = torch.tensor([[0] * len(input_ids[0])]) # all tokens belong to the same segment in our case\n",
    "    logits = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).logits\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predicted_label = torch.argmax(probs, dim=-1)\n",
    "    if predicted_label == 1:\n",
    "        predictions.append((text, 'Elder care', probs[0][1].item()))\n",
    "    else:\n",
    "        predictions.append((text, 'Other', probs[0][0].item()))\n",
    "\n",
    "df_pred = pd.DataFrame(predictions, columns=['text', 'prediction', 'probability'])\n",
    "df_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i need a phone number for alight</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>could pension plan selection be changed from j...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>where do i add my bank deposit info</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>virgin pluse</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>why att sent me enroll sheet</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text category\n",
       "0           0                   i need a phone number for alight    Other\n",
       "1           1  could pension plan selection be changed from j...    Other\n",
       "2           2                where do i add my bank deposit info    Other\n",
       "3           3                                       virgin pluse    Other\n",
       "4           4                       why att sent me enroll sheet    Other"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m])]) \u001b[38;5;66;03m# all tokens belong to the same segment in our case\u001b[39;00m\n\u001b[1;32m     20\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[0;32m---> 22\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1841\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m(dim)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.load_state_dict(torch.load(\"Bert_EC_model/ec_model_10_epoch.pth\"))\n",
    "\n",
    "# Load the input Excel file\n",
    "df = pd.read_excel('labelled_unseen_data.xlsx')\n",
    "cleaned_text_list = df['text'].to_list()\n",
    "category_list = df['category'].to_list()\n",
    "\n",
    "# Make predictions for each text in the Excel file\n",
    "predictions = []\n",
    "for i in range(len(cleaned_text_list)):\n",
    "    text = cleaned_text_list[i]\n",
    "    category = category_list[i]\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    attention_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids[0]]])\n",
    "    logits = model(input_ids, attention_mask).logits\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_label = torch.argmax(probs, dim=-1)\n",
    "    if predicted_label == 1:\n",
    "        predictions.append((text, 'Elder care', category, probs[0][1].item()))\n",
    "    else:\n",
    "        predictions.append((text, 'Other', category, probs[0][0].item()))\n",
    "\n",
    "# Save the predictions to a new file\n",
    "df_pred = pd.DataFrame(predictions, columns=['text', 'prediction', 'category', 'probability'])\n",
    "df_pred.to_excel('bert_labelled_ec_pred.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert saved model .bin and config.json to .pkl compatible files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert saved model .bin and config.json to .pkl compatible files\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Load the model configuration from the config.json file\n",
    "config = AutoConfig.from_pretrained('EC_model_outer_combined_texts_data_v6', num_labels=2)\n",
    "\n",
    "# Load the model from the binary file using the configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained('EC_model_outer_combined_texts_data_v6', config=config)\n",
    "\n",
    "# Save the model and configuration as a pickle file\n",
    "with open('EC_model_outer_combined_texts_data_v6/EC_model.pkl', 'wb') as f:\n",
    "    pickle.dump((config, model.state_dict()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tagging the unseen_data for analysisng the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the texts which contain exact phrases from synonyms list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = ['older parent','older people','grand parents','elder','old parents','elder women',\n",
    " 'silver generation','aged people', 'older women','older men','old age home','elder',\n",
    " 'aged',\n",
    " 'elderly people',\n",
    " 'senior assistance',\n",
    " 'aging-in-place',\n",
    " 'aged population',\n",
    " 'golden agers',\n",
    " 'aging in place',\n",
    " 'grey generation',\n",
    " 'silver generation',\n",
    " 'senior health',\n",
    " 'aged population',\n",
    " 'elderly companion',\n",
    " 'golden agers',\n",
    " 'senior citizen',\n",
    " 'elder support',\n",
    " 'elderly',\n",
    " 'senior members',\n",
    " 'elder population',\n",
    " 'elderly residents',\n",
    " 'senior assistance',\n",
    " 'oldsters',\n",
    " 'grey generation',\n",
    " 'aging population',\n",
    " 'elder statesmen',\n",
    " 'elderly',\n",
    " 'elderly people',\n",
    " 'aging',\n",
    " 'elderly residents',\n",
    " 'elder',\n",
    " 'elder women',\n",
    " 'senior',\n",
    " 'elder generation',\n",
    " 'gerontology',\n",
    " 'elderly population',\n",
    " 'senior members',\n",
    " 'retirees',\n",
    " 'elderly population',\n",
    " 'eldercare',\n",
    " 'geriatric',\n",
    " 'elder statesmen',\n",
    " 'age related',\n",
    " 'retirees',\n",
    " 'third age population',\n",
    " 'aging population',\n",
    " 'elder population',\n",
    " 'oldsters',\n",
    " 'third age population','eldercae', 'eldercarr', 'eldermann', \n",
    "'aged home','eldercre','eldery','elderman','elders','eldercrae',]\n",
    "words_4 = list(set([word.lower() for word in words_3]))\n",
    "len(words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcomeuserfollowup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medical plan credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welcomeuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>general purpose loans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chiropractor visits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text\n",
       "0    welcomeuserfollowup\n",
       "1    medical plan credit\n",
       "2            welcomeuser\n",
       "3  general purpose loans\n",
       "4    chiropractor visits"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.DataFrame()\n",
    "df_cleaned['text'] =cleaned_text_list\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/3427755096.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = (df_cleaned['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcomeuserfollowup</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medical plan credit</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welcomeuser</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>general purpose loans</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chiropractor visits</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text category\n",
       "0    welcomeuserfollowup         \n",
       "1    medical plan credit         \n",
       "2            welcomeuser         \n",
       "3  general purpose loans         \n",
       "4    chiropractor visits         "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (df_cleaned['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n",
    "\n",
    "df_cleaned['category'] = ''\n",
    "df_cleaned.loc[mask, 'category'] = 'Elder care'\n",
    "# df_combined_web_iva_search.loc[df_combined_web_iva_search['category'] == '', 'category'] = 'Other'\n",
    "\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>ongoing elderly care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      text    category\n",
       "5666  ongoing elderly care  Elder care"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned['category']=='Elder care'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned['category']=='Elder care'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get text which are similar to phrases in synonnyms list for texts other than which are filtered above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labelling unseen data for matrix after prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_sentences(df, sentences, phrases, threshold=0.95, category_name = 'Elder care'):\n",
    "    # encode the phrases using the model\n",
    "    phrase_embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "    \n",
    "    # initialize an empty list to store the similar sentences\n",
    "    similar_sentences = []\n",
    "    \n",
    "    # iterate over the sentences\n",
    "    for sentence in sentences:\n",
    "        # encode the sentence using the model\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "        # reshape the sentence embedding to a 2D array\n",
    "        sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "        \n",
    "        # calculate the cosine similarity between the sentence embedding and each phrase embedding\n",
    "        cosine_scores = 1 - cosine_distances(sentence_embedding, phrase_embeddings)\n",
    "        \n",
    "        # convert the cosine similarity scores to a list\n",
    "        scores_list = cosine_scores.tolist()[0]\n",
    "        \n",
    "        # iterate over the phrases and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "        for phrase, score in zip(phrases, scores_list):\n",
    "            if score >= threshold:\n",
    "                similar_sentences.append(sentence)\n",
    "                break\n",
    "    \n",
    "    # convert the list of similar sentences to a set to remove duplicates\n",
    "    similar_sentences = set(similar_sentences)\n",
    "    \n",
    "    # create a new dataframe containing only the rows with text that is in the set of similar sentences\n",
    "    similar_df = df[df['text'].isin(similar_sentences)]\n",
    "    similar_df['category']=category_name\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/2749835889.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_df['category']=category_name\n"
     ]
    }
   ],
   "source": [
    "similar_df_unseen = find_similar_sentences(df_cleaned, \n",
    "                                    df_cleaned[df_cleaned['category']=='']['text'].to_list(), \n",
    "                                    words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>retiremen</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text    category\n",
       "7095  retiremen  Elder care"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_EC_df = pd.concat([df_cleaned[df_cleaned['category']=='Elder care'], \n",
    "                       similar_df_unseen]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_EC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_EC_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_EC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/3802977147.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_similar_unseen_df['category'] = 'Other'\n"
     ]
    }
   ],
   "source": [
    "# filter out the rows with similar text from the original DataFrame\n",
    "non_similar_unseen_df = df_cleaned[~df_cleaned['text'].isin(unseen_EC_df['text'])]\n",
    "\n",
    "# sample twice as many rows from the non-similar DataFrame as there are in the similar DataFrame\n",
    "# non_similar_df = non_similar_df.sample(n=only_EC_df.shape[0]*2)\n",
    "non_similar_unseen_df['category'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen_concatenated = pd.concat([unseen_EC_df, non_similar_unseen_df]).sample(frac=1).reset_index(drop=True)\n",
    "# df_concatenated = df_concatenated\n",
    "# df_concatenated.drop(columns=['input', 'search_text', 'page_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12584, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unseen_concatenated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unseen_concatenated[(df_unseen_concatenated.text.str.contains('elder')) & (df_unseen_concatenated.category=='Other')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unseen_concatenated.to_excel('labelled_unseen_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
