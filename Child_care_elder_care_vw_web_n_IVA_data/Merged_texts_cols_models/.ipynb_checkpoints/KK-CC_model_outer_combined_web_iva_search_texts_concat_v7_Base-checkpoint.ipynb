{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers --index-url=https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple --trusted-host=artifactory.alight.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install en_core_web_sm-3.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (0,1,2,4,5,9,10,11,12,13,14,15,16,17,18,20,21,23,24,25,26,27,28,29,31,33,39,41,56,57,58,59,63,64,65,67,68,70,71,74,75,77,78,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,106,108,109,110,111,112,113,114,115,116,117,118,119,122,123,124,125,126,127,128,129,130,131,132,134,135,136,137,138,155,156,159,160,161,164,165) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_combined_web_iva_search = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/IVA/combined_new_adult-child_outer_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_combined_web_iva_search.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>unit_name</th>\n",
       "      <th>next_unit_hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child a dependent?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Benefits Eligibility For Stepchildren</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i change my dependent daycare deduction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adding or Removing a Dependent _CORE_</td>\n",
       "      <td>Flexible Spending Account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child last name change</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Updating Legal Name</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am trying to add my children as beneficiarie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life Insurance Beneficiaries</td>\n",
       "      <td>Website Issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how much timeoff do i get after the birth of m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paid Time Off Accrual</td>\n",
       "      <td>Contact Us Page - Web Link Only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input search_text page_name  \\\n",
       "0                            step child a dependent?         NaN       NaN   \n",
       "1     how do i change my dependent daycare deduction         NaN       NaN   \n",
       "2                             Child last name change         NaN       NaN   \n",
       "3  i am trying to add my children as beneficiarie...         NaN       NaN   \n",
       "4  how much timeoff do i get after the birth of m...         NaN       NaN   \n",
       "\n",
       "                               unit_name                    next_unit_hit  \n",
       "0  Benefits Eligibility For Stepchildren                              NaN  \n",
       "1  Adding or Removing a Dependent _CORE_        Flexible Spending Account  \n",
       "2                    Updating Legal Name                              NaN  \n",
       "3           Life Insurance Beneficiaries                   Website Issues  \n",
       "4                  Paid Time Off Accrual  Contact Us Page - Web Link Only  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_web_iva_search[['input','search_text','page_name','unit_name','next_unit_hit']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/Deepali/IVA_cleaned_labelled(session_id_added).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva1 = df_iva.drop(['Unnamed: 0','entry_id','client_id','person_internal_id','next_unit_hit',\n",
    "             'previous_unit_hit','response_text','session_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva2 = df_iva1.drop(['input_cleaned','input_cleaned_dl'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva3 = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/Deepali/IVA_cleaned_labelled_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva3 = df_iva3.rename(columns={'input_orig':'input_orig_1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_iva3 = df_iva3.drop(['client_id','person_internal_id','session_id','response_text','unit_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva3['input_orig_1'].head(10).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the 'col' column using '|' delimiter\n",
    "df_iva3['input_orig_1'] = df_iva3['input_orig_1'].str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explode the 'col' column to expand it row-wise\n",
    "df_iva3 = df_iva3.explode('input_orig_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_iva3 = df_iva3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_combined_web_iva_search, df_iva2, df_iva3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat2 = df_concat[['input','input_orig_1','input_orig','search_text','page_name','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the columns to stack\n",
    "cols_to_stack = ['input', 'input_orig_1', 'input_orig']\n",
    "\n",
    "# stack the columns using melt\n",
    "stacked = pd.melt(df_concat2, id_vars=['search_text', 'page_name', 'labels'], \n",
    "                  value_vars=cols_to_stack, var_name='stacked_cols', value_name='stacked_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat2 = df_concat2.rename(columns={'input_orig':'input_orig_1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use melt() to stack columns 'A', 'B', and 'C' into a single column 'value'\n",
    "# df_concat2 = pd.melt(df_concat2, value_vars=['input', 'input_orig', 'input_orig'], var_name='column', value_name='stacked_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stacked_concat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.dropna(subset=['search_text','page_name','stacked_input'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked = stacked.drop(['stacked_cols'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append('../../nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(df, text_cols):\n",
    "    # Create a new dataframe to hold the cleaned text columns\n",
    "    cleaned_df = pd.DataFrame()\n",
    "    \n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean each text column and add it to the cleaned dataframe\n",
    "    for text_col in text_cols:\n",
    "        text_list = df[text_col].tolist()\n",
    "        text_list = [str(text) for text in text_list]\n",
    "        text_list = [text if text.strip() and not\n",
    "                     set(text).issubset(set(string.punctuation + string.whitespace)) else '' \n",
    "                     for text in text_list]\n",
    "        text_list = [x.lower() for x in text_list]\n",
    "        translator = str.maketrans(string.punctuation + string.digits + \"_\", \" \" * len(\n",
    "            string.punctuation + string.digits + \"_\"))\n",
    "        cleaned_list = []\n",
    "        for text in text_list:\n",
    "            cleaned_text = text.translate(translator)\n",
    "            cleaned_text = ' '.join(cleaned_text.split())\n",
    "            cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "            cleaned_list.append(cleaned_text)\n",
    "        cleaned_df[text_col] = cleaned_list\n",
    "    \n",
    "    # Add the non-text columns to the cleaned dataframe\n",
    "    for col in df.columns:\n",
    "        if col not in text_cols:\n",
    "            cleaned_df[col] = df[col]\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat3 = clean_text(stacked, text_cols=['stacked_input','search_text','page_name'])\n",
    "# cleaned_stacked, removed_rows = clean_text(stacked, text_cols=['stacked_input','search_text','page_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4 = df_concat3.replace('nan', '')\n",
    "df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4['text'] = df_concat4[['stacked_input', 'search_text', 'page_name']].apply(lambda x: ' '.join([str(i) for i in x if not pd.isna(i)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop(['stacked_input','search_text','page_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat4.drop(['input_orig'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the value counts of the 'labels' column\n",
    "label_counts = df_concat4['labels'].value_counts()\n",
    "print(sum(label_counts>500))\n",
    "# Filter the dataframe to only include rows where the label count is greater than 10000\n",
    "df_concat5 = df_concat4[df_concat4['labels'].isin(label_counts[label_counts > 10000].index)]\n",
    "\n",
    "# Get the shape of the resulting filtered dataframe\n",
    "df_concat5_shape = df_concat5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat5_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat5['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4[df_concat4['text']=='nan '].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.to_parquet('cc_df_concat4.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the texts which contain exact phrases from synonyms list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# words_3 = ['day care', 'creche', 'childcare','daycare','nurseryschool','after school care',\n",
    "#            'pre school', 'child', 'baby', 'minor care','minorcare','offspring',\n",
    "#            'daynursery', 'infantschool','pre K', 'childcarer', 'childcarers','girl child',\n",
    "#  'infantcare', 'play school', 'playgroup','boy child','Adolescent','Little one','Young one',\n",
    "#  'nursery', 'nursery school', 'preschool', 'after schoolcare', 'day nursery', 'infant school', \n",
    "#  'infant care', 'playschool', 'play group', 'kindergarten', 'childminding', 'babysitting',\n",
    "#            'babysitter', \n",
    "#  'nanny care', 'children supervision', 'toddler care', 'prekindergarten',\n",
    "#            'Junior care','Sprout care',\n",
    "# 'child minding', 'baby sitting', 'baby sitter', 'children',\n",
    "#  'nannycare', 'childrensupervision','child supervision', 'childsupervision', \n",
    "#            'toddlercare','foster', 'childs','childrens',\n",
    "#            'stepchild','step daughter', 'step son', 'grandchildren','grandchild',\n",
    "#           'stepchildren','childbirth',\"children's\", 'childhood',\n",
    "#           'childsupport','childcarereimbursement','mychildren',\n",
    "#            'mychild','childcareplus','childcarea','stepchildren',\n",
    "#            'childbirthing','dependentchild','childern',\n",
    "#           'childcard','day cares','childrent','daycares',\n",
    "#           'childplus','childbonding']\n",
    "# words_4 = list(set([word.lower() for word in words_3]))\n",
    "\n",
    "# len(words_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = ['day care', 'creche', 'childcare','daycare','after school care',\n",
    "          'child', 'baby',\n",
    "            'infant','girl child', 'play school', \n",
    "           'boy child','Adolescent',\n",
    " 'nursery', 'preschool', 'after schoolcare', \n",
    "  'playschool', 'kindergarten', 'childminding', 'babysitting',\n",
    "           'babysitter', \n",
    "'children supervision', 'toddler care', 'kindergarten',\n",
    "'child minding', 'baby sitting', 'baby sitter', 'children',\n",
    " 'childrensupervision','child supervision', 'childsupervision', 'children supervision',\n",
    "           'toddlercare', 'childs','childrens',\n",
    "           'stepchild','step daughter', 'step son', 'grandchildren','grandchild','daughter','son', \n",
    "          'stepchildren',\"children's\", 'childhood',\n",
    "           'stepchildren',\n",
    "          'childrent','daycares',\n",
    "          'childplus',]\n",
    "words_4 = list(set([word.lower() for word in words_3]))\n",
    "\n",
    "len(words_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4 = pd.read_parquet('cc_df_concat4.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>step child dependent</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRA</td>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>child last name change</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSA related</td>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>much timeoff get birth child</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 labels                                          text  \\\n",
       "0            Enrollment                        step child dependent     \n",
       "1                   HRA          change dependent daycare deduction     \n",
       "2       Discounts Issue                      child last name change     \n",
       "3           HSA related  trying add children beneficiaries life ins     \n",
       "4  General Acount issue                much timeoff get birth child     \n",
       "\n",
       "     category  \n",
       "0  Child care  \n",
       "1  Child care  \n",
       "2  Child care  \n",
       "3  Child care  \n",
       "4  Child care  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (df_concat4['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n",
    "\n",
    "df_concat4['category'] = ''\n",
    "df_concat4.loc[mask, 'category'] = 'Child care'\n",
    "# df_combined_web_iva_search.loc[df_combined_web_iva_search['category'] == '', 'category'] = 'Other'\n",
    "\n",
    "df_concat4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat4[df_concat4['category']=='Child care'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277431, 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4[df_concat4['category']=='Child care'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    1277431\n",
       "              1097777\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get text which are similar to phrases in synonnyms list for texts other than which are filtered above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sklearn.metrics.pairwise import cosine_distances\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# def find_similar_sentences(df, sentences, phrases, model, threshold=0.95, category_name='Child care', device='cuda', batch_size=32):\n",
    "#     # set the device\n",
    "#     device = torch.device(device)\n",
    "\n",
    "#     # move the model to the device\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # encode the phrases using the model\n",
    "#     phrase_embeddings = model.encode(phrases, convert_to_tensor=True, device=device)\n",
    "\n",
    "#     # initialize empty lists to store the similar sentences and phrases\n",
    "#     similar_sentences = []\n",
    "#     similar_phrases = []\n",
    "\n",
    "#     # make sure the sentences input is in the correct format\n",
    "#     if isinstance(sentences, str):\n",
    "#         sentences = [sentences]\n",
    "\n",
    "#     # split the sentences into batches\n",
    "#     batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "#     # iterate over the batches\n",
    "#     for batch_sentences in batches:\n",
    "#         # encode the batch of sentences using the model\n",
    "#         sentence_embeddings = model.encode(batch_sentences, convert_to_tensor=True, device=device)\n",
    "\n",
    "#         # calculate the cosine similarity between the sentence embeddings and the phrase embeddings\n",
    "#         cosine_scores = cosine_similarity(sentence_embeddings.cpu().numpy(), phrase_embeddings.cpu().numpy())\n",
    "\n",
    "#         # iterate over the batch of sentences and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "#         for j, sentence in enumerate(batch_sentences):\n",
    "#             scores_list = cosine_scores[j]\n",
    "\n",
    "#             # iterate over the phrases and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "#             for phrase, score in zip(phrases, scores_list):\n",
    "#                 if score >= threshold:\n",
    "#                     similar_sentences.append(sentence)\n",
    "#                     similar_phrases.append(phrase)\n",
    "#                     break\n",
    "\n",
    "#     # create a new dataframe containing only the rows with text that is in the set of similar sentences\n",
    "#     similar_df = df[df['text'].isin(similar_sentences)]\n",
    "#     similar_df['synonym_phrase'] = similar_phrases\n",
    "#     similar_df['category'] = category_name\n",
    "#     return similar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similarity(sentence1, sentence2):\n",
    "    # Load the SBERT model\n",
    "    # model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "    # Encode the sentences and calculate embeddings\n",
    "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
    "\n",
    "    # Move the embeddings to the CPU and convert to numpy arrays\n",
    "    embeddings_cpu = embeddings.cpu().numpy()\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embeddings_cpu[0].reshape(1, -1), embeddings_cpu[1].reshape(1, -1))\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54044855]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_similarity(\"mom\", \"daughter\")\n",
    "get_similarity(\"parental\", \"daughter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "def find_similar_sentences(df, sentences, phrases, model, threshold=0.85, category_name='Child care', device='cuda', batch_size=32):\n",
    "    # Set the device\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Encode the phrases using the model\n",
    "    phrase_embeddings = model.encode(phrases, convert_to_tensor=True, device=device)\n",
    "\n",
    "    # Initialize empty lists to store the similar sentences and phrases\n",
    "    similar_sentences = []\n",
    "    similar_phrases = []\n",
    "\n",
    "    # Make sure the sentences input is in the correct format\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "\n",
    "    # Split the sentences into batches\n",
    "    batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the batches\n",
    "        for batch_sentences in batches:\n",
    "            # Encode the batch of sentences using the model\n",
    "            sentence_embeddings = model.encode(batch_sentences, convert_to_tensor=True, device=device)\n",
    "\n",
    "            # Calculate the cosine similarity between the sentence embeddings and the phrase embeddings\n",
    "            cosine_scores = torch.nn.functional.cosine_similarity(sentence_embeddings.unsqueeze(1), phrase_embeddings.unsqueeze(0), dim=-1)\n",
    "\n",
    "            # Iterate over the batch of sentences and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "            for j, scores_list in enumerate(cosine_scores):\n",
    "                mask = scores_list >= threshold\n",
    "                if mask.any():\n",
    "                    similar_sentences.extend([batch_sentences[j]] * mask.sum().item())\n",
    "                    similar_phrases.extend([phrases[i] for i in torch.nonzero(mask).flatten()])\n",
    "\n",
    "    # Remove duplicates from similar_sentences and similar_phrases while preserving order\n",
    "    seen_sentences = set()\n",
    "    seen_phrases = set()\n",
    "    unique_sentences = []\n",
    "    unique_phrases = []\n",
    "    for sentence, phrase in zip(similar_sentences, similar_phrases):\n",
    "        if sentence not in seen_sentences:\n",
    "            seen_sentences.add(sentence)\n",
    "            seen_phrases.add(phrase)\n",
    "            unique_sentences.append(sentence)\n",
    "            unique_phrases.append(phrase)\n",
    "\n",
    "    # Create a new DataFrame containing only the rows with text that is in the set of similar sentences\n",
    "    similar_df = df[df['text'].isin(unique_sentences)]\n",
    "    similar_df['synonym_phrase'] = unique_phrases\n",
    "    similar_df['category'] = category_name\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import concurrent.futures\n",
    "\n",
    "# def find_similar_sentences(df, sentences, phrases, model, threshold=0.73, category_name='Child care', device='cpu', batch_size=32):\n",
    "#     # Set the device\n",
    "#     device = torch.device(device)\n",
    "\n",
    "#     # Move the model to the device\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # Encode the phrases using the model\n",
    "#     phrase_embeddings = model.encode(phrases, convert_to_tensor=False)\n",
    "\n",
    "#     # Initialize empty lists to store the similar sentences and phrases\n",
    "#     similar_sentences = []\n",
    "#     similar_phrases = []\n",
    "\n",
    "#     # Make sure the sentences input is in the correct format\n",
    "#     if isinstance(sentences, str):\n",
    "#         sentences = [sentences]\n",
    "\n",
    "#     # Split the sentences into batches\n",
    "#     batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "#     with torch.no_grad(), concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#         # Function to process each batch of sentences\n",
    "#         def process_batch(batch_sentences):\n",
    "#             # Encode the batch of sentences using the model\n",
    "#             sentence_embeddings = model.encode(batch_sentences, convert_to_tensor=False)\n",
    "\n",
    "#             # Calculate the cosine similarity between the sentence embeddings and the phrase embeddings\n",
    "#             cosine_scores = torch.nn.functional.cosine_similarity(torch.tensor(sentence_embeddings, device=device).unsqueeze(1), torch.tensor(phrase_embeddings, device=device).unsqueeze(0), dim=-1)\n",
    "\n",
    "#             # Iterate over the batch of sentences and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "#             similar_sentences_batch = []\n",
    "#             similar_phrases_batch = []\n",
    "#             for j, scores_list in enumerate(cosine_scores):\n",
    "#                 mask = scores_list >= threshold\n",
    "#                 if mask.any():\n",
    "#                     similar_sentences_batch.extend([batch_sentences[j]] * mask.sum().item())\n",
    "#                     similar_phrases_batch.extend([phrases[i] for i in torch.nonzero(mask).flatten()])\n",
    "\n",
    "#             return similar_sentences_batch, similar_phrases_batch\n",
    "\n",
    "#         # Process each batch in parallel\n",
    "#         results = executor.map(process_batch, batches)\n",
    "\n",
    "#         # Collect the results\n",
    "#         for result in results:\n",
    "#             similar_sentences.extend(result[0])\n",
    "#             similar_phrases.extend(result[1])\n",
    "\n",
    "#     # Remove duplicates from similar_sentences and similar_phrases while preserving order\n",
    "#     seen_sentences = set()\n",
    "#     seen_phrases = set()\n",
    "#     unique_sentences = []\n",
    "#     unique_phrases = []\n",
    "#     for sentence, phrase in zip(similar_sentences, similar_phrases):\n",
    "#         if sentence not in seen_sentences:\n",
    "#             seen_sentences.add(sentence)\n",
    "#             seen_phrases.add(phrase)\n",
    "#             unique_sentences.append(sentence)\n",
    "#             unique_phrases.append(phrase)\n",
    "\n",
    "#     # Create a new DataFrame containing only the rows with text that is in the set of similar sentences\n",
    "#     mask = np.isin(df['text'], unique_sentences)\n",
    "#     similar_df = df[mask].copy()\n",
    "#     similar_df['synonym_phrase'] = unique_phrases\n",
    "#     similar_df['category'] = category_name\n",
    "#     return similar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587.0794568061829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# find_similar_sentences(df, sentences, phrases, model, threshold=0.73, category_name='Child care', device='cuda', batch_size=32)\n",
    "similar_df = find_similar_sentences(\n",
    "    df_concat4,\n",
    "    df_concat4[df_concat4['category']=='']['text'].to_list(),\n",
    "    words_4,\n",
    "    model,\n",
    "    threshold=0.85,\n",
    "    category_name='Child care',\n",
    "    device='cuda',\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>synonym_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>Other</td>\n",
       "      <td>childern</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>None</td>\n",
       "      <td>childre</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>None</td>\n",
       "      <td>childcar eplus documentation</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childsupervision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>Doctor</td>\n",
       "      <td>childnre</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21645</th>\n",
       "      <td>None</td>\n",
       "      <td>get childcareplus</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611014</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>compare coverage options daycaree</td>\n",
       "      <td>Child care</td>\n",
       "      <td>daycare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779893</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>wife eleginle</td>\n",
       "      <td>Child care</td>\n",
       "      <td>daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035972</th>\n",
       "      <td>None</td>\n",
       "      <td>kids plan</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childrensupervision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216578</th>\n",
       "      <td>None</td>\n",
       "      <td>assistance parental care</td>\n",
       "      <td>Child care</td>\n",
       "      <td>childcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292013</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>wife aileen</td>\n",
       "      <td>Child care</td>\n",
       "      <td>daughter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  labels                                text    category  \\\n",
       "4710               Other                          childern    Child care   \n",
       "5101                None                           childre    Child care   \n",
       "8655                None      childcar eplus documentation    Child care   \n",
       "10995             Doctor                          childnre    Child care   \n",
       "21645               None                 get childcareplus    Child care   \n",
       "...                  ...                                 ...         ...   \n",
       "3611014  Health Benefits  compare coverage options daycaree   Child care   \n",
       "3779893       Enrollment                     wife eleginle    Child care   \n",
       "4035972             None                         kids plan    Child care   \n",
       "4216578             None          assistance parental care    Child care   \n",
       "4292013       Enrollment                       wife aileen    Child care   \n",
       "\n",
       "              synonym_phrase  \n",
       "4710               childrent  \n",
       "5101               childrent  \n",
       "8655        childsupervision  \n",
       "10995              childrent  \n",
       "21645              childcare  \n",
       "...                      ...  \n",
       "3611014              daycare  \n",
       "3779893             daughter  \n",
       "4035972  childrensupervision  \n",
       "4216578            childcare  \n",
       "4292013             daughter  \n",
       "\n",
       "[218 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# similar_df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "similar_df.to_csv('cc_similar_df_85%_sim_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5889308]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity(\"wife\", \"daughter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    23772\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_CC_df = pd.concat([df_concat4[df_concat4['category']=='Child care'], \n",
    "                       similar_df]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1301279, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_CC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_CC_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1301279, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_CC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "non_similar_df = df_concat4[~df_concat4['text'].isin(only_CC_df['text'])]\n",
    "\n",
    "non_similar_df['category'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated = pd.concat([only_CC_df, non_similar_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2375208, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    1301279\n",
       "Other         1073929\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_cc_under_sampled = df_concatenated[df_concatenated['category']=='Child care'].sample(\n",
    "#     df_concatenated[df_concatenated['category']=='Other'].shape[0]*(100//25), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_cc_other_final = pd.concat([df_cc_under_sampled, df_other]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concatenated[(df_concatenated.text.str.contains('child')) & (df_concatenated.category=='Other')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concatenated[df_concatenated.category=='Elder care'].to_excel('fasttext_only_elder_care_training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## number og labels in label col where value of category col is Other\n",
    "# df_concatenated[df_concatenated['category'] == 'Other']['labels'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_cc_other_final = df_cc_other_final.drop('session_start_cst', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1337892"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated['labels'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated['category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concatenated['labels'] = df_concatenated['labels'].fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EC_model_train_df = pd.concat([df_concatenated2[df_concatenated2.category=='Elder care'], \n",
    "#                               Other_sample_df]).sample(frac=1).reset_index(drop=True)#.to_excel('fasttext_EC_model_training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EC_model_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    0.547859\n",
       "Other         0.452141\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated.category.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2375208, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated.to_csv('cc_df_concatenated_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated = pd.read_csv('cc_df_concatenated_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def count_intnt_entits(doc):\n",
    "    intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "    entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "    return len(intents), len(entities)\n",
    "\n",
    "def extract_ner_entities(doc, sentence):\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def length_entities(list_entities):\n",
    "    if (list_entities == np.nan or list_entities is None or list_entities == ''):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list_entities)\n",
    "\n",
    "def filter_named_entities(doc, text):\n",
    "    filtered_words = [token.text for token in doc if token.ent_type_ not in ['ORG', 'PERSON', 'GPE', 'LOC', 'FAC']]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    intents, entities = count_intnt_entits(doc)\n",
    "    ner_entities = extract_ner_entities(doc, text)\n",
    "    len_ner_entities = length_entities(ner_entities)\n",
    "    filtered_text = filter_named_entities(doc, text)\n",
    "    return intents, entities, ner_entities, len_ner_entities, filtered_text\n",
    "\n",
    "def text_preprocess(dataframe):\n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "    texts = dataframe['text'].tolist()\n",
    "    with ProcessPoolExecutor(max_workers=96) as executor:\n",
    "        processed_results = list(executor.map(process_text, texts))\n",
    "\n",
    "    intents, entities, ner_entities, len_ner_entities, filtered_texts = zip(*processed_results)\n",
    "    \n",
    "    dataframe['no_of_intents'] = intents\n",
    "    dataframe['no_of_entities'] = entities\n",
    "    dataframe['ner_enities'] = ner_entities\n",
    "    dataframe['len_ner_enities'] = len_ner_entities\n",
    "    dataframe['text'] = filtered_texts\n",
    "    \n",
    "    dataframe = dataframe[dataframe['len_ner_enities'] > 0]\n",
    "    dataframe['text'] = dataframe['text'].str.strip()\n",
    "    dataframe = dataframe.drop(['no_of_intents', 'no_of_entities', 'ner_enities', 'len_ner_enities'], axis=1)\n",
    "    \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160.2946898937225\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df_combined_cc_model_data_2 = text_preprocess(df_concatenated)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350743, 5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'labels', 'text', 'category', 'synonym_phrase'], dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Child care    183177\n",
       "Other         167566\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_cc_model_data_2.to_parquet('final_cc_model_data_v8.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_unseen = pd.read_excel('unseen_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243954</th>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738171</th>\n",
       "      <td>flex card child care subsidy contentpage child...</td>\n",
       "      <td>Child care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       label\n",
       "243954                                                          Other\n",
       "738171  flex card child care subsidy contentpage child...  Child care"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_combined_cc_model_data_2 = pd.read_parquet('final_cc_model_data_v8.parq')[['text', 'category']].rename(\n",
    "    columns={'category':'label'})\n",
    "df_combined_cc_model_data_2.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_cc_model_data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_cc_model_data_2 = df_combined_cc_model_data_2.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df_combined_cc_model_data_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load your dataset\n",
    "texts = data.text.values  # List of text samples\n",
    "labels =  data.label.values  # List of corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Step 1: Install necessary libraries\n",
    "# !pip install sentence-transformers\n",
    "# !pip install lightgbm\n",
    "# !pip install joblib\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 3: Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-0eb986c11547>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-0eb986c11547>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Step 1: Install necessary libraries\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# # Step 4: Prepare training data\n",
    "# texts = [\"This is the first text.\", \"Another example text.\", ...]\n",
    "# labels = [0, 1, ...]\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, \n",
    "                                                                      test_size=0.005, \n",
    "                                                                      random_state=42)\n",
    "\n",
    "# Step 6: Encode the text samples\n",
    "train_encodings = model.encode(train_texts)\n",
    "test_encodings = model.encode(test_texts)\n",
    "\n",
    "# Step 7: Train a classifier with LightGBM\n",
    "classifier = lgb.LGBMClassifier()\n",
    "classifier.fit(train_encodings, train_labels)\n",
    "\n",
    "# Step 8: Make predictions and evaluate the model\n",
    "predictions = classifier.predict(test_encodings)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "test_df = pd.DataFrame({'text':test_texts, 'actual_label':test_labels, \n",
    "                        'predicted_label':predictions})\n",
    "test_df.to_csv('cc_test_df.csv')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 9: Save the model\n",
    "joblib.dump(classifier, 'Bert_CC_model/cc_model_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Step 15: Generate the confusion matrix\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "label_names = ['Child care', 'Other']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "test_labels_numeric = np.where(np.array(test_labels) == 'Child care', 0, 1)\n",
    "predictions_numeric = np.where(np.array(predictions) == 'Child care', 0, 1)\n",
    "\n",
    "# Calculate additional metrics\n",
    "classification_metrics = classification_report(test_labels_numeric, predictions_numeric, target_names=label_names, output_dict=True)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels_numeric, predictions_numeric)\n",
    "auc_score = roc_auc_score(test_labels_numeric, predictions_numeric)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "for label in label_names:\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Precision: {classification_metrics[label]['precision']}\")\n",
    "    print(f\"  Recall: {classification_metrics[label]['recall']}\")\n",
    "    print(f\"  F1-Score: {classification_metrics[label]['f1-score']}\")\n",
    "    print(f\"  Support: {classification_metrics[label]['support']}\")\n",
    "\n",
    "# Print additional metrics\n",
    "print(\"F1 Score:\", classification_metrics['weighted avg']['f1-score'])\n",
    "print(\"Accuracy:\", classification_metrics['accuracy'])\n",
    "print(\"Precision:\", classification_metrics['weighted avg']['precision'])\n",
    "print(\"Recall:\", classification_metrics['weighted avg']['recall'])\n",
    "print(\"AUC:\", auc_score)\n",
    "\n",
    "# Step 16: Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   person_internal_id  client_id  \\\n",
      "0            84580040       3349   \n",
      "1           261800048       1012   \n",
      "2           189230015       6352   \n",
      "3           412710098        695   \n",
      "4           412710098        695   \n",
      "\n",
      "                                               input  \\\n",
      "0                                      pet insurance   \n",
      "1  how can I find Cigna human resources contact i...   \n",
      "2                            Associate Discount Card   \n",
      "3                                          Tax forms   \n",
      "4                                            HSA tax   \n",
      "\n",
      "                             unit_name    session_start_utc  session_id  \\\n",
      "0           Coverage for Pet Insurance  2023-04-03 00:06:54    19429827   \n",
      "1  Medical Carrier Contact Information  2023-04-03 00:09:11    19429853   \n",
      "2     Employee/Associate Discount Card  2023-04-03 00:16:57    19429937   \n",
      "3                  Tax Forms Clarifier  2023-04-03 00:18:18    19429950   \n",
      "4                          Tying Units  2023-04-03 00:18:18    19429950   \n",
      "\n",
      "   entry_id  entry_order  \n",
      "0  89662494            5  \n",
      "1  89662690            5  \n",
      "2  89662993            3  \n",
      "3  89663051            6  \n",
      "4  89663066            7  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append('../../nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify the S3 bucket and prefix where the Parquet files are stored\n",
    "# s3://adl-core-sagemaker-studio/external/IVA/IVA_daily/\n",
    "# s3://adl-core-sagemaker-studio/external/IVA/Search_daily/\n",
    "# s3://adl-core-sagemaker-studio/external/Deepali/iva-data(3-apr).csv\n",
    "bucket_name = 'adl-core-sagemaker-studio'\n",
    "# prefix = 'external/web_clickstream/clickstream20230403_20230403/'\n",
    "prefix = 'external/Deepali/iva_data/'\n",
    "\n",
    "# List all Parquet files in the bucket with the specified prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "if 'Contents' in response:\n",
    "    # parquet_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parq')]\n",
    "    csv_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.csv')]\n",
    "else:\n",
    "    # parquet_files = []\n",
    "    csv_files = []\n",
    "\n",
    "# Read Parquet files and concatenate them together\n",
    "dfs = []\n",
    "s3fs = s3fs.S3FileSystem()\n",
    "\n",
    "# for file in parquet_files:\n",
    "for file in csv_files:\n",
    "    # Read the Parquet file into a PyArrow table\n",
    "    s3_key = f\"{bucket_name}/{file}\"\n",
    "    # dataset = pq.ParquetDataset(f\"s3://{s3_key}\", filesystem=s3fs)\n",
    "    df = pd.read_csv(f\"s3://{s3_key}\")\n",
    "    # table = dataset.read()\n",
    "    \n",
    "    # Convert the PyArrow table to a Pandas DataFrame\n",
    "    # df = table.to_pandas()\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames together\n",
    "if dfs:\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(concatenated_df.head())\n",
    "else:\n",
    "    # print(\"No Parquet files found.\")\n",
    "    print(\"No csv files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54871, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(df, text_cols):\n",
    "    # Create a new dataframe to hold the cleaned text columns\n",
    "    cleaned_df = pd.DataFrame()\n",
    "    \n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean each text column and add it to the cleaned dataframe\n",
    "    for text_col in text_cols:\n",
    "        text_list = df[text_col].tolist()\n",
    "        text_list = [str(text) for text in text_list]\n",
    "        text_list = [text if text.strip() and not\n",
    "                     set(text).issubset(set(string.punctuation + string.whitespace)) else '' \n",
    "                     for text in text_list]\n",
    "        text_list = [x.lower() for x in text_list]\n",
    "        translator = str.maketrans(string.punctuation + string.digits + \"_\", \" \" * len(\n",
    "            string.punctuation + string.digits + \"_\"))\n",
    "        cleaned_list = []\n",
    "        for text in text_list:\n",
    "            cleaned_text = text.translate(translator)\n",
    "            cleaned_text = ' '.join(cleaned_text.split())\n",
    "            cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "            cleaned_list.append(cleaned_text)\n",
    "        cleaned_df[text_col] = cleaned_list\n",
    "    \n",
    "    # Add the non-text columns to the cleaned dataframe\n",
    "    for col in df.columns:\n",
    "        if col not in text_cols:\n",
    "            cleaned_df[col] = df[col]\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleaned_concatenated_df = clean_text(concatenated_df, text_cols=['page_name'])\n",
    "cleaned_concatenated_df = clean_text(concatenated_df, text_cols=['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = ['day care', 'creche', 'childcare','daycare','after school care',\n",
    "           'pre school', 'child', 'baby',\n",
    "            'infant','girl child', 'play school', \n",
    "           'boy child','Adolescent',\n",
    " 'nursery', 'preschool', 'after schoolcare', 'day nursery', \n",
    "  'playschool', 'kindergarten', 'childminding', 'babysitting',\n",
    "           'babysitter', \n",
    " 'nanny', 'children supervision', 'toddler care', 'kindergarten',\n",
    "'child minding', 'baby sitting', 'baby sitter', 'children',\n",
    " 'childrensupervision','child supervision', 'childsupervision', 'children supervision',\n",
    "           'toddlercare','foster', 'childs','childrens',\n",
    "           'stepchild','step daughter', 'step son', 'grandchildren','grandchild','daughter','son', \n",
    "          'stepchildren',\"children's\", 'childhood',\n",
    "           'stepchildren',\n",
    "          'day cares','childrent','daycares',\n",
    "          'childplus',]\n",
    "\n",
    "words_4 = set([word.lower() for word in words_3])\n",
    "\n",
    "len(words_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# # Compile regular expressions for pattern matching\n",
    "# pattern = re.compile(r'\\b(?:{})\\b'.format('|'.join(map(re.escape, words_4))), flags=re.IGNORECASE)\n",
    "\n",
    "# # Function to check if any phrase from the list is present in the row\n",
    "# def check_phrases(row):\n",
    "#     if isinstance(row['search_text'], str) and re.search(pattern, row['search_text']):\n",
    "#         return 'Yes'\n",
    "#     return 'No'\n",
    "\n",
    "mask = (cleaned_concatenated_df['input'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n",
    "\n",
    "cleaned_concatenated_df['Contains_syn'] = ''\n",
    "cleaned_concatenated_df.loc[mask, 'Contains_syn'] = 'yes'\n",
    "# df_combined_web_iva_search.loc[df_combined_web_iva_search['category'] == '', 'category'] = 'Other'\n",
    "\n",
    "# cleaned_concatenated_df.head(5)\n",
    "# # Apply the function to create a new column\n",
    "# cleaned_concatenated_df['Contains_syn'] = cleaned_concatenated_df.apply(check_phrases, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleaned_concatenated_df.to_csv('cc_cleaned_concatenated_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenated_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenated_df= concatenated_df[['client_id','person_internal_id','page_name']]\n",
    "cleaned_concatenated_df= cleaned_concatenated_df[['client_id','person_internal_id','input', 'Contains_syn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54871, 4)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleaned_concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# def count_intnt_entits(doc):\n",
    "#     intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "#     entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "#     return len(intents), len(entities)\n",
    "\n",
    "# def extract_ner_entities(doc, sentence):\n",
    "#     entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "#     return entities\n",
    "\n",
    "# def length_entities(list_entities):\n",
    "#     if (list_entities == np.nan or list_entities is None or list_entities == ''):\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return len(list_entities)\n",
    "\n",
    "# def filter_named_entities(doc, text):\n",
    "#     filtered_words = [token.text for token in doc if token.ent_type_ not in ['ORG', 'PERSON', 'GPE', 'LOC', 'FAC']]\n",
    "#     filtered_text = ' '.join(filtered_words)\n",
    "#     return filtered_text\n",
    "\n",
    "# def process_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     intents, entities = count_intnt_entits(doc)\n",
    "#     ner_entities = extract_ner_entities(doc, text)\n",
    "#     len_ner_entities = length_entities(ner_entities)\n",
    "#     filtered_text = filter_named_entities(doc, text)\n",
    "#     return intents, entities, ner_entities, len_ner_entities, filtered_text\n",
    "\n",
    "# def text_preprocess(dataframe, col='reportingfields_fieldvalue1'):\n",
    "#     dataframe = dataframe.drop_duplicates()\n",
    "#     texts = dataframe[col].tolist()\n",
    "#     # texts = dataframe['text'].tolist()\n",
    "#     with ProcessPoolExecutor(max_workers=96) as executor:\n",
    "#         processed_results = list(executor.map(process_text, texts))\n",
    "\n",
    "#     intents, entities, ner_entities, len_ner_entities, filtered_texts = zip(*processed_results)\n",
    "    \n",
    "#     dataframe['no_of_intents'] = intents\n",
    "#     dataframe['no_of_entities'] = entities\n",
    "#     dataframe['ner_enities'] = ner_entities\n",
    "#     dataframe['len_ner_enities'] = len_ner_entities\n",
    "#     dataframe[col]= filtered_texts\n",
    "    \n",
    "#     dataframe = dataframe[dataframe['len_ner_enities'] > 0]\n",
    "#     dataframe[col] = dataframe[col].str.strip()\n",
    "#     dataframe = dataframe.drop(['no_of_intents', 'no_of_entities', 'ner_enities', 'len_ner_enities'], axis=1)\n",
    "    \n",
    "#     return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text_preprocessed_cleaned_concatenated_df = text_preprocess(cleaned_concatenated_df, col='reportingfields_fieldvalue1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.65678286552429\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Load the model\n",
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load('Bert_CC_model/cc_model.pkl')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 12: Encode the text samples from the Parquet file\n",
    "encodings = model.encode(cleaned_concatenated_df['input'].fillna(' ').tolist())\n",
    "\n",
    "# Step 13: Make predictions for the encoded samples\n",
    "cleaned_concatenated_df['prediction'] = loaded_model.predict(encodings)\n",
    "# text_preprocessed_cleaned_concatenated_df['confidence_level'] = np.max(loaded_model.predict_proba(encodings), axis=1)\n",
    "\n",
    "# Step 14: Save the predictions in an Excel file\n",
    "cleaned_concatenated_df.to_excel('cc_iva_3april_predictions.xlsx', index=False)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('cc_search_3april_predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_unique_words = list(words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode phrases using the SentenceTransformer model\n",
    "phrase_embeddings = model.encode(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate similarity between a text and the phrases\n",
    "def calculate_similarity(text):\n",
    "    text = str(text)\n",
    "    text_embedding = model.encode([text])\n",
    "    similarity_scores = cosine_similarity(text_embedding, phrase_embeddings)\n",
    "    \n",
    "    pos = np.argmax(similarity_scores[0])\n",
    "    return similarity_scores[0][pos], list_unique_words[pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Similarity_Scores'] = df[df['Contains_syn'].isna()]['search_text'].apply(calculate_similarity)\n",
    "# df['Similarity_Scores'] = df[df['Contains_syn'].isna()]['search_text'].apply(calculate_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[(df['prediction']=='Child care') & df['Contains_syn'].isna()].to_csv('cc_cat_not_contain_syn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Contains_syn']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.max(loaded_model.predict_proba(encodings[:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loaded_model.predict(encodings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loaded_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/3565708624.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('\\d+', '')\n",
      "/tmp/ipykernel_74027/3565708624.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['text'] = df3['text'].apply(filter_named_entities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>senior citizen care expense reimbursement</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elderly care plus</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aging care home</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>retirees care reimbirsement</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>senior assistance required</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>daycare expense reimbursement</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baby care licensed</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.999976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oldsters care home</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.991685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>contentpage eldercare subsidy</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elder statesmen care</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>elder women care</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>silver generation care</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.996034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>contentpage elder care subsidy</td>\n",
       "      <td>Elder care</td>\n",
       "      <td>0.999981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gerontology care</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.931354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text  prediction  probability\n",
       "0   senior citizen care expense reimbursement  Elder care     0.999973\n",
       "1                           elderly care plus  Elder care     0.999985\n",
       "2                             aging care home  Elder care     0.999983\n",
       "3                 retirees care reimbirsement  Elder care     0.999841\n",
       "4                  senior assistance required  Elder care     0.999977\n",
       "5               daycare expense reimbursement       Other     0.999982\n",
       "6                          baby care licensed       Other     0.999976\n",
       "7                          oldsters care home       Other     0.991685\n",
       "8               contentpage eldercare subsidy  Elder care     0.999982\n",
       "9                        elder statesmen care  Elder care     0.999982\n",
       "10                           elder women care  Elder care     0.999983\n",
       "11                     silver generation care       Other     0.996034\n",
       "12             contentpage elder care subsidy  Elder care     0.999981\n",
       "13                           gerontology care       Other     0.931354"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_texts = [\"senior-citizen?care expense reimbursement\", \n",
    "                 \"elderly*care plus\",\n",
    "                 \"aging care home\", \"retirees care reimbirsement\", \n",
    "                 \"senior assistance required\",\"Daycare@expense reimbursement\",\n",
    "                 \"baby care licensed\", \"oldsters care home\",\"geriatric care home\",\n",
    "                 \"contentPage 2023 Eldercare!!!!!!!!!!!!!****@_Subsidy\",\"Elder statesmen care\",\n",
    "                 \"Elder women care\",\"Silver generation care\",\n",
    "                 \"contentPage {}[]/\\|?><,.;:!@#+\\t\\n\\r\\f\\v 2023 Elder care Subsidy\",\"gerontology care\",\n",
    "                 \"Elderly Care Plus Information\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert saved model .bin and config.json to .pkl compatible files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert saved model .bin and config.json to .pkl compatible files\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Load the model configuration from the config.json file\n",
    "config = AutoConfig.from_pretrained('EC_model_outer_combined_texts_data_v6', num_labels=2)\n",
    "\n",
    "# Load the model from the binary file using the configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained('EC_model_outer_combined_texts_data_v6', config=config)\n",
    "\n",
    "# Save the model and configuration as a pickle file\n",
    "with open('EC_model_outer_combined_texts_data_v6/EC_model.pkl', 'wb') as f:\n",
    "    pickle.dump((config, model.state_dict()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tagging the unseen_data for analysisng the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the texts which contain exact phrases from synonyms list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = ['older parent','older people','grand parents','elder','old parents','elder women',\n",
    " 'silver generation','aged people', 'older women','older men','old age home','elder',\n",
    " 'aged',\n",
    " 'elderly people',\n",
    " 'senior assistance',\n",
    " 'aging-in-place',\n",
    " 'aged population',\n",
    " 'golden agers',\n",
    " 'aging in place',\n",
    " 'grey generation',\n",
    " 'silver generation',\n",
    " 'senior health',\n",
    " 'aged population',\n",
    " 'elderly companion',\n",
    " 'golden agers',\n",
    " 'senior citizen',\n",
    " 'elder support',\n",
    " 'elderly',\n",
    " 'senior members',\n",
    " 'elder population',\n",
    " 'elderly residents',\n",
    " 'senior assistance',\n",
    " 'oldsters',\n",
    " 'grey generation',\n",
    " 'aging population',\n",
    " 'elder statesmen',\n",
    " 'elderly',\n",
    " 'elderly people',\n",
    " 'aging',\n",
    " 'elderly residents',\n",
    " 'elder',\n",
    " 'elder women',\n",
    " 'senior',\n",
    " 'elder generation',\n",
    " 'gerontology',\n",
    " 'elderly population',\n",
    " 'senior members',\n",
    " 'retirees',\n",
    " 'elderly population',\n",
    " 'eldercare',\n",
    " 'geriatric',\n",
    " 'elder statesmen',\n",
    " 'age related',\n",
    " 'retirees',\n",
    " 'third age population',\n",
    " 'aging population',\n",
    " 'elder population',\n",
    " 'oldsters',\n",
    " 'third age population','eldercae', 'eldercarr', 'eldermann', \n",
    "'aged home','eldercre','eldery','elderman','elders','eldercrae',]\n",
    "words_4 = list(set([word.lower() for word in words_3]))\n",
    "len(words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcomeuserfollowup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medical plan credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welcomeuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>general purpose loans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chiropractor visits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text\n",
       "0    welcomeuserfollowup\n",
       "1    medical plan credit\n",
       "2            welcomeuser\n",
       "3  general purpose loans\n",
       "4    chiropractor visits"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = pd.DataFrame()\n",
    "df_cleaned['text'] =cleaned_text_list\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/3427755096.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = (df_cleaned['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcomeuserfollowup</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medical plan credit</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welcomeuser</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>general purpose loans</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chiropractor visits</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text category\n",
       "0    welcomeuserfollowup         \n",
       "1    medical plan credit         \n",
       "2            welcomeuser         \n",
       "3  general purpose loans         \n",
       "4    chiropractor visits         "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (df_cleaned['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n",
    "\n",
    "df_cleaned['category'] = ''\n",
    "df_cleaned.loc[mask, 'category'] = 'Elder care'\n",
    "# df_combined_web_iva_search.loc[df_combined_web_iva_search['category'] == '', 'category'] = 'Other'\n",
    "\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>ongoing elderly care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      text    category\n",
       "5666  ongoing elderly care  Elder care"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned['category']=='Elder care'].sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned['category']=='Elder care'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get text which are similar to phrases in synonnyms list for texts other than which are filtered above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_sentences(df, sentences, phrases, threshold=0.95, category_name = 'Elder care'):\n",
    "    # encode the phrases using the model\n",
    "    phrase_embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "    \n",
    "    # initialize an empty list to store the similar sentences\n",
    "    similar_sentences = []\n",
    "    \n",
    "    # iterate over the sentences\n",
    "    for sentence in sentences:\n",
    "        # encode the sentence using the model\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "        # reshape the sentence embedding to a 2D array\n",
    "        sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "        \n",
    "        # calculate the cosine similarity between the sentence embedding and each phrase embedding\n",
    "        cosine_scores = 1 - cosine_distances(sentence_embedding, phrase_embeddings)\n",
    "        \n",
    "        # convert the cosine similarity scores to a list\n",
    "        scores_list = cosine_scores.tolist()[0]\n",
    "        \n",
    "        # iterate over the phrases and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "        for phrase, score in zip(phrases, scores_list):\n",
    "            if score >= threshold:\n",
    "                similar_sentences.append(sentence)\n",
    "                break\n",
    "    \n",
    "    # convert the list of similar sentences to a set to remove duplicates\n",
    "    similar_sentences = set(similar_sentences)\n",
    "    \n",
    "    # create a new dataframe containing only the rows with text that is in the set of similar sentences\n",
    "    similar_df = df[df['text'].isin(similar_sentences)]\n",
    "    similar_df['category']=category_name\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/2749835889.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_df['category']=category_name\n"
     ]
    }
   ],
   "source": [
    "similar_df_unseen = find_similar_sentences(df_cleaned, \n",
    "                                    df_cleaned[df_cleaned['category']=='']['text'].to_list(), \n",
    "                                    words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>retiremen</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text    category\n",
       "7095  retiremen  Elder care"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_EC_df = pd.concat([df_cleaned[df_cleaned['category']=='Elder care'], \n",
    "                       similar_df_unseen]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_EC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_EC_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_EC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74027/3802977147.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_similar_unseen_df['category'] = 'Other'\n"
     ]
    }
   ],
   "source": [
    "# filter out the rows with similar text from the original DataFrame\n",
    "non_similar_unseen_df = df_cleaned[~df_cleaned['text'].isin(unseen_EC_df['text'])]\n",
    "\n",
    "# sample twice as many rows from the non-similar DataFrame as there are in the similar DataFrame\n",
    "# non_similar_df = non_similar_df.sample(n=only_EC_df.shape[0]*2)\n",
    "non_similar_unseen_df['category'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen_concatenated = pd.concat([unseen_EC_df, non_similar_unseen_df]).sample(frac=1).reset_index(drop=True)\n",
    "# df_concatenated = df_concatenated\n",
    "# df_concatenated.drop(columns=['input', 'search_text', 'page_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12584, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unseen_concatenated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unseen_concatenated[(df_unseen_concatenated.text.str.contains('elder')) & (df_unseen_concatenated.category=='Other')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unseen_concatenated.to_excel('labelled_unseen_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
