{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install en_core_web_sm-3.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade fasttext --index-url=https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple --trusted-host=artifactory.alight.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/1765538524.py:1: DtypeWarning: Columns (0,1,2,4,5,9,10,11,12,13,14,15,16,17,18,20,21,23,24,25,26,27,28,29,31,33,39,41,56,57,58,59,63,64,65,67,68,70,71,74,75,77,78,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,106,108,109,110,111,112,113,114,115,116,117,118,119,122,123,124,125,126,127,128,129,130,131,132,134,135,136,137,138,155,156,159,160,161,164,165) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_combined_web_iva_search = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/IVA/combined_new_adult-child_outer_new.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_combined_web_iva_search = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/IVA/combined_new_adult-child_outer_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child a dependent?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i change my dependent daycare deduction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child last name change</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am trying to add my children as beneficiarie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how much timeoff do i get after the birth of m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input search_text page_name\n",
       "0                            step child a dependent?         NaN       NaN\n",
       "1     how do i change my dependent daycare deduction         NaN       NaN\n",
       "2                             Child last name change         NaN       NaN\n",
       "3  i am trying to add my children as beneficiarie...         NaN       NaN\n",
       "4  how much timeoff do i get after the birth of m...         NaN       NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_web_iva_search[['input','search_text','page_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva = pd.read_csv(\"s3://adl-core-sagemaker-studio/external/Deepali/IVA_cleaned_labelled(session_id_added).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_iva.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'entry_id', 'client_id', 'person_internal_id',\n",
       "       'input_orig', 'labels', 'input_cleaned', 'input_cleaned_dl',\n",
       "       'next_unit_hit', 'previous_unit_hit', 'unit_name', 'response_text',\n",
       "       'session_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iva.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva1 = df_iva.drop(['Unnamed: 0','entry_id','client_id','person_internal_id','next_unit_hit',\n",
    "             'previous_unit_hit','response_text','session_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_cleaned</th>\n",
       "      <th>input_cleaned_dl</th>\n",
       "      <th>unit_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>open enrol</td>\n",
       "      <td>open enrollment</td>\n",
       "      <td>Annual Enrollment Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>enrol hra</td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>Health Reimbursement Account Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>tire discount</td>\n",
       "      <td>tire discount</td>\n",
       "      <td>Discounts Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>elig</td>\n",
       "      <td>eligible</td>\n",
       "      <td>Health Savings Account (HSA) Eligible Expenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>need updat mail address</td>\n",
       "      <td>need to update my mail address</td>\n",
       "      <td>Manage Address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       input_orig                labels  \\\n",
       "0                 Open enrollment            Enrollment   \n",
       "1                   enroll in hra                   HRA   \n",
       "2                   TIRE DISCOUNT       Discounts Issue   \n",
       "3                        eligible           HSA related   \n",
       "4  Need to update my mail address  General Acount issue   \n",
       "\n",
       "             input_cleaned                input_cleaned_dl  \\\n",
       "0               open enrol                 open enrollment   \n",
       "1                enrol hra                   enroll in hra   \n",
       "2            tire discount                   tire discount   \n",
       "3                     elig                        eligible   \n",
       "4  need updat mail address  need to update my mail address   \n",
       "\n",
       "                                        unit_name  \n",
       "0                     Annual Enrollment Clarifier  \n",
       "1          Health Reimbursement Account Clarifier  \n",
       "2                             Discounts Clarifier  \n",
       "3  Health Savings Account (HSA) Eligible Expenses  \n",
       "4                                  Manage Address  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iva1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iva2 = df_iva1.drop(['input_cleaned','input_cleaned_dl'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>unit_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>Annual Enrollment Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>Health Reimbursement Account Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>Discounts Clarifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>Health Savings Account (HSA) Eligible Expenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>Manage Address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       input_orig                labels  \\\n",
       "0                 Open enrollment            Enrollment   \n",
       "1                   enroll in hra                   HRA   \n",
       "2                   TIRE DISCOUNT       Discounts Issue   \n",
       "3                        eligible           HSA related   \n",
       "4  Need to update my mail address  General Acount issue   \n",
       "\n",
       "                                        unit_name  \n",
       "0                     Annual Enrollment Clarifier  \n",
       "1          Health Reimbursement Account Clarifier  \n",
       "2                             Discounts Clarifier  \n",
       "3  Health Savings Account (HSA) Eligible Expenses  \n",
       "4                                  Manage Address  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iva2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_combined_web_iva_search, df_iva2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent_entitlement_client_name',\n",
       " 'agent_entitlement_access_key',\n",
       " 'channel',\n",
       " 'companytracking_clientid',\n",
       " 'companytracking_companyid',\n",
       " 'companytracking_companyname',\n",
       " 'companytracking_multitenantid',\n",
       " 'entry_id',\n",
       " 'entry_order',\n",
       " 'entry_type',\n",
       " 'input',\n",
       " 'inputnotunderstood_type',\n",
       " 'knowledge_category',\n",
       " 'knowledge_category_path',\n",
       " 'navigation_url',\n",
       " 'next_unit_hit',\n",
       " 'previous_unit_hit',\n",
       " 'response_condition',\n",
       " 'response_text',\n",
       " 'session_id',\n",
       " 'session_start_utc',\n",
       " 'session_start_cst',\n",
       " 'transactioncontribution_factor',\n",
       " 'unit_name',\n",
       " 'agent_session_id',\n",
       " 'time_submitted_utc',\n",
       " 'time_submitted_cst',\n",
       " 'unit_id',\n",
       " 'response_name',\n",
       " 'related_links',\n",
       " 'matched_search_criteria',\n",
       " 'agent_user_id',\n",
       " 'escalation_type',\n",
       " 'companytracking_personalid',\n",
       " 'person_internal_id',\n",
       " 'client_id',\n",
       " 'prompting_task',\n",
       " 'prompting_task_step',\n",
       " 'other_active_tasks',\n",
       " 'complete_tasks',\n",
       " 'cancelled_tasks',\n",
       " 'conversation_rating_random_sample_queue',\n",
       " 'conversation_rating_all_conversations_queue',\n",
       " 'conversation_rating_reason_random_sample_queue',\n",
       " 'conversation_rating_comment_random_sample_queue',\n",
       " 'conversation_rating_reason_all_conversations_queue',\n",
       " 'conversation_rating_comment_all_conversations_queue',\n",
       " 'entry_rating_random_sample_queue',\n",
       " 'entry_rating_reason_random_sample_queue',\n",
       " 'entry_rating_comment_random_sample_queue',\n",
       " 'entry_rating_all_conversations_queue',\n",
       " 'entry_rating_reason_all_conversations_queue',\n",
       " 'entry_rating_comment_all_conversations_queue',\n",
       " 'entry_feedback_rating_end_user_feedback_queue',\n",
       " 'entry_feedback_rating_reason_end_user_feedback_queue',\n",
       " 'entry_feedback_rating_comment_end_user_feedback_queue',\n",
       " 'fpml_version',\n",
       " 'last_unit_in_conversation',\n",
       " 'transactionnegation',\n",
       " 'knowledgetie_units_0_label',\n",
       " 'knowledgetie_units_0_unit_id',\n",
       " 'referralaccepted_destchannel',\n",
       " 'referralaccepted_destqueue',\n",
       " 'load_ts',\n",
       " 'language',\n",
       " 'reporting_events',\n",
       " 'yyyymm',\n",
       " 'web_page_detail_event_key',\n",
       " 'web_page_detail_event_key_columns',\n",
       " 'client_key',\n",
       " 'implementation_key',\n",
       " 'participant_key',\n",
       " 'platform_id_x',\n",
       " 'implementation_hierarchy_key',\n",
       " 'person_id',\n",
       " 'platform_person_internal_id',\n",
       " 'source_person_internal_id',\n",
       " 'source_person_id',\n",
       " 'person_birth_date',\n",
       " 'person_age',\n",
       " 'person_gender',\n",
       " 'person_employment_status',\n",
       " 'person_state',\n",
       " 'person_union_vs_non_union',\n",
       " 'person_salary',\n",
       " 'person_eeo',\n",
       " 'admin_id',\n",
       " 'user_id_type',\n",
       " 'source_global_session_id',\n",
       " 'source_global_session_create_timestamp',\n",
       " 'global_session_id',\n",
       " 'global_session_create_timestamp_cst_timezone',\n",
       " 'web_session_id',\n",
       " 'session_create_timestamp_cst_timezone',\n",
       " 'web_user_type_id',\n",
       " 'web_user_type_description',\n",
       " 'web_user_system_id',\n",
       " 'remote_ip_address',\n",
       " 'page_name',\n",
       " 'page_visit_date_cst_timezone',\n",
       " 'page_visit_timestamp_cst_timezone',\n",
       " 'portlet_name',\n",
       " 'business_page_name',\n",
       " 'page_domain',\n",
       " 'page_type_code',\n",
       " 'page_type',\n",
       " 'page_web_type_description',\n",
       " 'page_sequence_number',\n",
       " 'request_method_description',\n",
       " 'global_session_end_page_name',\n",
       " 'global_session_end_portlet_name',\n",
       " 'global_session_end_page_process_name',\n",
       " 'global_session_end_business_page_name',\n",
       " 'global_session_end_page_domain',\n",
       " 'global_session_end_page_type_code',\n",
       " 'global_session_end_page_type',\n",
       " 'global_session_end_page_web_type_description',\n",
       " 'link_id',\n",
       " 'link_destination_url',\n",
       " 'link_id_display',\n",
       " 'link_external_info',\n",
       " 'link_flow',\n",
       " 'link_type_code',\n",
       " 'link_type_description',\n",
       " 'business_link_id',\n",
       " 'web_conversation_id',\n",
       " 'web_channel_application_code',\n",
       " 'web_channel_application_description',\n",
       " 'web_channel_application_version',\n",
       " 'initial_web_channel_application_code',\n",
       " 'initial_web_channel_application_description',\n",
       " 'initial_web_channel_application_version',\n",
       " 'host_name_description',\n",
       " 'web_authentication_level_id',\n",
       " 'web_authentication_level_description',\n",
       " 'web_security_method_description',\n",
       " 'web_logon_type_description',\n",
       " 'web_device_type',\n",
       " 'browser_language',\n",
       " 'page_visit_duration',\n",
       " 'web_session_page_indicator',\n",
       " 'outbound_page_link_indicator',\n",
       " 'hint_page_indicator',\n",
       " 'error_page_indicator',\n",
       " 'logon_page_indicator',\n",
       " 'logoff_page_indicator',\n",
       " 'last_visited_page_indicator',\n",
       " 'password_reset_page_indicator',\n",
       " 'hm_enrollment_abandoned_indicator',\n",
       " 'hm_enrollment_initiated_indicator',\n",
       " 'hm_enrollment_completed_indicator',\n",
       " 'web_page_abandoned_indicator',\n",
       " 'web_chat_request_indicator',\n",
       " 'page_slide_indicator',\n",
       " 'web_page_payload_size',\n",
       " 'page_process_name',\n",
       " 'page_process_detail',\n",
       " 'iva_used_indicator',\n",
       " 'event_position',\n",
       " 'action_type',\n",
       " 'load_timestamp',\n",
       " 'source_system_code',\n",
       " 'mobile_app_device_type',\n",
       " 'page_load_elapsed_time',\n",
       " 'mobile_app_type',\n",
       " 'session_create_date_cst_timezone_partition',\n",
       " 'platform_id_y',\n",
       " 'search_text',\n",
       " 'search_results',\n",
       " 'session_created_timestamp',\n",
       " 'input_orig',\n",
       " 'labels',\n",
       " 'unit_name']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat2 = df_concat[['input','input_orig','search_text','page_name','labels','session_start_cst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>input_orig</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child a dependent?</td>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do i change my dependent daycare deduction</td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child last name change</td>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am trying to add my children as beneficiarie...</td>\n",
       "      <td>eligible</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how much timeoff do i get after the birth of m...</td>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                            step child a dependent?   \n",
       "1     how do i change my dependent daycare deduction   \n",
       "2                             Child last name change   \n",
       "3  i am trying to add my children as beneficiarie...   \n",
       "4  how much timeoff do i get after the birth of m...   \n",
       "\n",
       "                       input_orig search_text page_name                labels  \\\n",
       "0                 Open enrollment         NaN       NaN            Enrollment   \n",
       "1                   enroll in hra         NaN       NaN                   HRA   \n",
       "2                   TIRE DISCOUNT         NaN       NaN       Discounts Issue   \n",
       "3                        eligible         NaN       NaN           HSA related   \n",
       "4  Need to update my mail address         NaN       NaN  General Acount issue   \n",
       "\n",
       "     session_start_cst  \n",
       "0  2017-12-05 21:48:56  \n",
       "1  2017-12-11 09:45:09  \n",
       "2  2018-01-22 08:12:28  \n",
       "3  2017-10-15 10:34:01  \n",
       "4  2017-12-29 10:46:58  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2330066, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/580359627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_concat2.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_concat2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971001, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append('../../nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(df, text_cols):\n",
    "    # Create a new dataframe to hold the cleaned text columns\n",
    "    cleaned_df = pd.DataFrame()\n",
    "    \n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Clean each text column and add it to the cleaned dataframe\n",
    "    for text_col in text_cols:\n",
    "        text_list = df[text_col].tolist()\n",
    "        text_list = [str(text) for text in text_list]\n",
    "        text_list = [text for text in text_list if text.strip() and not\n",
    "                     set(text).issubset(set(string.punctuation + string.whitespace))]\n",
    "        text_list = [x.lower() for x in text_list]\n",
    "        translator = str.maketrans(string.punctuation + string.digits + \"_\", \" \" * len(\n",
    "            string.punctuation + string.digits + \"_\"))\n",
    "        cleaned_list = []\n",
    "        for text in text_list:\n",
    "            cleaned_text = text.translate(translator)\n",
    "            cleaned_text = ' '.join(cleaned_text.split())\n",
    "            cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "            cleaned_list.append(cleaned_text)\n",
    "        cleaned_df[text_col] = cleaned_list\n",
    "    \n",
    "    # Add the non-text columns to the cleaned dataframe\n",
    "    for col in df.columns:\n",
    "        if col not in text_cols:\n",
    "            cleaned_df[col] = df[col]\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(df, text_cols):\n",
    "#     # Create a new dataframe to hold the cleaned text columns\n",
    "#     cleaned_df = pd.DataFrame()\n",
    "    \n",
    "#     # Define the list of stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "#     # Clean each text column and add it to the cleaned dataframe\n",
    "#     for text_col in text_cols:\n",
    "#         text_list = df[text_col].tolist()\n",
    "#         text_list = [str(text) for text in text_list]\n",
    "#         text_list = [text for text in text_list if text.strip() and not\n",
    "#                      set(text).issubset(set(string.punctuation + string.whitespace))]\n",
    "#         text_list = [x.lower() for x in text_list]\n",
    "#         translator = str.maketrans(string.punctuation + string.digits + \"_\", \" \" * len(\n",
    "#             string.punctuation + string.digits + \"_\"))\n",
    "#         cleaned_list = []\n",
    "#         for text in text_list:\n",
    "#             cleaned_text = text.translate(translator)\n",
    "#             cleaned_text = ' '.join(cleaned_text.split())\n",
    "#             cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stop_words])\n",
    "#             cleaned_list.append(cleaned_text)\n",
    "#         cleaned_df[text_col] = cleaned_list\n",
    "    \n",
    "#     return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat3 = clean_text(df_concat2, text_cols=['input','search_text','page_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child dependent</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>child last name change</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much timeoff get birth child</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input search_text page_name  \\\n",
       "0                        step child dependent         nan       nan   \n",
       "1          change dependent daycare deduction         nan       nan   \n",
       "2                      child last name change         nan       nan   \n",
       "3  trying add children beneficiaries life ins         nan       nan   \n",
       "4                much timeoff get birth child         nan       nan   \n",
       "\n",
       "                       input_orig                labels    session_start_cst  \n",
       "0                 Open enrollment            Enrollment  2017-12-05 21:48:56  \n",
       "1                   enroll in hra                   HRA  2017-12-11 09:45:09  \n",
       "2                   TIRE DISCOUNT       Discounts Issue  2018-01-22 08:12:28  \n",
       "3                        eligible           HSA related  2017-10-15 10:34:01  \n",
       "4  Need to update my mail address  General Acount issue  2017-12-29 10:46:58  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>search_text</th>\n",
       "      <th>page_name</th>\n",
       "      <th>input_orig</th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step child dependent</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Open enrollment</td>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>enroll in hra</td>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>child last name change</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TIRE DISCOUNT</td>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eligible</td>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much timeoff get birth child</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Need to update my mail address</td>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input search_text page_name  \\\n",
       "0                        step child dependent                         \n",
       "1          change dependent daycare deduction                         \n",
       "2                      child last name change                         \n",
       "3  trying add children beneficiaries life ins                         \n",
       "4                much timeoff get birth child                         \n",
       "\n",
       "                       input_orig                labels    session_start_cst  \n",
       "0                 Open enrollment            Enrollment  2017-12-05 21:48:56  \n",
       "1                   enroll in hra                   HRA  2017-12-11 09:45:09  \n",
       "2                   TIRE DISCOUNT       Discounts Issue  2018-01-22 08:12:28  \n",
       "3                        eligible           HSA related  2017-10-15 10:34:01  \n",
       "4  Need to update my mail address  General Acount issue  2017-12-29 10:46:58  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4 = df_concat3.replace('nan', '')\n",
    "df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4['text'] = df_concat4[['input', 'search_text', 'page_name']].apply(lambda x: ' '.join([str(i) for i in x if not pd.isna(i)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop(['input','search_text','page_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop(['input_orig'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "      <td>step child dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "      <td>child last name change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "      <td>much timeoff get birth child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 labels    session_start_cst  \\\n",
       "0            Enrollment  2017-12-05 21:48:56   \n",
       "1                   HRA  2017-12-11 09:45:09   \n",
       "2       Discounts Issue  2018-01-22 08:12:28   \n",
       "3           HSA related  2017-10-15 10:34:01   \n",
       "4  General Acount issue  2017-12-29 10:46:58   \n",
       "\n",
       "                                           text  \n",
       "0                        step child dependent    \n",
       "1          change dependent daycare deduction    \n",
       "2                      child last name change    \n",
       "3  trying add children beneficiaries life ins    \n",
       "4                much timeoff get birth child    "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat4.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_combined_web_iva_search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# Get the value counts of the 'labels' column\n",
    "label_counts = df_concat4['labels'].value_counts()\n",
    "print(sum(label_counts>500))\n",
    "# Filter the dataframe to only include rows where the label count is greater than 10000\n",
    "df_concat5 = df_concat4[df_concat4['labels'].isin(label_counts[label_counts > 10000].index)]\n",
    "\n",
    "# Get the shape of the resulting filtered dataframe\n",
    "df_concat5_shape = df_concat5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36517, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat5_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other              25225\n",
       "Health Benefits    11292\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat5['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4[df_concat4['text']=='nan '].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the texts which contain exact phrases from synonyms list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = ['older parent','older people','grand parents','elder','old parents','elder women',\n",
    " 'silver generation','aged people', 'older women','older men','old age home','elder',\n",
    " 'aged',\n",
    " 'elderly people',\n",
    " 'senior assistance',\n",
    " 'aging-in-place',\n",
    " 'aged population',\n",
    " 'golden agers',\n",
    " 'aging in place',\n",
    " 'grey generation',\n",
    " 'silver generation',\n",
    " 'senior health',\n",
    " 'aged population',\n",
    " 'elderly companion',\n",
    " 'golden agers',\n",
    " 'senior citizen',\n",
    " 'elder support',\n",
    " 'elderly',\n",
    " 'senior members',\n",
    " 'elder population',\n",
    " 'elderly residents',\n",
    " 'senior assistance',\n",
    " 'oldsters',\n",
    " 'grey generation',\n",
    " 'aging population',\n",
    " 'elder statesmen',\n",
    " 'elderly',\n",
    " 'elderly people',\n",
    " 'aging',\n",
    " 'elderly residents',\n",
    " 'elder',\n",
    " 'elder women',\n",
    " 'senior',\n",
    " 'elder generation',\n",
    " 'gerontology',\n",
    " 'elderly population',\n",
    " 'senior members',\n",
    " 'retirees',\n",
    " 'elderly population',\n",
    " 'eldercare',\n",
    " 'geriatric',\n",
    " 'elder statesmen',\n",
    " 'age related',\n",
    " 'retirees',\n",
    " 'third age population',\n",
    " 'aging population',\n",
    " 'elder population',\n",
    " 'oldsters',\n",
    " 'third age population','eldercae', 'eldercarr', 'eldermann', \n",
    "'aged home','eldercre','eldery','elderman','elders','eldercrae',]\n",
    "words_4 = list(set([word.lower() for word in words_3]))\n",
    "len(words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# child_care = set(['childd care'])\n",
    "# words_3 = ['adult', 'older parent', 'older people', 'grand parents', 'elder', 'parents', 'elder women',\n",
    "#            'silver generation', 'aged people', 'older women', 'older men', 'old age home', 'elder', 'aged',\n",
    "#            'elderly people', 'senior assistance', 'aging-in-place', 'aged population', 'golden agers',\n",
    "#            'aging in place', 'grey generation', 'silver generation', 'senior health', 'aged population', 'old',\n",
    "#            'elderly companion', 'golden agers', 'senior citizen', 'elder support', 'adult daycare', 'elderly',\n",
    "#            'senior members', 'elder population', 'elderly residents', 'adult', 'senior assistance', 'oldsters',\n",
    "#            'grey generation', 'aging population', 'elder statesmen', 'elderly', 'elderly people', 'aging', 'adult',\n",
    "#            'elderly residents', 'elder', 'elder women', 'adult', 'senior', 'elder generation', 'gerontology',\n",
    "#            'elderly population', 'senior members', 'retirees', 'elderly population', 'eldercare', 'geriatric',\n",
    "#            'elder statesmen', 'age related', 'retirees', 'third age population', 'aging population', 'elder population',\n",
    "#            'oldsters', 'third age population', 'aged home']\n",
    "\n",
    "# words_4 = list(set([word.lower() for word in words_3]))\n",
    "\n",
    "# # Compute Jaccard similarity\n",
    "# similarity_scores = {}\n",
    "# for phrase in words_4:\n",
    "#     phrase_set = set(phrase.split())\n",
    "#     similarity_scores[phrase] = len(child_care.intersection(phrase_set)) / len(child_care.union(phrase_set))\n",
    "\n",
    "# # Sort the similarity scores in descending order\n",
    "# sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Print the top 10 most similar phrases\n",
    "# print(\"Top 10 most similar phrases to 'child care':\")\n",
    "# for phrase, score in sorted_scores:\n",
    "#     print(f\"{phrase}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/2312389668.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = (df_concat4['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>2017-12-05 21:48:56</td>\n",
       "      <td>step child dependent</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HRA</td>\n",
       "      <td>2017-12-11 09:45:09</td>\n",
       "      <td>change dependent daycare deduction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discounts Issue</td>\n",
       "      <td>2018-01-22 08:12:28</td>\n",
       "      <td>child last name change</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSA related</td>\n",
       "      <td>2017-10-15 10:34:01</td>\n",
       "      <td>trying add children beneficiaries life ins</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Acount issue</td>\n",
       "      <td>2017-12-29 10:46:58</td>\n",
       "      <td>much timeoff get birth child</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 labels    session_start_cst  \\\n",
       "0            Enrollment  2017-12-05 21:48:56   \n",
       "1                   HRA  2017-12-11 09:45:09   \n",
       "2       Discounts Issue  2018-01-22 08:12:28   \n",
       "3           HSA related  2017-10-15 10:34:01   \n",
       "4  General Acount issue  2017-12-29 10:46:58   \n",
       "\n",
       "                                           text category  \n",
       "0                        step child dependent             \n",
       "1          change dependent daycare deduction             \n",
       "2                      child last name change             \n",
       "3  trying add children beneficiaries life ins             \n",
       "4                much timeoff get birth child             "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (df_concat4['text'].str.contains(r'\\b(' + '|'.join(words_4) + r')\\b', case=False, na=False))\n",
    "\n",
    "df_concat4['category'] = ''\n",
    "df_concat4.loc[mask, 'category'] = 'Elder care'\n",
    "# df_combined_web_iva_search.loc[df_combined_web_iva_search['category'] == '', 'category'] = 'Other'\n",
    "\n",
    "df_concat4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565931</th>\n",
       "      <td>Other</td>\n",
       "      <td>2023-03-08 11:08:48</td>\n",
       "      <td>hey want apply back child elder care company s...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518885</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2023-02-02 13:31:35</td>\n",
       "      <td>need information elder adult care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214835</th>\n",
       "      <td>Doctor</td>\n",
       "      <td>2023-02-28 08:20:20</td>\n",
       "      <td>two kids attend school paid program also go ca...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330935</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get paperwork fmla time care elderly father</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586218</th>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>2021-11-08 12:29:29</td>\n",
       "      <td>contribute paying nursing home care relative g...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166361</th>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elder fmily members</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322134</th>\n",
       "      <td>W-2 Form</td>\n",
       "      <td>2022-12-28 09:14:12</td>\n",
       "      <td>elderly parewnt help</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588769</th>\n",
       "      <td>Other</td>\n",
       "      <td>2020-11-29 09:48:48</td>\n",
       "      <td>elder care inforo</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160600</th>\n",
       "      <td>Eye care</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elder family care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323267</th>\n",
       "      <td>Rollovers Clarifier</td>\n",
       "      <td>2023-02-06 15:14:32</td>\n",
       "      <td>son aging child health plus need pick insuranc...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597671</th>\n",
       "      <td>Dependent issue</td>\n",
       "      <td>2020-09-22 14:31:53</td>\n",
       "      <td>elder care support</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317257</th>\n",
       "      <td>HSA related</td>\n",
       "      <td>2023-02-16 10:23:32</td>\n",
       "      <td>add grandchild beneficiary caring elderly pare...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255360</th>\n",
       "      <td>Other</td>\n",
       "      <td>2022-04-12 14:59:07</td>\n",
       "      <td>add children health benefits elder care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430849</th>\n",
       "      <td>HSA related</td>\n",
       "      <td>2023-02-22 15:36:42</td>\n",
       "      <td>mother elderly woman resides considered family...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249666</th>\n",
       "      <td>Coverage Issue</td>\n",
       "      <td>2022-08-24 11:21:04</td>\n",
       "      <td>looking information assistance elder care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564088</th>\n",
       "      <td>Dependent issue</td>\n",
       "      <td>2023-01-04 15:35:03</td>\n",
       "      <td>something allows take time care elderly parent</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177004</th>\n",
       "      <td>Payment Issue</td>\n",
       "      <td>2022-07-07 08:43:45</td>\n",
       "      <td>find things elder care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31144</th>\n",
       "      <td>Clarifier Issue</td>\n",
       "      <td>2023-01-29 02:16:00</td>\n",
       "      <td>elder parent health insurance</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161744</th>\n",
       "      <td>Login Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>emergency elder care</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106617</th>\n",
       "      <td>Clarifier Issue</td>\n",
       "      <td>2020-06-12 15:03:18</td>\n",
       "      <td>filling fasfa information high school senior c...</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      labels    session_start_cst  \\\n",
       "565931                 Other  2023-03-08 11:08:48   \n",
       "518885       Health Benefits  2023-02-02 13:31:35   \n",
       "214835                Doctor  2023-02-28 08:20:20   \n",
       "330935                   NaN                  NaN   \n",
       "586218        Life Insurance  2021-11-08 12:29:29   \n",
       "1166361                Other                  NaN   \n",
       "322134              W-2 Form  2022-12-28 09:14:12   \n",
       "588769                 Other  2020-11-29 09:48:48   \n",
       "1160600             Eye care                  NaN   \n",
       "323267   Rollovers Clarifier  2023-02-06 15:14:32   \n",
       "597671       Dependent issue  2020-09-22 14:31:53   \n",
       "317257           HSA related  2023-02-16 10:23:32   \n",
       "255360                 Other  2022-04-12 14:59:07   \n",
       "430849           HSA related  2023-02-22 15:36:42   \n",
       "249666        Coverage Issue  2022-08-24 11:21:04   \n",
       "564088       Dependent issue  2023-01-04 15:35:03   \n",
       "177004         Payment Issue  2022-07-07 08:43:45   \n",
       "31144        Clarifier Issue  2023-01-29 02:16:00   \n",
       "1161744          Login Issue                  NaN   \n",
       "106617       Clarifier Issue  2020-06-12 15:03:18   \n",
       "\n",
       "                                                      text    category  \n",
       "565931   hey want apply back child elder care company s...  Elder care  \n",
       "518885                 need information elder adult care    Elder care  \n",
       "214835   two kids attend school paid program also go ca...  Elder care  \n",
       "330935       get paperwork fmla time care elderly father    Elder care  \n",
       "586218   contribute paying nursing home care relative g...  Elder care  \n",
       "1166361                               elder fmily members   Elder care  \n",
       "322134                              elderly parewnt help    Elder care  \n",
       "588769                                 elder care inforo    Elder care  \n",
       "1160600                                 elder family care   Elder care  \n",
       "323267   son aging child health plus need pick insuranc...  Elder care  \n",
       "597671                                elder care support    Elder care  \n",
       "317257   add grandchild beneficiary caring elderly pare...  Elder care  \n",
       "255360            add children health benefits elder care   Elder care  \n",
       "430849   mother elderly woman resides considered family...  Elder care  \n",
       "249666         looking information assistance elder care    Elder care  \n",
       "564088    something allows take time care elderly parent    Elder care  \n",
       "177004                            find things elder care    Elder care  \n",
       "31144                      elder parent health insurance    Elder care  \n",
       "1161744                              emergency elder care   Elder care  \n",
       "106617   filling fasfa information high school senior c...  Elder care  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4[df_concat4['category']=='Elder care'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1638, 4)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat4[df_concat4['category']=='Elder care'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get text which are similar to phrases in synonnyms list for texts other than which are filtered above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def find_similar_sentences(df, sentences, phrases, threshold=0.90, category_name = 'Elder care'):\n",
    "#     # encode the phrases and sentences using the model\n",
    "#     phrase_embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "#     sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "#     # calculate the cosine similarity between the sentence embeddings and the phrase embeddings using multi-reference cosine similarity\n",
    "#     cosine_scores = 1 - cosine_distances(sentence_embeddings, phrase_embeddings)\n",
    "    \n",
    "#     # iterate over the sentences and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "#     similar_sentences = []\n",
    "#     for i, sentence in enumerate(sentences):\n",
    "#         scores_list = cosine_scores[i].tolist()\n",
    "#         for phrase, score in zip(phrases, scores_list):\n",
    "#             if score >= threshold:\n",
    "#                 similar_sentences.append(sentence)\n",
    "#                 break\n",
    "    \n",
    "#     # convert the list of similar sentences to a set to remove duplicates\n",
    "#     similar_sentences = set(similar_sentences)\n",
    "    \n",
    "#     # create a new dataframe containing only the rows with text that is in the set of similar sentences\n",
    "#     similar_df = df[df['text'].isin(similar_sentences)]\n",
    "#     similar_df['category']=category_name\n",
    "#     return similar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_sentences(df, sentences, phrases, threshold=0.95, category_name = 'Elder care'):\n",
    "    # encode the phrases using the model\n",
    "    phrase_embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "    \n",
    "    # initialize an empty list to store the similar sentences\n",
    "    similar_sentences = []\n",
    "    \n",
    "    # iterate over the sentences\n",
    "    for sentence in sentences:\n",
    "        # encode the sentence using the model\n",
    "        sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "        # reshape the sentence embedding to a 2D array\n",
    "        sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "        \n",
    "        # calculate the cosine similarity between the sentence embedding and each phrase embedding\n",
    "        cosine_scores = 1 - cosine_distances(sentence_embedding, phrase_embeddings)\n",
    "        \n",
    "        # convert the cosine similarity scores to a list\n",
    "        scores_list = cosine_scores.tolist()[0]\n",
    "        \n",
    "        # iterate over the phrases and similarity scores and append the sentence to the list if it meets the threshold for at least one phrase\n",
    "        for phrase, score in zip(phrases, scores_list):\n",
    "            if score >= threshold:\n",
    "                similar_sentences.append(sentence)\n",
    "                break\n",
    "    \n",
    "    # convert the list of similar sentences to a set to remove duplicates\n",
    "    similar_sentences = set(similar_sentences)\n",
    "    \n",
    "    # create a new dataframe containing only the rows with text that is in the set of similar sentences\n",
    "    similar_df = df[df['text'].isin(similar_sentences)]\n",
    "    similar_df['category']=category_name\n",
    "    return similar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat4 = df_concat4.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/3468478405.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_df['category']=category_name\n"
     ]
    }
   ],
   "source": [
    "similar_df = find_similar_sentences(df_concat4, \n",
    "                                    df_concat4[df_concat4['category']=='']['text'].to_list(), \n",
    "                                    words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1149570</th>\n",
       "      <td>Wellness issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elderbright</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149592</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elderc</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150598</th>\n",
       "      <td>Account issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elderkin</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        labels session_start_cst           text    category\n",
       "1149570         Wellness issue               NaN   elderbright   Elder care\n",
       "1149592  General Account issue               NaN        elderc   Elder care\n",
       "1150598          Account issue               NaN      elderkin   Elder care"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_EC_df = pd.concat([df_concat4[df_concat4['category']=='Elder care'], \n",
    "                       similar_df]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1641, 4)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_EC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_EC_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1641, 4)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_EC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/4228989628.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_similar_df['category'] = 'Other'\n"
     ]
    }
   ],
   "source": [
    "# filter out the rows with similar text from the original DataFrame\n",
    "non_similar_df = df_concat4[~df_concat4['text'].isin(only_EC_df['text'])]\n",
    "\n",
    "# sample twice as many rows from the non-similar DataFrame as there are in the similar DataFrame\n",
    "# non_similar_df = non_similar_df.sample(n=only_EC_df.shape[0]*2)\n",
    "non_similar_df['category'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated = pd.concat([only_EC_df, non_similar_df]).sample(frac=1).reset_index(drop=True)\n",
    "# df_concatenated = df_concatenated\n",
    "# df_concatenated.drop(columns=['input', 'search_text', 'page_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111098, 4)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>session_start_cst</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wayne elderkin</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18603</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2020-12-31 23:00:21</td>\n",
       "      <td>password catweldersare</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19627</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sina melder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21389</th>\n",
       "      <td>Vacation Issue</td>\n",
       "      <td>2022-12-15 08:37:43</td>\n",
       "      <td>management provider prescription drugs blue cross blue shielder still cvs caremark</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23700</th>\n",
       "      <td>Other</td>\n",
       "      <td>2020-03-09 00:46:48</td>\n",
       "      <td>belder gmail com sarasota</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31881</th>\n",
       "      <td>Coverage Issue</td>\n",
       "      <td>2017-10-30 10:25:20</td>\n",
       "      <td>hi filled forms morning line taxes federal married exceptions missouri put divided monthly taxes wondering right help faye fielder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37237</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2022-12-14 15:52:54</td>\n",
       "      <td>hi name andy i’m welder republic service would like apply health insurance help make happen</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37577</th>\n",
       "      <td>Other</td>\n",
       "      <td>2023-02-05 04:06:55</td>\n",
       "      <td>wanted add son insurance looks like get added talked people well ahead time filled forms need make sure covered currently works hii welder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39278</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ivory felder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41627</th>\n",
       "      <td>IVA help</td>\n",
       "      <td>NaN</td>\n",
       "      <td>welders</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46638</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>2020-10-13 16:41:11</td>\n",
       "      <td>question able purchase elderberry counter</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60673</th>\n",
       "      <td>401k/403b/457 Clarifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pipe welder apprentice program</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67356</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lisa fielder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68844</th>\n",
       "      <td>Dependent issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>melder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69081</th>\n",
       "      <td>General Account issue</td>\n",
       "      <td>2018-11-15 12:47:08</td>\n",
       "      <td>new employee lauren felder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77425</th>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>welder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81123</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celeste felder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82107</th>\n",
       "      <td>Clarifier Issue</td>\n",
       "      <td>2017-10-04 09:02:05</td>\n",
       "      <td>sylvia j felder</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84704</th>\n",
       "      <td>Job</td>\n",
       "      <td>2020-03-09 00:46:48</td>\n",
       "      <td>belder gmail com</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91591</th>\n",
       "      <td>IVA help</td>\n",
       "      <td>2019-05-13 19:19:31</td>\n",
       "      <td>hello name anthony felder friday paid day receive direct deposit accidently put wrong account number speaking bank told check rejected</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97033</th>\n",
       "      <td>Savings Plan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pipe welder apprentice</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103882</th>\n",
       "      <td>Other</td>\n",
       "      <td>2018-08-23 12:33:58</td>\n",
       "      <td>schelder quest health wellness</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106717</th>\n",
       "      <td>Enrollment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elderhorst</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         labels    session_start_cst  \\\n",
       "3352      General Account issue                  NaN   \n",
       "18603           Health Benefits  2020-12-31 23:00:21   \n",
       "19627     General Account issue                  NaN   \n",
       "21389            Vacation Issue  2022-12-15 08:37:43   \n",
       "23700                     Other  2020-03-09 00:46:48   \n",
       "31881            Coverage Issue  2017-10-30 10:25:20   \n",
       "37237           Health Benefits  2022-12-14 15:52:54   \n",
       "37577                     Other  2023-02-05 04:06:55   \n",
       "39278     General Account issue                  NaN   \n",
       "41627                  IVA help                  NaN   \n",
       "46638           Health Benefits  2020-10-13 16:41:11   \n",
       "60673   401k/403b/457 Clarifier                  NaN   \n",
       "67356           Health Benefits                  NaN   \n",
       "68844           Dependent issue                  NaN   \n",
       "69081     General Account issue  2018-11-15 12:47:08   \n",
       "77425                     Other                  NaN   \n",
       "81123                Enrollment                  NaN   \n",
       "82107           Clarifier Issue  2017-10-04 09:02:05   \n",
       "84704                       Job  2020-03-09 00:46:48   \n",
       "91591                  IVA help  2019-05-13 19:19:31   \n",
       "97033              Savings Plan                  NaN   \n",
       "103882                    Other  2018-08-23 12:33:58   \n",
       "106717               Enrollment                  NaN   \n",
       "\n",
       "                                                                                                                                                text  \\\n",
       "3352                                                                                                                                 wayne elderkin    \n",
       "18603                                                                                                                       password catweldersare     \n",
       "19627                                                                                                                                   sina melder    \n",
       "21389                                                           management provider prescription drugs blue cross blue shielder still cvs caremark     \n",
       "23700                                                                                                                    belder gmail com sarasota     \n",
       "31881           hi filled forms morning line taxes federal married exceptions missouri put divided monthly taxes wondering right help faye fielder     \n",
       "37237                                                  hi name andy i’m welder republic service would like apply health insurance help make happen     \n",
       "37577   wanted add son insurance looks like get added talked people well ahead time filled forms need make sure covered currently works hii welder     \n",
       "39278                                                                                                                                  ivory felder    \n",
       "41627                                                                                                                                       welders    \n",
       "46638                                                                                                    question able purchase elderberry counter     \n",
       "60673                                                                                                                pipe welder apprentice program    \n",
       "67356                                                                                                                                  lisa fielder    \n",
       "68844                                                                                                                                        melder    \n",
       "69081                                                                                                                   new employee lauren felder     \n",
       "77425                                                                                                                                        welder    \n",
       "81123                                                                                                                                celeste felder    \n",
       "82107                                                                                                                              sylvia j felder     \n",
       "84704                                                                                                                             belder gmail com     \n",
       "91591       hello name anthony felder friday paid day receive direct deposit accidently put wrong account number speaking bank told check rejected     \n",
       "97033                                                                                                                        pipe welder apprentice    \n",
       "103882                                                                                                              schelder quest health wellness     \n",
       "106717                                                                                                                                   elderhorst    \n",
       "\n",
       "       category  \n",
       "3352      Other  \n",
       "18603     Other  \n",
       "19627     Other  \n",
       "21389     Other  \n",
       "23700     Other  \n",
       "31881     Other  \n",
       "37237     Other  \n",
       "37577     Other  \n",
       "39278     Other  \n",
       "41627     Other  \n",
       "46638     Other  \n",
       "60673     Other  \n",
       "67356     Other  \n",
       "68844     Other  \n",
       "69081     Other  \n",
       "77425     Other  \n",
       "81123     Other  \n",
       "82107     Other  \n",
       "84704     Other  \n",
       "91591     Other  \n",
       "97033     Other  \n",
       "103882    Other  \n",
       "106717    Other  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated[(df_concatenated.text.str.contains('elder')) & (df_concatenated.category=='Other')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenated[df_concatenated.category=='Elder care'].to_excel('fasttext_only_elder_care_training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number og labels in label col where value of category col is Other\n",
    "df_concatenated[df_concatenated['category'] == 'Other']['labels'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated2 = df_concatenated.drop('session_start_cst', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2['labels'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2['category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenated2['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concatenated2['labels'] = df_concatenated2['labels'].fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "Other                 0.227158\n",
      "Health Benefits       0.101629\n",
      "Savings Plan          0.042729\n",
      "Clarifier Issue       0.036718\n",
      "Dependent issue       0.036023\n",
      "                        ...   \n",
      "RMSA                  0.000009\n",
      "Rebalancing Issue     0.000009\n",
      "Financial Advisors    0.000009\n",
      "Facilities Issue      0.000009\n",
      "Hysterectomy          0.000009\n",
      "Name: labels, Length: 166, dtype: float64\n",
      "\n",
      "Sampled dataset:\n",
      "Other              0.2336\n",
      "Health Benefits    0.1056\n",
      "Savings Plan       0.0374\n",
      "Dependent issue    0.0366\n",
      "Clarifier Issue    0.0354\n",
      "                    ...  \n",
      "Care Issue         0.0002\n",
      "Plan               0.0002\n",
      "Brokerage          0.0002\n",
      "HR related         0.0002\n",
      "Deferrals          0.0002\n",
      "Name: labels, Length: 130, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## get 5000 rows including all labels where category is Other in same proportion as original \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the number of folds to use for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Create a StratifiedKFold object to generate the cross-validation folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define an empty DataFrame to store the sampled data\n",
    "Other_sample_df = pd.DataFrame()\n",
    "\n",
    "# Split the DataFrame into training and testing sets using cross-validation\n",
    "for train_index, test_index in skf.split(df_concatenated2[df_concatenated2['category'] == 'Other'], \n",
    "                                         df_concatenated2[df_concatenated2['category'] == 'Other']['labels']):\n",
    "    # Obtain a random sample of 5000 rows from the training set\n",
    "    train_df = df_concatenated2.iloc[train_index]\n",
    "    train_df_other = train_df[train_df['category'] == 'Other']\n",
    "    train_df_other_sample = train_df_other.sample(n=5000//n_splits, random_state=42)\n",
    "    Other_sample_df = pd.concat([Other_sample_df, train_df_other_sample])\n",
    "    \n",
    "# Print the value counts of the label column in the original DataFrame and the sample\n",
    "print('Original dataset:')\n",
    "print(df_concatenated2[df_concatenated2['category'] == 'Other']['labels'].value_counts(normalize=True))\n",
    "print('\\nSampled dataset:')\n",
    "print(Other_sample_df['labels'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73147</th>\n",
       "      <td>Health Benefits</td>\n",
       "      <td>need find child care resources birth child</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92005</th>\n",
       "      <td>Loan related</td>\n",
       "      <td>coverage ending child</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93949</th>\n",
       "      <td>Other</td>\n",
       "      <td>checking child care plus wondering reason denied  hmcstmchildcarepluslandingpageopen</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18560</th>\n",
       "      <td>Form 1095 Issue</td>\n",
       "      <td>child colleague scholarship</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51422</th>\n",
       "      <td>Rollovers Clarifier</td>\n",
       "      <td>child goes state college covered insurance</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    labels  \\\n",
       "73147      Health Benefits   \n",
       "92005         Loan related   \n",
       "93949                Other   \n",
       "18560      Form 1095 Issue   \n",
       "51422  Rollovers Clarifier   \n",
       "\n",
       "                                                                                       text  \\\n",
       "73147                                           need find child care resources birth child    \n",
       "92005                                                               coverage ending child     \n",
       "93949  checking child care plus wondering reason denied  hmcstmchildcarepluslandingpageopen   \n",
       "18560                                                         child colleague scholarship     \n",
       "51422                                          child goes state college covered insurance     \n",
       "\n",
       "      category  \n",
       "73147    Other  \n",
       "92005    Other  \n",
       "93949    Other  \n",
       "18560    Other  \n",
       "51422    Other  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Other_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EC_model_train_df = pd.concat([df_concatenated2[df_concatenated2.category=='Elder care'], \n",
    "                              Other_sample_df]).sample(frac=1).reset_index(drop=True)#.to_excel('fasttext_EC_model_training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>hi life event need add children insurance</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other</td>\n",
       "      <td>elder care elderly care contentpage health care fsa day care dcap page</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Savings Plan</td>\n",
       "      <td>hi lisa accidently added two sons dependent children removed redo add fix proceed</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loan related</td>\n",
       "      <td>service offered hearst help elderly family number</td>\n",
       "      <td>Elder care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>much plan cost employee children plan</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels  \\\n",
       "0         Other   \n",
       "1         Other   \n",
       "2  Savings Plan   \n",
       "3  Loan related   \n",
       "4         Other   \n",
       "\n",
       "                                                                                  text  \\\n",
       "0                                          hi life event need add children insurance     \n",
       "1               elder care elderly care contentpage health care fsa day care dcap page   \n",
       "2  hi lisa accidently added two sons dependent children removed redo add fix proceed     \n",
       "3                                  service offered hearst help elderly family number     \n",
       "4                                              much plan cost employee children plan     \n",
       "\n",
       "     category  \n",
       "0       Other  \n",
       "1  Elder care  \n",
       "2       Other  \n",
       "3  Elder care  \n",
       "4       Other  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EC_model_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other         0.752899\n",
       "Elder care    0.247101\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EC_model_train_df.category.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6641, 3)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EC_model_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_intnt_entits(text):\n",
    "    doc = nlp(text)\n",
    "    intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "    entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "    return len(intents), len(entities)\n",
    "\n",
    "import re\n",
    "# def clean_text(text):\n",
    "#     clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#     return clean_text\n",
    "\n",
    "def extract_ner_entities(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "import numpy as np\n",
    "def length_entities(list_entities):\n",
    "    if (list_entities==np.nan or list_entities==None or list_entities==''):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list_entities)\n",
    "    \n",
    "# filter_func = lambda x: any(item[1] in [\"PERSON\", \"GPE\", \"ORG\", \"LOC\", \"FAC\"] for item in x)\n",
    "def filter_named_entities(text):\n",
    "    # Process the text using Spacy\n",
    "    doc = nlp(text)\n",
    "    # Filter out named entities (ORG, PERSON, and GPE tags)\n",
    "    filtered_words = [token.text for token in doc if token.ent_type_ not in ['ORG', 'PERSON', 'GPE', \"LOC\", \"FAC\"]]\n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "# list_1 = ['ira','RMD','HRdirect','livechat','what is my hsa','P45','Payslip?',\n",
    "#     'sps','F80.2','ub','What is YSA','Paystub please','Sh','mfv','C-128','ax','no is hsa','FormL564','HIS','cif','GreT','YSACard',\n",
    "#     'Heli','RxPCN','403(b)','Hsa yes or no','ypr','Gv','ONA?','What is UHC?','HC-2','uo','what is 4DX?','osh','what is my hsa?',\n",
    "#     'sPRAVATO','sdr','RMD’s','coverage?How','This is for my hsa','pto?','A&DD','childcareplus','fs','mbi','Is that my lowesbenefit.com',\n",
    "#     'hra yes','mri?']\n",
    "# list_2 = [word.lower() for word in list_1]\n",
    "\n",
    "def text_preprocess(dataframe):\n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    dataframe[['no_of_intents', 'no_of_entities']] = dataframe.apply(lambda x: pd.Series(count_intnt_entits(x['text'])), axis=1)  \n",
    "\n",
    "    dataframe['ner_enities'] = ''\n",
    "    dataframe.loc[dataframe['text']!='', 'ner_enities'] = dataframe.loc[dataframe['text']!='', 'text'].apply(extract_ner_entities)\n",
    "    dataframe['len_ner_enities'] = dataframe['ner_enities'].apply(length_entities)\n",
    "    dataframe3 = dataframe[dataframe['len_ner_enities']>0]\n",
    "    dataframe3['text'] = dataframe3['text'].apply(filter_named_entities)\n",
    "    dataframe6 = pd.concat([dataframe[dataframe['len_ner_enities']==0], dataframe3], axis = 0)\n",
    "    dataframe6 = dataframe6.drop(['no_of_intents','no_of_entities','ner_enities','len_ner_enities'], axis=1)\n",
    "\n",
    "    dataframe6['text'] = dataframe6['text'].str.strip()\n",
    "    \n",
    "    return dataframe6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/3440049557.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[['no_of_intents', 'no_of_entities']] = dataframe.apply(lambda x: pd.Series(count_intnt_entits(x['text'])), axis=1)\n",
      "/tmp/ipykernel_15/3440049557.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[['no_of_intents', 'no_of_entities']] = dataframe.apply(lambda x: pd.Series(count_intnt_entits(x['text'])), axis=1)\n",
      "/tmp/ipykernel_15/3440049557.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['ner_enities'] = ''\n",
      "/tmp/ipykernel_15/3440049557.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.loc[dataframe['text']!='', 'ner_enities'] = dataframe.loc[dataframe['text']!='', 'text'].apply(extract_ner_entities)\n",
      "/tmp/ipykernel_15/3440049557.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['len_ner_enities'] = dataframe['ner_enities'].apply(length_entities)\n",
      "/tmp/ipykernel_15/3440049557.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe3['text'] = dataframe3['text'].apply(filter_named_entities)\n"
     ]
    }
   ],
   "source": [
    "df_combined_ec_model_data_2 = text_preprocess(EC_model_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6539, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_ec_model_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['labels', 'text', 'category'], dtype='object')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_ec_model_data_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other         4898\n",
       "Elder care    1641\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_ec_model_data_2['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_ec_model_data_2.to_excel('final_ec_model_data_v6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unseen = pd.read_excel('unseen_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WelcomeUserFollowUp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wellness Program Incentive Credit or Rewards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Medical Plan Credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WelcomeUser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input\n",
       "0                                           TGT\n",
       "1                           WelcomeUserFollowUp\n",
       "2  Wellness Program Incentive Credit or Rewards\n",
       "3                           Medical Plan Credit\n",
       "4                                   WelcomeUser"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "# from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# elder_care_sentences = df_combined_ec_model_data_2[df_combined_ec_model_data_2['category']=='Elder care']['text'].to_list() # a list of Elder care related sentences\n",
    "# other_sentences =df_combined_ec_model_data_2[df_combined_ec_model_data_2['category']=='Other']['text'].to_list() # a list of sentences not related to Elder care\n",
    "# categories = [1] * len(elder_care_sentences) + [0] * len(other_sentences)\n",
    "\n",
    "# # Convert data to sequences of integers\n",
    "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "# tokenizer.fit_on_texts(elder_care_sentences + other_sentences)\n",
    "# elder_care_seq = tokenizer.texts_to_sequences(elder_care_sentences)\n",
    "# other_seq = tokenizer.texts_to_sequences(other_sentences)\n",
    "\n",
    "# # Pad sequences to the same length\n",
    "# max_len = 100\n",
    "# elder_care_seq = tf.keras.preprocessing.sequence.pad_sequences(elder_care_seq, maxlen=max_len)\n",
    "# other_seq = tf.keras.preprocessing.sequence.pad_sequences(other_seq, maxlen=max_len)\n",
    "\n",
    "# # Create input and output tensors\n",
    "# inputs = Input(shape=(max_len,))\n",
    "# x = Embedding(len(tokenizer.word_index)+1, 128)(inputs)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = GlobalMaxPooling1D()(x)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "# outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# # Compile the model\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# x_train = np.concatenate((elder_care_seq, other_seq))\n",
    "# y_train = np.array(categories)\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Import necessary libraries\n",
    "# import torch\n",
    "# import transformers\n",
    "\n",
    "# # Define the model and tokenizer\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# # Define the optimizer and loss function\n",
    "# optimizer = transformers.AdamW(model.parameters(), lr=1e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Define the training loop\n",
    "# def train(model, dataloader, optimizer, loss_fn):\n",
    "#     model.train()\n",
    "#     for batch in dataloader:\n",
    "#         inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
    "#         labels = batch['label']\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(**inputs, labels=labels)\n",
    "#         loss = loss_fn(outputs.logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Define the evaluation function\n",
    "# def evaluate(model, dataloader):\n",
    "#     model.eval()\n",
    "#     total_loss, total_accuracy = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
    "#             labels = batch['label']\n",
    "#             outputs = model(**inputs, labels=labels)\n",
    "#             total_loss += outputs.loss.item()\n",
    "#             total_accuracy += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "#     return total_loss / len(dataloader.dataset), total_accuracy / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = df_combined_ec_model_data_2[['text', 'category']].rename(columns={'category':'label'}).replace(\n",
    "#     {'Elder care':1, 'Other':0}).to_dict(orient='records')\n",
    "\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define the training data and validation data\n",
    "# train_data = [(x['text'], x['label']) for x in data[:6000]]\n",
    "# val_data = [(x['text'], x['label']) for x in data[6000:]]\n",
    "\n",
    "# # Convert the data to PyTorch datasets\n",
    "# train_dataset = torch.utils.data.Dataset(train_data)\n",
    "# val_dataset = torch.utils.data.Dataset(val_data)\n",
    "\n",
    "# # Define the dataloaders\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(5):\n",
    "#     train(model, train_dataloader, optimizer, loss_fn)\n",
    "#     train_loss, train_acc = evaluate(model, train_dataloader)\n",
    "#     val_loss, val_acc = evaluate(model, val_dataloader)\n",
    "#     print(f'Epoch {epoch + 1}: train loss = {train_loss:.4f}, train acc = {train_acc:.4f}, val loss = {val_loss:.4f}, val acc = {val_acc:.4f}')\n",
    "\n",
    "# # Define a function to predict the class of a sentence\n",
    "# def predict_class(model, sentence):\n",
    "#     inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "#     outputs = model(**inputs)\n",
    "#     probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "#     if probabilities[0][0] > probabilities[0][1]:\n",
    "#         return 'Other'\n",
    "#     else:\n",
    "#         return 'Elder care'\n",
    "\n",
    "# # Test the model on a sentence\n",
    "# sentence = 'My grandmother needs help with cooking and cleaning.'\n",
    "# predicted_class = predict_class(model, sentence)\n",
    "# print(f'The predicted class for \"{sentence}\" is \"{predicted_class}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_combined_ec_model_data_2 = pd.read_excel('final_ec_model_data_v6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "labels         0\n",
       "text          12\n",
       "category       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_ec_model_data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined_ec_model_data_2 = df_combined_ec_model_data_2.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi life event need add children insurance', 'Other')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df_combined_ec_model_data_2[['text', 'category']].rename(columns={'category':'label'})\n",
    "dataset = list(data.itertuples(index=False, name=None))\n",
    "\n",
    "#.replace({'Elder care':1, 'Other':0}).to_dict(orient='records')\n",
    "\n",
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = []\n",
    "for text, label in dataset:\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    tokenized_dataset.append((input_ids, attention_mask, label))\n",
    "\n",
    "# Convert the tokenized dataset into PyTorch tensors\n",
    "input_ids = pad_sequence([torch.tensor(x[0]) for x in tokenized_dataset], batch_first=True)\n",
    "attention_mask = pad_sequence([torch.tensor(x[1]) for x in tokenized_dataset], batch_first=True)\n",
    "labels = torch.tensor([1 if x[2] == \"Elder care\" else 0 for x in tokenized_dataset])\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Create the DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = TextClassifier(num_labels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         input_ids, attention_mask, labels = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(input_ids, attention_mask)\n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(\"Epoch {}/{} complete. Loss: {}\".format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 complete. Loss: 0.0006256530177779496\n",
      "Epoch 2/5 complete. Loss: 0.00013219437096267939\n",
      "Epoch 3/5 complete. Loss: 0.00011121608258690685\n",
      "Epoch 4/5 complete. Loss: 0.0002215855201939121\n",
      "Epoch 5/5 complete. Loss: 0.00030632095877081156\n",
      "Loaded best checkpoint from epoch 4.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to save the checkpoints\n",
    "checkpoint_dir = \"checkpoints/\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Define the checkpoint file name\n",
    "checkpoint_file = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
    "\n",
    "# Load the last saved checkpoint\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_file):\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(\"Loaded checkpoint from epoch {}.\".format(start_epoch - 1))\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save a checkpoint at the end of each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_file)\n",
    "\n",
    "    print(\"Epoch {}/{} complete. Loss: {}\".format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Load the best saved checkpoint\n",
    "if os.path.exists(checkpoint_file):\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(\"Loaded best checkpoint from epoch {}.\".format(checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_intnt_entits(text):\n",
    "    if str(text).isnumeric():\n",
    "        return 0,0    \n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "        intents = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "        entities = [token.text for token in doc if token.pos_ in {'NOUN', 'PROPN', 'ADJ', 'NUM', 'ADV'}]\n",
    "    except:\n",
    "        print(text)\n",
    "        raise\n",
    "    return len(intents), len(entities)\n",
    "\n",
    "def extract_ner_entities(sentence):\n",
    "    doc = nlp(str(sentence))\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def length_entities(list_entities):\n",
    "    if (list_entities==np.nan or list_entities==None or list_entities==''):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list_entities)\n",
    "    \n",
    "def filter_named_entities(text):\n",
    "    # Process the text using Spacy\n",
    "    doc = nlp(str(text))\n",
    "    # Filter out named entities (ORG, PERSON, and GPE tags)\n",
    "    filtered_words = [token.text for token in doc if token.ent_type_ not in ['ORG', 'PERSON', 'GPE', \"LOC\", \"FAC\"]]\n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "def text_preprocess(col):\n",
    "    df = pd.DataFrame({ 'text': col })\n",
    "    df = df.drop_duplicates()\n",
    "    df['text'] = df['text'].str.replace('\\d+', '')\n",
    "    df[['no_of_intents', 'no_of_entities']] = df.apply(lambda x: pd.Series(count_intnt_entits(x['text'])), axis=1)  \n",
    "\n",
    "    df['ner_enities'] = ''\n",
    "    df.loc[df['text']!='', 'ner_enities'] = df.loc[df['text']!='', 'text'].apply(extract_ner_entities)\n",
    "    df['len_ner_enities'] = df['ner_enities'].apply(length_entities)\n",
    "    df3 = df[df['len_ner_enities']>0]\n",
    "    df3['text'] = df3['text'].apply(filter_named_entities)\n",
    "    df6 = pd.concat([df[df['len_ner_enities']==0], df3], axis = 0)\n",
    "    df6 = df6.drop(['no_of_intents','no_of_entities','ner_enities','len_ner_enities'], axis=1)\n",
    "\n",
    "    df6['text'] = df6['text'].str.strip()\n",
    "    \n",
    "    return df6['text'].to_list()\n",
    "\n",
    "def clean_text(text_list):\n",
    "    # Clean the text\n",
    "    text_list = text_preprocess(text_list)\n",
    "    #text_list = [text for text in text_list if text.strip() and not set(text).issubset(set(string.punctuation + string.whitespace))]\n",
    "    text_list1 = []\n",
    "    for text in text_list:\n",
    "        if isinstance(text, str):\n",
    "            if text.strip() and not set(text).issubset(set(string.punctuation + string.whitespace)):\n",
    "                text_list1.append(text)\n",
    "            \n",
    "    text_list = text_list1\n",
    "    \n",
    "    text_list = [x.lower() for x in text_list]\n",
    "    # Define a translation table to replace punctuation and special characters with empty string\n",
    "    translator = str.maketrans(string.punctuation + \"_\", \" \" * len(string.punctuation + \"_\"))\n",
    "    # Loop through each text in the list and clean it\n",
    "    cleaned_list = []\n",
    "    for text in text_list:\n",
    "        # Replace punctuation and special characters with empty string\n",
    "        cleaned_text = text.translate(translator)\n",
    "        # Remove any remaining special characters, punctuation, or whitespaces\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        cleaned_list.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45215/3565708624.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('\\d+', '')\n",
      "/tmp/ipykernel_45215/3565708624.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['text'] = df3['text'].apply(filter_named_entities)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "It looks like the config file at 'model.pth' is not a valid JSON file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_from_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_config_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:755\u001b[0m, in \u001b[0;36mPretrainedConfig._dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 755\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(text)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Use the trained model to make predictions for each input text\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_text \u001b[38;5;129;01min\u001b[39;00m input_texts:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:1840\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1839\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1840\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:534\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    537\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:561\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_files\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:674\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like the config file at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid JSON file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m     )\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;241m==\u001b[39m config_file:\n\u001b[1;32m    679\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading configuration file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: It looks like the config file at 'model.pth' is not a valid JSON file."
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import string\n",
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# # Read Excel file\n",
    "# data = pd.read_excel('unseen_data.xlsx', sheet_name='Sheet1')\n",
    "# input_texts = clean_text(data['text'].head(1000).tolist())\n",
    "\n",
    "# # Use the trained model to make predictions for each input text\n",
    "# model_path = \"model.pth\"\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# predicted_labels = []\n",
    "# for input_text in input_texts:\n",
    "#     input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=True)])\n",
    "#     attention_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids[0]]])\n",
    "#     logits = model(input_ids, attention_mask)\n",
    "#     probs = nn.functional.softmax(logits, dim=-1)\n",
    "#     predicted_label = torch.argmax(probs, dim=-1)\n",
    "#     if predicted_label == 1:\n",
    "#         predicted_labels.append(\"Elder care\")\n",
    "#     else:\n",
    "#         predicted_labels.append(\"Other\")\n",
    "\n",
    "# # Save the predicted labels to a new column in the Excel file\n",
    "# data['Predicted Label'] = predicted_labels\n",
    "# data.to_excel('BERT_ec_pred_file_2_epoch.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45215/3565708624.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('\\d+', '')\n",
      "/tmp/ipykernel_45215/3565708624.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['text'] = df3['text'].apply(filter_named_entities)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "It looks like the config file at 'model.pth' is not a valid JSON file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_from_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_config_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:755\u001b[0m, in \u001b[0;36mPretrainedConfig._dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 755\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(text)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Use the trained model to make predictions for each input text\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_text \u001b[38;5;129;01min\u001b[39;00m input_texts:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py:1840\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1839\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1840\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:534\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    537\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:561\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_files\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py:674\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like the config file at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid JSON file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m     )\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;241m==\u001b[39m config_file:\n\u001b[1;32m    679\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading configuration file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: It looks like the config file at 'model.pth' is not a valid JSON file."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel('unseen_data.xlsx', sheet_name='Sheet1')\n",
    "input_texts = clean_text(data['text'].head(1000).tolist())\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Use the trained model to make predictions for each input text\n",
    "model_path = \"model.pth\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "predicted_labels = []\n",
    "for input_text in input_texts:\n",
    "    input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=True)])\n",
    "    attention_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids[0]]])\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_label = torch.argmax(probs, dim=-1)\n",
    "    if predicted_label == 1:\n",
    "        predicted_labels.append(\"Elder care\")\n",
    "    else:\n",
    "        predicted_labels.append(\"Other\")\n",
    "\n",
    "# Save the predicted labels to a new column in the Excel file\n",
    "data['Predicted Label'] = predicted_labels\n",
    "data.to_excel('BERT_ec_pred_file_2_epoch.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/3565708624.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('\\d+', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15/3565708624.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['text'] = df3['text'].apply(filter_named_entities)\n"
     ]
    }
   ],
   "source": [
    "# # Predict new sentences\n",
    "# new_sentences =  clean_text(df_unseen['input'].head(2000).to_list()) # a list of new sentences to predict\n",
    "# new_seq = tokenizer.texts_to_sequences(new_sentences)\n",
    "# new_seq = tf.keras.preprocessing.sequence.pad_sequences(new_seq, maxlen=max_len)\n",
    "# predictions = model.predict(new_seq)\n",
    "# pred_df = pd.DataFrame()\n",
    "# pred_df['text'] = new_sentences\n",
    "# pred_df['pred'] = np.where(predictions>0.5, 'Elder care', 'Other')\n",
    "# pred_df['pred_probab'] = predictions\n",
    "# pred_df.to_csv('ec_cnn_pred_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert data to fasttext format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Excel file path\n",
    "# excel_file = \"final_ec_model_data_v6.xlsx\"\n",
    "\n",
    "# # Sheet name\n",
    "# # sheet_name = \"Sheet1\"\n",
    "\n",
    "# # FastText training file path\n",
    "# train_file = \"EC_fasttext_train_file.txt\"\n",
    "\n",
    "# # Load Excel data into a Pandas dataframe\n",
    "# df = pd.read_excel(excel_file)\n",
    "\n",
    "# # Open the FastText training file and write the data in the appropriate format\n",
    "# with open(train_file, \"w\") as f:\n",
    "#     for i, row in df.iterrows():\n",
    "#         text = row[\"text\"]\n",
    "#         label = '_'.join(row[\"category\"].split())\n",
    "#         f.write(f\"__label__{label} {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Excel file path\n",
    "# excel_file1 = \"EC_unseen_data_v5.xlsx\"\n",
    "\n",
    "# # Sheet name\n",
    "# # sheet_name = \"Sheet1\"\n",
    "\n",
    "# # FastText training file path\n",
    "# unseen_file = \"EC_fasttext_unseen_file.txt\"\n",
    "\n",
    "# # Load Excel data into a Pandas dataframe\n",
    "# df2 = pd.read_excel(excel_file1)\n",
    "\n",
    "# # Open the FastText training file and write the data in the appropriate format\n",
    "# with open(unseen_file, \"w\") as f:\n",
    "#     for i, row in df2.iterrows():\n",
    "#         text = row[\"text\"]\n",
    "#         label = '_'.join(row[\"category\"].split())\n",
    "        # f.write(f\"__label__{label} {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building fasttext classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# print(fasttext._version_)\n",
    "# !pip show fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import fasttext\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# train_data = \"EC_fasttext_train_file.txt\"\n",
    "# # test_data = \"EC_fasttext_unseen_file.txt\"\n",
    "\n",
    "# # Set up hyperparameters\n",
    "# lr = 1.0\n",
    "# word_ngrams = 3\n",
    "\n",
    "# # Initialize the model\n",
    "# classifier = fasttext.train_supervised(input=train_data, lr=lr, word_ngrams=word_ngrams)\n",
    "\n",
    "# # Set the number of epochs\n",
    "# num_epochs = 50\n",
    "\n",
    "# # Train the model for each epoch and print the results\n",
    "# for epoch in range(1, num_epochs+1):\n",
    "#     model = fasttext.train_supervised(input=train_data, \n",
    "#                                            lr=lr, word_ngrams=word_ngrams, \n",
    "#                                            epoch=epoch)\n",
    "\n",
    "#     # Evaluate model on test data\n",
    "#     # result = classifier.test(test_data)\n",
    "#     # print(result)  # Debugging line\n",
    "\n",
    "#     # if len(result) == 3:\n",
    "#     #     num_samples, precision, recall = result\n",
    "#     #     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     # else:\n",
    "#     #     precision, recall, f1_score = result[1], result[2], result[3]\n",
    "\n",
    "#     # Calculate confusion matrix and accuracy\n",
    "#     # y_true = []\n",
    "#     # y_pred = []\n",
    "#     # with open(test_data, \"r\") as f:\n",
    "#     #     for line in f:\n",
    "#     #         splited = line.split(\" \")\n",
    "#     #         label = splited[0]\n",
    "#     #         text = ' '.join(splited[1:])\n",
    "#     #         y_true.append(label)\n",
    "#     #         y_pred.append(classifier.predict(text.strip())[0][0])\n",
    "\n",
    "# #     cm = confusion_matrix(y_true, y_pred)\n",
    "# #     accuracy = sum(cm[i][i] for i in range(len(cm))) / sum(sum(cm[i]) for i in range(len(cm)))\n",
    "\n",
    "# #     # Print results for current epoch\n",
    "# #     print(f\"Epoch: {epoch} - Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}, Accuracy: {accuracy}\")\n",
    "# #     print(\"Confusion Matrix:\")\n",
    "# #     print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.save_model(\"ec_fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "# #df_pred = pd.DataFrame(columns=['text', 'actual_label', 'predicted_label', 'score'])\n",
    "\n",
    "# list_pred_rows = []\n",
    "# with open(test_data, \"r\") as g:\n",
    "#     for line in g:\n",
    "#         splited = line.split(\" \")\n",
    "#         actual_label = splited[0].replace('__label__','').replace('_',' ')\n",
    "#         text = ' '.join(splited[1:]).strip()\n",
    "#         predicted_label, score = classifier.predict(text)\n",
    "#         list_pred_rows.append({'text':text, 'actual_label':actual_label, \n",
    "#                             'predicted_label':predicted_label[0].replace('__label__','').replace('_',' '),\n",
    "#                                'score':score})\n",
    "\n",
    "# #df_pred = pd.concat([df_pred, pd.DataFrame(list_pred_rows)])\n",
    "\n",
    "# # Save the predictions to an Excel file\n",
    "# pd.DataFrame(list_pred_rows).to_excel(\"EC_fasttext_predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAflklEQVR4nO3debxd473H8c/3nCQkJCSChiBoQtOUGmosNZVSvamSolwpaY+q0hrqyr3V0DaqqFapIca05iEtqoiGSLmViJhjSIobIYSIIYaQk9/9Y63Ddpxhn521zt7r5Pv2Wq+z9rPWfp5nS/Lbz/mtZz1LEYGZmRVHXbU7YGZmHePAbWZWMA7cZmYF48BtZlYwDtxmZgXjwG1mVjAO3GZmGZN0qaT5kh5vVn6UpKclPSHp9JLy0ZJmp8f2aK/+bnl02sxsOXc5cC7wp6YCSTsDw4FNImKxpDXS8qHAAcDngbWAf0gaEhGNrVXuEbeZWcYiYgrwerPiI4DTImJxes78tHw4cE1ELI6I54DZwFZt1V+zI+7n377Ft3Tap6zXe0i1u2A1SGykZa2j57oHlh1z3n/hmsOBhpKicRExrp23DQF2kDQWeB84PiIeANYG7i85b25a1qqaDdxmZp1JKj8BkQbp9gJ1c92AvsA2wJeA6yRtALT0pdPml4gDt5kZoPwzx3OBCZEsEDVN0lKgf1q+Tsl5A4GX2qrIOW4zM5IRd7lbhf4K7JK0pSFAD+A14GbgAEkrSFofGAxMa6sij7jNzOhYqqT9unQ1sBPQX9JcYAxwKXBpOkXwA2BkOvp+QtJ1wExgCXBkWzNKwIHbzAwAqT6zuiLiwFYOHdzK+WOBseXW78BtZka2I+68OXCbmeHAbWZWOJ0wqyQzDtxmZnjEbWZWOA7cZmYFU5fhrJK8OXCbmeERt5lZ4Thwm5kVjAO3mVnhOHCbmRVKXV1xwmFxempmliPfgGNmVjDOcZuZFYy0zE8/6zQO3GZmeMRtZlY4znGbmRWMZ5WYmRWMR9xmZkVToBx3cXpqZpajLJ/yLulSSfPTBwM3P3a8pJDUv6RstKTZkp6WtEd79Ttwm5mRTAcsdyvD5cDXWmhjHeCrwJySsqHAAcDn0/ecp3aeXOzAbWZGkuMud2tPREwBXm/h0O+AE4AoKRsOXBMRiyPiOWA2sFVb9Ttwm5kBqqsvf5MaJE0v2RrarV/6D+DFiHik2aG1gRdKXs9Ny1rli5NmZtChYWxEjAPGlXu+pF7A/wC7t3S4pSbaqs+B28wMIN9b3jcE1gceSXPkA4EZkrYiGWGvU3LuQOCltipzqsTMDJLAXe7WQRHxWESsERGDImIQSbDePCJeBm4GDpC0gqT1gcHAtLbqyy1wSxoiaVLTdBhJm0j6WV7tmZktk7oObO2QdDXwL2AjSXMljWrt3Ih4ArgOmAncDhwZEY1t1Z9nquQi4KfAhWnnHpV0FfCrHNs0M6tI1GWXKomIA9s5PqjZ67HA2HLrzzNw94qIac3mPC7JsT0zs8plGLjzlmfgfk3ShqRXRyXtB8zLsT0zs8p5PW4AjiSZLrOxpBeB54CDcmzPzKxyxYnb+QTu9HbNIyJiN0krAXUR8XYebZmZZWJ5T5VERKOkLdL9d/Jow8wsU06VAPCQpJuB64GPgndETMixTTOzytQ7cAP0AxYAu5SUBeDAbWa1pzhxO7/AHRGH5lW3mVnWwqkSkLQiMIpkjdkVm8oj4rC82jQzq1iBLk7muVbJn4HPAHsA95AsnOKZJWZWm9SBrcryDNyfjYiTgHciYjzwdeALObZnZla5HBeZylqeFyc/TH++IWkY8DIwKMf2zMwq51klAIyT1Bf4GcmyhSsDP8+xPTOzytXASLpcec4quTjdnQJskFc7ZmaZKFDgznM97lMlrVryuq8kL+lqZrUpw/W485ZnF/aMiDeaXkTEQmCvHNszM6ucL04CUC9phYhYDCCpJ7BCju2ZmVUsfHESgCuASZIuI7nV/TBgfI7tdQkfLP6Q475/Hh9+uITGxqXssOsmHHL4Hkz5xyP8edxEXnhuPn8YfzRDhq7TfmXWZe2yy/dYaaWe1NfVUV9fz40Tzqp2l4qvBkbS5crz4uTpkh4FdiOZsv7LiLgjr/a6iu49unH6BT+gZ68VWLKkkWNHncuXttuYQRt+hp+fPpI/nHpDtbtoNeJP48fSt1+fanej6yhO3M51xE1E3E7y8EsrkyR69koySkuWNNK4ZCkSrLv+mlXumVkXl+Et75IuBfYG5kfEsLTsDOAbwAfAv4FDm64DShpNskRII3B0e4PcGrg+as01Ni7liO+cxf5fPZnNth7MxsPWq3aXrMYIGDXq53zrW8dw7bUeG2Ui24uTlwNfa1Z2JzAsIjYBngFGJ81qKHAAybpOXwPOSx9G06qaCtySGiRNlzT9qsuW37+M9fV1nH/VsVz595N4+okXeH62H9Vpn3TV1b9hwl9+z0UXjeGqK//OAw88Xu0uFV+Ga5VExBTg9WZlEyOi6YHp95Os3wQwHLgmIhZHxHPAbGCrturPJXBLqpd0RUffFxHjImLLiNjyO4c2/7Ja/qzcuyebbrEhD/zr6Wp3xWrMmmuuBsBqq63Kbl/dhkcfnVXlHnUB3erK3koHmenW0MHWDgNuS/fXBl4oOTY3LWtVLoE7IhqB1SX1yKP+ruyNhYtY9PZ7ACx+/0NmTJvFOoPWqHKvrJa8++77LFr07kf79933MEMGr1vlXhVfqANbySAz3caV246k/wGWAFc2FbXUnbbqyPPi5PPAfenjy0ofXeZ5S214/bW3OHPMNSxdGixdupQdv7op2+wwlPvufozzzvgrby5cxEk/uYQNh6zFqed29EveuoIFC97gR0eeCkBjYyN77/0Vdthxiyr3qgvohPW4JY0kuWi5a0Q0Bee5QOn83oHAS23W8/F7M+/gmJbKI+KUct7//Nu35NMxK7T1eg+pdhesBomNljnqbnD4jWXHnGcv3Lfd9iQNAv5WMqvka8BZwFci4tWS8z4PXEWS114LmAQMTjMXLcpzHvcpaadW8pPezazmZTsd8GpgJ6C/pLnAGJJZJCsAdyqZmXJ/RPwgIp6QdB0wkySFcmRbQRvyfXTZtsAlJMu5ritpU+DwiPhhXm2amVUswyt+EXFgC8WXtHH+WGBsufXnOR3w9ySPLVsAEBGPADvm2J6ZWeXq68rfqizvOydf0Ccnq7c5/DczqxY/5T3xgqTtgEinBR4NPJlje2Zmlav+QLpseQbuHwBnk0wknwtMBI7MsT0zs8p1wnTArOQ5q+Q14KC86jczy9TynCqRdA5t3PUTEUdn3aaZ2TIr0IMU8sjqTAceBFYENgdmpdsX8cVJM6tRUaeyt2rLfMQdEeMBJH0X2DkiPkxfX0CS5zYzqz01EJDLlefFybWA3ny8tOHKaZmZWe1ZnnPcJU4DHpJ0d/r6K8DJObZnZlY5TweEiLhM0m3A1mnRiRHxcl7tmZktk+V5xC1p82ZFTQuEryVprYiYkXWbZmbLrFtxhtx5jLh/28axAHbJoU0zs2WyXN/yHhE7Z12nmVnuijPgzr6rkk4o2R/R7NipWbdnZpaJbJ/ynqs8vmMOKNkf3eyYnwBsZrWpTuVvVZZHjlut7Lf02sysNtRAQC5XHoE7Wtlv6bWZWU2IAq1Vkkfg3lTSWySj657pPunrFXNoz8xs2dVA7rpcmee4I6I+IvpERO+I6JbuN73unnV7ZmaZyDDHLelSSfMlPV5S1k/SnZJmpT/7lhwbLWm2pKcl7dFuVyv+kGZmXYk6sLXvcj49GeNEYFJEDAYmpa+RNJRkUsfn0/ecJ6m+rcoduM3MgLq68rf2RMQUPl5gr8lwYHy6Px74Zkn5NRGxOCKeA2YDW7XZ1/I/lplZ19WRwC2pQdL0kq2hjCbWjIh5AOnPNdLytfl4aRBIHvW4dlsV5fqUdzOzolAHLk5GxDhgXFZNt9REW2/wiNvMjE65cfIVSQOStjQAmJ+WzwXWKTlvIPBSWxU5cJuZ0SmB+2ZgZLo/ErippPwASStIWh8YDExrqyKnSszMAGU4jJV0NbAT0F/SXGAMycNlrpM0CpgDjACIiCckXQfMBJYAR0ZEm8/ndeA2MyPb+28i4sBWDu3ayvljgbHl1u/AbWYG1BcocezAbWZGoe54d+A2M4OOTQesNgduMzOyvTiZNwduMzOcKjEzK5xy1iCpFQ7cZmYU6gE4rQduSefQxv3yEXF0Lj0yM6uCrpIqmd5pvTAzq7IuEbgjYnxrx8zMuhoVKFfSbo5b0urAfwFDKXlmZETskmO/zMw6VZFG3OVcR70SeBJYHzgFeB54IMc+mZl1uiyfgJN7X8s4Z7WIuAT4MCLuiYjDgG1y7peZWafK8FnBuStnOuCH6c95kr5OssD3wPy6ZGbW+YqUKikncP9K0irAccA5QB/gmFx7ZWbWybrULe8R8bd0901g53y7Y2ZWHV1qxC3pMlq4ESfNdZuZdQldbXXAv5XsrwjsQzsPsjQzK5pamC1SrnJSJTeWvk6fpfaP3HpkZlYFBRpwV7TI1GBg3aw70tyg3hvl3YQVUM91x1S7C1aD3ptz9TLXkeU0P0nHAN8jSTM/BhwK9AKuBQaR3A/z7YhYWEn97f5yIOltSW81bcAtJHdSmpl1GVnN45a0NnA0sGVEDAPqgQOAE4FJETEYmJS+rkg5qZLelVZuZlYUdWp1MdRKdAN6SvqQZKT9EjAa2Ck9Ph6YTIWD4HJG3JPKKTMzK7JuKn+T1CBpesnW0FRPRLwInAnMAeYBb0bERGDNiJiXnjMPWKPivrZ2QNKKJN8U/SX1BZp+QegDrFVpg2ZmtagjI+6IGAeMa+lYGi+Hk6zv9AZwvaSDM+jiR9pKlRwO/IQkSD/Ix4H7LeCPWXbCzKzaMrw4uRvwXES8CiBpArAd8IqkARExT9IAYH6lDbS1HvfZwNmSjoqIcyptwMysCDKcxj0H2EZSL+A9YFeSB9O8A4wETkt/3lRpA+VMB1wqadWIeAM++jXgwIg4r9JGzcxqTVYj7oiYKukGYAawBHiIJK2yMnCdpFEkwX1EpW2UE7i/HxEfpUYiYqGk7wMO3GbWZSjDWSURMQZoftPBYpLR9zIrJ3DXSVJEBICkeqBHFo2bmdWKbl3szsk7SIb3F5DcBfQD4LZce2Vm1skynsedq3IC938BDcARJDNLHgIG5NkpM7POVgtPtilXuxdSI2IpcD/wLLAlSY7myZz7ZWbWqeo6sFVbWzfgDCG5v/5AYAHJ4ihEhB+mYGZdTpFG3G2lSp4C/gl8IyJmw0crXpmZdTlFynG3NerfF3gZuFvSRZJ25eO7J83MupSOrFVSba0G7oj4S0TsD2xMsorVMcCaks6XtHsn9c/MrFPUKcreqq2ci5PvRMSVEbE3MBB4mGVYR9bMrBZltR53p/S1IydHxOsRcWFE7JJXh8zMqqFIgbuSR5eZmXU5tTDNr1wO3GZmQLe66ueuy+XAbWaGR9xmZoVTC7nrcjlwm5mR7bKueXPgNjPDI24zs8JxjtvMrGA8q8TMrGCKlCop0m8HZma5qe/A1h5Jq0q6QdJTkp6UtK2kfpLulDQr/dm30r46cJuZkfkiU2cDt0fExsCmJA+fORGYFBGDgUksw5pPDtxmZmS3VomkPsCOwCUAEfFBRLwBDAfGp6eNB75ZcV8rfaOZWVfSkcAtqUHS9JKtoaSqDYBXgcskPSTpYkkrAWtGxDyA9OcalfbVFyfNzIDuHRjGRsQ4YFwrh7sBmwNHRcRUSWeT8VLYHnGbmZFpjnsuMDcipqavbyAJ5K9IGgCQ/pxfcV8rfaOZWVeSVY47Il4GXpC0UVq0KzATuBkYmZaNBG6qtK9OlZiZUd40vw44CrhSUg/gWeBQkoHydZJGAXOAEZVW7sBtZka2N+BExMPAli0c2jWL+h24zcyA7r7l3cysWIp0y7sDt5kZDtxmZoXjwG1mVjD1BXoCTm7zuCXVSdour/rNzLJU14Gt2nLrQ0QsBX6bV/1mZlnqVlf+Vm15d2GipH0lFSh7ZGbLo3pF2Vu15Z3jPhZYCWiU9B4gICKiT87tmpl1iC9OpiKid571m5llpUiBO9dUiRIHSzopfb2OpK3ybNPMrBJZLTLVKX3Nuf7zgG2B76SvFwF/zLlNM7MO614XZW/VlneOe+uI2FzSQwARsTBdLcvMrKbUwGSRsuUduD+UVA8EgKTVgaU5t9mlNDY2su++x7Lmmv248MIx1e6OLYMLzjicPXfdjFcXvMWWXz2hxXN22OZznDHmELp378aC199m92//Ypna7NGjG5f87ods9oX1eX3hIg4+8mzmzH2NTYauxx/GHkbv3r1obFzK6ef+hRtuuX+Z2iq6WkiBlCvvL5k/AH8B1pA0FrgXODXnNruUP/3pFjbccGC1u2EZ+PP19zD8kNNaPb5Kn16cPfYwRow6ky12+ykHHfH7suted2B/7rj2pE+Vf3f/nVn45jsM2/EYzrn474wdnWQt331vMaOOOZ8tdvspww85jdPHHMIqfXp1+DN1JfUqf6u2XAN3RFwJnAD8GpgHfDMirs+zza7k5ZdfY/LkB9hvv92r3RXLwH3TnuL1Nxa1enz/4dtz020P8MJLCwB4dcFbHx07YJ8v88+bf8n9t/2ac349iroyh4d7774FV94wBYAJf5/KTtsPA2D2cy/z7+dfBmDeKwt59bW36N9v+Z6lm+Gjy/Lvaye0MYtk1H0z8I6kdTuhzS7h1FMv4qc/PZS6uiJl36xSgzcYwKqrrMQd157EfbeO5Tv77gDARp9di/2+sQ07f+tkttlzNI2NwQH7fLmsOtf6TD/mpl8EjY1Leevtd1mt7ydn6W656Yb06N6NZ//vlUw/T9EUaVZJrjluSUcBY4BXgEbSG3CATVo5vwFoALjwwl/Q0LB/nt2raXffPY1+/VZh2LDPMnXqY9XujnWCbvV1bP6F9dnzwLH0XLEHk/96CtNmzGLn7Yex+Rc24N5bfgVAzxV78OqCNwG4dtyxrLfO6vTo0Y111urP/bf9GoA/Xno7f77+Hlq6aTni4xHjZ9ZYlUt+/0O+f+z5nyhfHnWrgYBcrrwvTv4Y2CgiFpRz8icfef/Mcv23aMaMJ7nrrmlMmfIgixd/wKJF73L88b/lzDOPq3bXLCcvvvw6ry18m3ffW8y77y3m3qlPscnQ9ZDEFTdM4ee/ueZT79m/4SwgyXFf9Nsj2GP/X36yznkLGLjWarz48uvU19fRp3evj9I1vVfuyYTLTuCUM69j2kOz8/+ANS7rhTnSiRnTgRcjYm9J/YBrgUHA88C3I2JhJXXn/Tv4C8CbObfRJR133EimTLmcu+66hLPOOoFtttnEQbuLu2XidLbfamPq6+vouWIPvrTZZ3lq1ovcfd/j7LPXVqy+WpKD7rvKSqy7dv+y6rz1zgc5aL8dAfjWXltzz/8+AUD37vVce9GxXDXhn0y4dWo+H6hg1IGtTD8Gnix5fSIwKSIGA5PS1xXJZcQt6dh091lgsqRbgcVNxyPirDzaNatl4885ih22/Rz9+/Zm9tRz+eVZN9C9e/JP8OIr/sHTs1/izsmP8MDE37B0aXD5NXcz85m5AJxy5nXccsVo6urq+HDJEo752WXMefG1dtu8/NrJXPr7H/L4lN+x8I1F/OePzgFg37235ctbbUy/VVfm4DSwNxx3AY/O/L+cPn3ty3LELWkg8HVgLMmaTQDDgZ3S/fHAZOC/Kqo/j7yWpLYmHEdElDE5dflOlVjLeq7ruez2ae/NuXqZw+6M124tO+Zs3v/rbbYn6QaS2XS9gePTVMkbEbFqyTkLI6JvJX3NZcQdEacASBrRfPqfpBF5tGlmtizUgWl+pRMpUuPSa3RI2huYHxEPStopyz42yfvi5Gig+bztlsrMzKqqI9P8PjmR4lO2B/5D0l7AikAfSVcAr0gaEBHzJA0A5lfa17xy3HsCewFrS/pDyaE+wJI82jQzWxZZpbgjYjTJAJV0xH18RBws6QxgJHBa+vOmStvIa8T9Esk0mBHAMyRztxtJ5nMfk1ObZmYV64Qba04DrpM0CphDEh8rklfgngkcBPQADiP5MlsHuAz4W05tmplVLI+4HRGTSWaPkN7PsmsW9eY1j/t0oC+wXkRsHhGbARsAqwBn5tSmmVnFpPK3astrxL03MCRK5hpGxFuSjgCeIpmYbmZWM4q0IlBegTuihQniEdGojsy5MTPrJLWweFS58vqSmSnpkOaFkg4mGXGbmdWUHG55z01eI+4jgQmSDgMeJJlV8iWgJ7BPTm2amVWsSMmAvO6cfBHYWtIuwOdJvqRui4hJebRnZrasamEkXa5c75yMiLuAu/Jsw8wsC7UwW6Rced/ybmZWCLXwLMlyOXCbmeFUiZlZ4ThVYmZWMAWK2w7cZmZQrBtwHLjNzPCI28yscOqW9xtwzMyKxhcnzcwKpkBx24HbzAy8rKuZWeE4VWJmVjAq0Ji7OD01M8uRVFf21nY9WkfS3ZKelPSEpB+n5f0k3SlpVvqzb6V9deA2MwMyfJTCEuC4iPgcsA1wpKShwInApIgYDExKX1fEgdvMDFAH/mtLRMyLiBnp/tvAk8DawHBgfHraeOCblfbVgdvMDOjIiFtSg6TpJVtDizVKg4DNgKnAmhExD5LgDqxRaU99cdLMDNrNXZeKiHHAuLbr08rAjcBPIuItZThtxYHbzIxsZ5VI6k4StK+MiAlp8SuSBkTEPEkDgPmV1u9UiZkZ2eW4lQytLwGejIizSg7dDIxM90cCN1XaV4+4zcyADMex2wP/CTwm6eG07L+B04DrJI0C5gAjKm3AgdvMDMgqBx0R99L6nMFds2jDgdvMDCjSMlMO3GZm0G7uupY4cJuZAaK+2l0omwO3mRnZ5bg7gwO3mRngHLeZWcEUaVlXB24zM8AjbjOzgunIWiXV5sBtZoZTJWZmBeRUiZlZofgGHDOzgvE8bjOzwnGO28ysUHxx0sysYJwqMTMrHI+4zcwKpUizShQR1e6DtUNSQ/pUabOP+O/F8qs4vxss3xqq3QGrSf57sZxy4DYzKxgHbjOzgnHgLgbnMa0l/nuxnPLFSTOzgvGI28ysYBy4zcwKxoE7A5IaJT1csp2Ylk+WtGUL539X0rmd31OrVZIGSrpJ0ixJ/5Z0tqQekr4oaa+S806WdHw1+2rV5zsns/FeRHwxr8oldYuIJbVWl2VDySIZE4DzI2K4pHqSC49jgSeALYG/Z9RWfUQ0ZlGXVY9H3J1E0qGSnpF0D7B9Sfnqkm6U9EC6bZ+WnyxpnKSJwJ9aqO8ESY9JekTSaWnZ99M6Hknr7JWWXy7pLEl3A7+RtKGk2yU9KOmfkjbulP8J1ppdgPcj4jKANLAeA3wPOB3YP/1Nbv/0/KHpb3PPSjq6qRJJB0ualp57YfoFgKRFkn4haSqwbad+MstHRHhbxg1oBB4u2fZPyyeTjJYGAHOA1YEewH3Auek5VwFfTvfXBZ5M908GHgR6ttDensD/Ar3S1/3Sn6uVnPMr4Kh0/3Lgb0B9+noSMDjd3xq4q9r/D5fnDTga+F0L5Q+lx84tKTs5/bNfAegPLAC6A58DbgG6p+edBxyS7gfw7Wp/Tm/ZbU6VZKO9VMnWwOSIeBVA0rXAkPTYbiQjqKZz+0jqne7fHBHvtVDfbsBlEfEuQES8npYPk/QrYFVgZeCOkvdcHxGNklYGtgOuL2lzhbI+peVFJMG13PJbI2IxsFjSfGBNYFdgC+CB9M+1JzA/Pb8RuDHrTlv1OHB3ntYmzNcB2zYP0Ok/vndaeU9r/6AvB74ZEY9I+i6wU8mxprrqgDfa+aKxzvUEsG9pgaQ+wDokQbe5xSX7jST/jgWMj4jRLZz/fjiv3aU4x905pgI7SVpNUndgRMmxicCPml5I+mIZ9U0EDivJYfdLy3sD89I2DmrpjRHxFvCcpBHpeyVp0w5+HsvWJKCXpEMguYAI/Jbki/gVkj/XcurYT9IaaR39JK2XT3et2hy4s9Gz2XTA00oPRsQ8ktzkv4B/ADNKDh8NbCnpUUkzgR+011hE3A7cDEyX9DDQND3sJJIviTuBp9qo4iBglKRHSEZ7w9v/iJaXiAhgH2CEpFnAM8D7wH8Dd5Ok0kovTrZUx0zgZ8BESY+S/B0YkHvnrSp8y7uZWcF4xG1mVjAO3GZmBePAbWZWMA7cZmYF48BtZlYwDtyWi5IVEx+XdH3TnPMK67pc0n7p/sWShrZx7k6Stqugjecl9a+0j2adyYHb8vJeRHwxIoYBH9BsfnrTAkgdFRHfS+cst2Ynklv6zbosB27rDP8EPpuOhu+WdBXwmKR6SWekKxo+Kulw+OhuznMlzZR0K7BGU0UqWeNc0tckzUhXQ5wkaRDJF8Qx6Wh/hzZWX1xN0kRJD0m6kOSWcbNC8FollitJ3UhWM7w9LdoKGBYRz0lqAN6MiC9JWgG4L13GdjNgI+ALJAsozQQubVbv6sBFwI5pXf0i4nVJFwCLIuLM9LyrSFbeu1fSuiQLb30OGAPcGxG/kPR1oCHX/xFmGXLgtrz0TG/Hh2TEfQlJCmNaRDyXlu8ObNKUvwZWAQYDOwJXpwsjvSTprhbq3waY0lRXyQqJzbW2+uKOwLfS994qaWFlH9Os8zlwW14+tdRtCyseimTN8DuanbcXra+mWPrectZraGv1Ra/3YIXkHLdV0x3AEelqhkgaImklYApwQJoDHwDs3MJ7/wV8RdL66XubVkh8m0+uptfa6otTSFdQlLQn0DerD2WWNwduq6aLSfLXMyQ9DlxI8lvgX4BZwGPA+cA9zd+YPpSiAZiQrnJ4bXroFmCfpouTtL764inAjpJmkKRs5uT0Gc0y59UBzcwKxiNuM7OCceA2MysYB24zs4Jx4DYzKxgHbjOzgnHgNjMrGAduM7OC+X9kw4RHtfctbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the predictions from the Excel file\n",
    "# df_pred = pd.read_excel(\"EC_fasttext_predictions.xlsx\")\n",
    "\n",
    "# # Create the confusion matrix\n",
    "# conf_mat = pd.crosstab(df_pred['actual_label'], df_pred['predicted_label'])\n",
    "\n",
    "# # Plot the confusion matrix with accuracy\n",
    "# sns.heatmap(conf_mat, annot=True, cmap=\"YlGnBu\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# # plt.title(f\"Confusion Matrix (Accuracy = {accuracy})\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Load the predictions from the Excel file\n",
    "# df_pred = pd.read_excel(\"EC_fasttext_predictions.xlsx\")\n",
    "\n",
    "# # Reset index to ensure labels are numbered sequentially from 0\n",
    "# # df_pred['actual_label'] = pd.Categorical(df_pred['actual_label'])\n",
    "# # df_pred['predicted_label'] = pd.Categorical(df_pred['predicted_label'])\n",
    "# # df_pred['actual_label'] = df_pred['actual_label'].cat.codes\n",
    "# # df_pred['predicted_label'] = df_pred['predicted_label'].cat.codes\n",
    "\n",
    "# # Create the confusion matrix\n",
    "# conf_mat = pd.crosstab(df_pred['actual_label'], df_pred['predicted_label'])\n",
    "\n",
    "# # Reset index to ensure labels are numbered sequentially from 0\n",
    "# conf_mat = conf_mat.reset_index(drop=True)\n",
    "\n",
    "# # Calculate the overall accuracy\n",
    "# accuracy = round(sum(conf_mat[i][i] for i in range(len(conf_mat))) / df_pred.shape[0], 2)\n",
    "\n",
    "# # Calculate the F1 score for each category\n",
    "# f1_score = f1_score(df_pred['actual_label'], df_pred['predicted_label'], average=None)\n",
    "\n",
    "# # Plot the confusion matrix with accuracy and F1 score\n",
    "# sns.heatmap(conf_mat, annot=True, cmap=\"YlGnBu\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title(f\"Confusion Matrix (Accuracy = {accuracy}, F1 score = {f1_score})\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the test data and make predictions\n",
    "# list_pred_rows = []\n",
    "# with open(test_data, \"r\") as g:\n",
    "#     for line in g:\n",
    "#         splited = line.split(\" \")\n",
    "#         actual_label = splited[0].replace('__label__','').replace('_',' ')\n",
    "#         text = ' '.join(splited[1:]).strip()\n",
    "#         predicted_label, score = classifier.predict(text)\n",
    "#         list_pred_rows.append({'text':text, 'actual_label':actual_label, \n",
    "#                             'predicted_label':predicted_label[0].replace('__label__','').replace('_',' '),\n",
    "#                                'score':score})\n",
    "\n",
    "# # Create a DataFrame from the predictions\n",
    "# df_pred = pd.DataFrame(list_pred_rows)\n",
    "\n",
    "# # Calculate the confusion matrix\n",
    "# conf_mat = pd.crosstab(df_pred['actual_label'], df_pred['predicted_label'])\n",
    "\n",
    "# # Calculate the overall accuracy\n",
    "# if not conf_mat.empty:\n",
    "#     accuracy = round(sum(conf_mat[i][i] for i in range(len(conf_mat))) / df_pred.shape[0], 2)\n",
    "# else:\n",
    "#     accuracy = 0.0\n",
    "\n",
    "# # Plot the confusion matrix with accuracy\n",
    "# sns.heatmap(conf_mat, annot=True, cmap=\"YlGnBu\")\n",
    "# plt.title(f\"Confusion Matrix (Accuracy={accuracy})\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert saved model .bin and config.json to .pkl compatible files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert saved model .bin and config.json to .pkl compatible files\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Load the model configuration from the config.json file\n",
    "config = AutoConfig.from_pretrained('EC_model_outer_combined_texts_data_v6', num_labels=2)\n",
    "\n",
    "# Load the model from the binary file using the configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained('EC_model_outer_combined_texts_data_v6', config=config)\n",
    "\n",
    "# Save the model and configuration as a pickle file\n",
    "with open('EC_model_outer_combined_texts_data_v6/EC_model.pkl', 'wb') as f:\n",
    "    pickle.dump((config, model.state_dict()), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify sentences as 'Child care' on the similarity with list of phrases threshold 0.9 and others as 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def classify_sentences(sentences):\n",
    "#     # List of phrases related to child care\n",
    "#     child_care_phrases = [\n",
    "#         \"child care\",\n",
    "#         \"daycare\",\n",
    "#         \"nanny\",\n",
    "#         \"babysitter\",\n",
    "#         \"after-school program\",\n",
    "#         \"child development\",\n",
    "#         \"playgroup\",\n",
    "#         \"childcare provider\",\n",
    "#         \"preschool\",\n",
    "#         \"early childhood education\",\n",
    "#         \"child-friendly\",\n",
    "#         \"parenting\",\n",
    "#         \"family support\",\n",
    "#         \"children's health\",\n",
    "#         \"child psychology\"\n",
    "#     ]\n",
    "\n",
    "#     # Tokenize, remove stopwords and lemmatize the sentences\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "#     filtered_sentences = [[lemmatizer.lemmatize(word) for word in sentence if word not in stop_words] for sentence in tokenized_sentences]\n",
    "#     preprocessed_sentences = [' '.join(sentence) for sentence in filtered_sentences]\n",
    "\n",
    "#     # Vectorize the sentences and calculate cosine similarity with child care phrases\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     child_care_matrix = vectorizer.fit_transform(child_care_phrases)\n",
    "#     sentence_matrix = vectorizer.transform(preprocessed_sentences)\n",
    "#     similarity_matrix = cosine_similarity(sentence_matrix, child_care_matrix)\n",
    "\n",
    "#     # Classify the sentences based on similarity threshold\n",
    "#     classifications = []\n",
    "#     for i in range(len(sentences)):\n",
    "#         if max(similarity_matrix[i]) >= 0.9:\n",
    "#             classifications.append(\"Child care\")\n",
    "#         else:\n",
    "#             classifications.append(\"Other\")\n",
    "    \n",
    "#     return classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\n",
    "#     \"I need to find a daycare for my child.\",\n",
    "#     \"The new playground is very child-friendly.\",\n",
    "#     \"I love spending time with my family.\",\n",
    "#     \"She works as a nanny for several families.\",\n",
    "#     \"The after-school program offers homework help.\",\n",
    "#     \"Child psychology is a fascinating field.\",\n",
    "#     \"My son is enrolled in a preschool program.\",\n",
    "#     \"We need more support for children's health.\",\n",
    "#     \"Early childhood education is important.\",\n",
    "#     \"I enjoy playing with my kids at the park.\",\n",
    "#     \"We provide quality child care services.\",\n",
    "#     \"The playgroup meets every Wednesday morning.\",\n",
    "#     \"Parenting is not easy, but it is rewarding.\",\n",
    "#     \"The childcare provider has many years of experience.\",\n",
    "#     \"The book provides helpful tips for child development.\"\n",
    "# ]\n",
    "\n",
    "# classifications = classify_sentences(sentences)\n",
    "# print(classifications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a one class classification efficient model to classify sentences as 'Child care' on the similarity with list of phrases threshold 0.9 and others as 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import OneClassSVM\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a list of child care phrases\n",
    "# child_care_phrases = [\"child care\", \"day care\", \"childcare services\", \"nursery school\", \"pre-school\", \"after-school program\", \"babysitting\", \"playgroup\"]\n",
    "\n",
    "# # Define a threshold for the similarity score\n",
    "# threshold = 0.9\n",
    "\n",
    "# # Define a function to calculate the similarity score\n",
    "# def similarity_score(text):\n",
    "#     # Tokenize the input text\n",
    "#     words = text.split()\n",
    "#     # Calculate the TF-IDF vectors for the child care phrases and the input text\n",
    "#     tfidf = TfidfVectorizer().fit_transform(child_care_phrases + [text])\n",
    "#     # Calculate the cosine similarity between the child care phrases and the input text\n",
    "#     similarity = tfidf[:-1].dot(tfidf[-1].T).toarray().flatten()\n",
    "#     # Return the maximum similarity score\n",
    "#     return np.max(similarity)\n",
    "\n",
    "# # Define a function to classify the input text\n",
    "# def classify(text):\n",
    "#     # Calculate the similarity score\n",
    "#     score = similarity_score(text)\n",
    "#     # If the similarity score is above the threshold, classify as \"Child care\", otherwise classify as \"Other\"\n",
    "#     if score >= threshold:\n",
    "#         return \"Child care\"\n",
    "#     else:\n",
    "#         return \"Other\"\n",
    "\n",
    "# # Load the data\n",
    "# data = [...]  # list of input sentences\n",
    "\n",
    "# # Train the One-Class SVM model\n",
    "# X_train = [similarity_score(text) for text in data]\n",
    "# clf = OneClassSVM().fit(X_train)\n",
    "\n",
    "# # Classify the input sentences\n",
    "# predictions = [classify(text) if clf.predict([similarity_score(text)]) == 1 else \"Other\" for text in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.24xlarge",
  "kernelspec": {
   "display_name": "python3 (adl-core-custom-docker/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:660999144548:image-version/adl-core-custom-docker/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
